{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS w261 - Machine Learning At Scale\n",
    "## Assignment - HW13\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Alex Smith<br>\n",
    "__Class:__ Section 2, e.g., Summer 2016<br>\n",
    "__Email:__ aksmith@ischool.berkely.edu<br>\n",
    "__Week:__   14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW 3 Problems](#3)    \n",
    "    13.1.  [Build a decision to predict whether you can play tennis or no](#13.1)   \n",
    "    13.2.  [Regression Tree (OPTIONAL Homework)](#13.2)    \n",
    "    13.3.  [Predict survival on the Titanic](#13.3)    \n",
    "    13.4.  [Heritage Healthcare Prize (Predict # Days in Hospital next year)](#13.4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "* Homework submissions are due by Tuesday, 08/23/2016 at 11AM (West Coast Time).\n",
    "\n",
    "\n",
    "* Prepare a single Jupyter notebook (not a requirment), please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + [Submission Link - Google Form](https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form)\n",
    "\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "\n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* Lecture 12 (Async)\n",
    "\n",
    "* Chapter 17 on decision Trees,   https://www.dropbox.com/s/5ca98ah5chqlcmn/Data_Science_from_Scratch%20%281%29.pdf?dl=0   [Please do not share this PDF]\n",
    "* Karau, Holden, Konwinski, Andy, Wendell, Patrick, & Zaharia, Matei. (2015). Learning Spark: Lightning-fast big data analysis. Sebastopol, CA: O’Reilly Publishers.\n",
    "* Hastie, Trevor, Tibshirani, Robert, & Friedman, Jerome. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). Stanford, CA: Springer Science+Business Media. __(Download for free [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf))__\n",
    "* Ryza, Sandy, Laserson, Uri, Owen, Sean, & Wills, Josh. (2015). Advanced analytics with Spark: Patterns for learning from data at scale. Sebastopol, CA: O’Reilly Publishers.\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"13.1\"></a>\n",
    "## HW13.1 Build a decision to predict whether you can play tennis or not\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Write a program in Python (or in Spark; this part is optional) to implement the ID3 decision tree algorithm. You should build a tree to predict PlayTennis, based on the other attributes (but, do not use the Day attribute in your tree.). You should read in a space delimited dataset in a file called dataset.txt and output to the screen your decision tree and the training set accuracy in some readable format. For example, here is the tennis dataset. The first line will contain the names of the fields:\n",
    "\n",
    "<PRE>\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no\n",
    "</PRE>\n",
    "\n",
    "The last column is the classification attribute, and will always contain contain the values yes or no.\n",
    "\n",
    "For output, you can choose how to draw the tree so long as it is clear what the tree is. You might find it easier if you turn the decision tree on its side, and use indentation to show levels of the tree as it grows from the left. For example:\n",
    "\n",
    "<PRE>\n",
    "outlook = sunny\n",
    "|  humidity = high: no\n",
    "|  humidity = normal: yes\n",
    "outlook = overcast: yes\n",
    "outlook = rainy\n",
    "|  windy = TRUE: no\n",
    "|  windy = FALSE: yes\n",
    "\n",
    "</PRE>\n",
    "\n",
    "You don't need to make your tree output look exactly like above: feel free to print out something similarly readable if you think it is easier to code.\n",
    "\n",
    "You may find Python dictionaries especially useful here, as they will give you a quick an easy way to help manage counting the number of times you see a particular attribute.\n",
    "\n",
    "Here are some FAQs that I've gotten in the past regarding this assignment, and some I might get if I don't answer them now.\n",
    "\n",
    "__Should my code work for other datasets besides the tennis dataset?__ \n",
    "Yes. We will give your program a different dataset to try it out with. You may assume that our dataset is correct and well-formatted, but you should not make assumptions regrading number of rows, number of columns, or values that will appear within. The last column will also be the classification, and will always contain yes or no values.\n",
    "\n",
    "__Is it possible that some value, like \"normal,\" could appear in more than one column?__\n",
    "Yes. In addition to the column \"humidity\", we might have had another column called \"skycolor\" which could have values \"normal,\" \"weird,\" and \"bizarre.\"\n",
    "\n",
    "__Could \"yes\" and \"no\" appear as possible values in columns other than the classification column?__\n",
    "Yes. In addition to the classification column \"playtennis,\" we might have had another column called \"seasonalweather\" which would contain \"yes\" and \"no.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tennis.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile tennis.txt\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadData(path2file):\n",
    "    \"\"\"takes a filepath and loads in the data. the \n",
    "    first row of data MUST be the labels for the \n",
    "    data. returns the dictionary of the data and a\n",
    "    list of all the labels\"\"\"\n",
    "    \n",
    "    # create a list for storing the labels\n",
    "    labels = []\n",
    "    \n",
    "    # create a dictionary for storing the\n",
    "    # data for each record\n",
    "    records = []\n",
    "    \n",
    "    # open the file\n",
    "    with open(path2file,'r') as myfile:\n",
    "        \n",
    "        # read in the lines\n",
    "        for index_i,line in enumerate(myfile.readlines()):\n",
    "            \n",
    "            # split the line by spaces\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            \n",
    "            # if it's the first line, then\n",
    "            # save the line to the list of labels\n",
    "            if index_i == 0:\n",
    "                labels = line[1:]\n",
    "            \n",
    "            # otherwise add a record to the dictionary\n",
    "            else:\n",
    "                \n",
    "                # set up the label for that data\n",
    "                label = line[-1]\n",
    "                if label == 'yes': \n",
    "                    label = True\n",
    "                else:\n",
    "                    label = False\n",
    "                    \n",
    "                # gather the attributes\n",
    "                attributes = line[1:-1]\n",
    "                \n",
    "                # create a new dictionary for the record\n",
    "                record = {}\n",
    "                \n",
    "                # loop through the attributes\n",
    "                # and add each attribute to the \n",
    "                # dictionary for that day\n",
    "                for index_j,attribute in enumerate(attributes):\n",
    "                    record[labels[index_j]] = attribute\n",
    "                    \n",
    "                # append the record with its label to the list\n",
    "                info = record,label\n",
    "                records.append(info)\n",
    "        \n",
    "    # return the labels and a dictionary of \n",
    "    # all the data\n",
    "    return labels[:-1],records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'outlook': 'sunny', 'temperature': 'hot', 'wind': 'FALSE', 'humidity': 'high'}, False)\n",
      "({'outlook': 'sunny', 'temperature': 'hot', 'wind': 'TRUE', 'humidity': 'high'}, False)\n",
      "({'outlook': 'overcast', 'temperature': 'hot', 'wind': 'FALSE', 'humidity': 'high'}, True)\n",
      "({'outlook': 'rainy', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'high'}, True)\n",
      "({'outlook': 'rainy', 'temperature': 'cool', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
      "({'outlook': 'rainy', 'temperature': 'cool', 'wind': 'TRUE', 'humidity': 'normal'}, False)\n",
      "({'outlook': 'overcast', 'temperature': 'cool', 'wind': 'TRUE', 'humidity': 'normal'}, True)\n",
      "({'outlook': 'sunny', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'high'}, False)\n",
      "({'outlook': 'sunny', 'temperature': 'cool', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
      "({'outlook': 'rainy', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
      "({'outlook': 'sunny', 'temperature': 'mild', 'wind': 'TRUE', 'humidity': 'normal'}, True)\n",
      "({'outlook': 'overcast', 'temperature': 'mild', 'wind': 'TRUE', 'humidity': 'high'}, True)\n",
      "({'outlook': 'overcast', 'temperature': 'hot', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
      "({'outlook': 'rainy', 'temperature': 'mild', 'wind': 'TRUE', 'humidity': 'high'}, False)\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "labels,data = loadData('tennis.txt')\n",
    "\n",
    "# preview the data\n",
    "for line in data:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to compute entropy among the different possible partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    \n",
    "    # create a value to hold entropy\n",
    "    entrop_sum = 0.0\n",
    "    \n",
    "    # loop through each probability\n",
    "    for prob in class_probabilities:\n",
    "\n",
    "        # calculate the entropy for each\n",
    "        # probability\n",
    "        if prob:   # ignore zero probabilities\n",
    "            entrop_part = -prob * math.log(prob,2)\n",
    "\n",
    "            # calculate the sum of the entropies\n",
    "            entrop_sum = entrop_sum + entrop_part\n",
    "    \n",
    "    return entrop_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def classProbabilities(labels): \n",
    "    \"\"\"takes as input a bunch of labels\n",
    "    and returns the probabilities for\n",
    "    each label\"\"\"\n",
    "    \n",
    "    # set the total labels as the length\n",
    "    # of the label array\n",
    "    total_count = len(labels) \n",
    "\n",
    "    # create an array to store the \n",
    "    # probabilities of each subset\n",
    "    probs = []\n",
    "\n",
    "    # covert the labels into a special\n",
    "    # dictionary of type counter\n",
    "    for count in Counter(labels).values():\n",
    "        \n",
    "        # take the partial probability\n",
    "        # and append it to the main array\n",
    "        prob_partial = float(count) / float(total_count)\n",
    "        probs.append(prob_partial)\n",
    "        \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataEntropy(labeled_data):\n",
    "    \n",
    "    # create an array to hold the labels\n",
    "    labels = []\n",
    "    \n",
    "    # loop through the data to get the labels\n",
    "    # and append them to our labels array\n",
    "    for _,label in labeled_data:\n",
    "        labels.append(label)\n",
    "    \n",
    "    # calculate the probabilities based on the\n",
    "    # labels\n",
    "    probabilities = classProbabilities(labels) \n",
    "    \n",
    "    # return the entropy of these probabilities\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partitionEntropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data \n",
    "    into subsets. subsets is a list of lists of labeled data\"\"\"\n",
    "\n",
    "    # calculate the total count as the sum of the\n",
    "    # number of elements in each subset of subsets\n",
    "    total_count = 0\n",
    "    for subset in subsets:\n",
    "        total_count = total_count + len(subset)\n",
    "        \n",
    "    # set a variable for the total entropy\n",
    "    total_entropy = 0.0\n",
    "    \n",
    "    # loop through each subset\n",
    "    for subset in subsets:\n",
    "        \n",
    "        # calculate the partial entropy\n",
    "        partial_entropy = dataEntropy(subset) * len(subset) / total_count\n",
    "\n",
    "        # add to the total entropy\n",
    "        total_entropy = total_entropy + partial_entropy\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def partitionBy(data,label):\n",
    "    \"\"\"takes as input a list of attributes\n",
    "    and labels and outputs the groups and \n",
    "    their corresponding values\"\"\"\n",
    "    \n",
    "    # create a special type of dictionary, \n",
    "    # a list dictionary that only adds unique values\n",
    "    groups = defaultdict(list)\n",
    "    \n",
    "    # loop through input\n",
    "    for inpt in data:\n",
    "        key = inpt[0][label]\n",
    "        groups[key].append(inpt)\n",
    "    \n",
    "    # return the groups\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'overcast': [({'humidity': 'high',\n",
       "                'outlook': 'overcast',\n",
       "                'temperature': 'hot',\n",
       "                'wind': 'FALSE'},\n",
       "               True),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'overcast',\n",
       "                'temperature': 'cool',\n",
       "                'wind': 'TRUE'},\n",
       "               True),\n",
       "              ({'humidity': 'high',\n",
       "                'outlook': 'overcast',\n",
       "                'temperature': 'mild',\n",
       "                'wind': 'TRUE'},\n",
       "               True),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'overcast',\n",
       "                'temperature': 'hot',\n",
       "                'wind': 'FALSE'},\n",
       "               True)],\n",
       "             'rainy': [({'humidity': 'high',\n",
       "                'outlook': 'rainy',\n",
       "                'temperature': 'mild',\n",
       "                'wind': 'FALSE'},\n",
       "               True),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'rainy',\n",
       "                'temperature': 'cool',\n",
       "                'wind': 'FALSE'},\n",
       "               True),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'rainy',\n",
       "                'temperature': 'cool',\n",
       "                'wind': 'TRUE'},\n",
       "               False),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'rainy',\n",
       "                'temperature': 'mild',\n",
       "                'wind': 'FALSE'},\n",
       "               True),\n",
       "              ({'humidity': 'high',\n",
       "                'outlook': 'rainy',\n",
       "                'temperature': 'mild',\n",
       "                'wind': 'TRUE'},\n",
       "               False)],\n",
       "             'sunny': [({'humidity': 'high',\n",
       "                'outlook': 'sunny',\n",
       "                'temperature': 'hot',\n",
       "                'wind': 'FALSE'},\n",
       "               False),\n",
       "              ({'humidity': 'high',\n",
       "                'outlook': 'sunny',\n",
       "                'temperature': 'hot',\n",
       "                'wind': 'TRUE'},\n",
       "               False),\n",
       "              ({'humidity': 'high',\n",
       "                'outlook': 'sunny',\n",
       "                'temperature': 'mild',\n",
       "                'wind': 'FALSE'},\n",
       "               False),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'sunny',\n",
       "                'temperature': 'cool',\n",
       "                'wind': 'FALSE'},\n",
       "               True),\n",
       "              ({'humidity': 'normal',\n",
       "                'outlook': 'sunny',\n",
       "                'temperature': 'mild',\n",
       "                'wind': 'TRUE'},\n",
       "               True)]})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the partitionBy function\n",
    "partitionBy(data,'outlook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partitionEntropyBy(data, label):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "\n",
    "    # gathers up the partions of the data \n",
    "    partitions = partitionBy(data, label)\n",
    "\n",
    "    return partitionEntropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook 0.693536138896\n",
      "temperature 0.911063393012\n",
      "humidity 0.788450457308\n",
      "wind 0.892158928262\n"
     ]
    }
   ],
   "source": [
    "# test out by printing the entropies for making each of many divisions\n",
    "for label in labels: \n",
    "    print label, partitionEntropyBy(data,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def buildTreeID3(inputs, split_candidates=None):\n",
    "    \"\"\"builds an decision tree based on the ID3 algorithm,\n",
    "    takes as input, some data and potential split_candidates\n",
    "    where the split candidates are none in the initialization\"\"\"\n",
    "    \n",
    "    # if this is our first pass,\n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None: \n",
    "        split_candidates = inputs[0][0].keys()\n",
    "        \n",
    "    # count True's and False's in the inputs\n",
    "    # as the total number of inputs minutes\n",
    "    # the number of true labels\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = 0\n",
    "    for item,label in inputs:\n",
    "        if label==True:\n",
    "            num_trues = num_trues + 1\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    # if there are no trues present, return\n",
    "    # that this leaf node should be false\n",
    "    if num_trues == 0: \n",
    "        return False \n",
    "    \n",
    "    # if there are no falses present, return\n",
    "    # that this leaf node should be true\n",
    "    if num_falses == 0: \n",
    "        return True\n",
    "    \n",
    "    # if we don't have any split candidates left, \n",
    "    # then return the majority class for the leaf\n",
    "    if not split_candidates:\n",
    "        return num_trues >= num_falses\n",
    "    \n",
    "    # otherwise, split on the best attribute\n",
    "    # we take the minimum of the entropies for \n",
    "    # each potential split. the partial function\n",
    "    # helps us out by running the partitionEntropyBy\n",
    "    # function on the inputs and the split_candidates\n",
    "    best_attribute = min(split_candidates,\n",
    "                         key=partial(partitionEntropyBy, inputs))\n",
    "    \n",
    "    # generate the partitions for the best attribute split\n",
    "    partitions = partitionBy(inputs, best_attribute) \n",
    "    \n",
    "    # gather the new candidates for the subtrees\n",
    "    # create a blank array to hold the new candidates\n",
    "    new_candidates = []\n",
    "    \n",
    "    # loop through each candidate\n",
    "    for attribute in split_candidates:\n",
    "        \n",
    "        # provided it's not the attribute on which we've already split\n",
    "        if attribute != best_attribute:\n",
    "            new_candidates.append(attribute)\n",
    "    \n",
    "    # create an empty dictionary to hold the subtrees\n",
    "    subtrees = {}\n",
    "    \n",
    "    # loop through each of the partitions\n",
    "    for attribute_value, subset in partitions.iteritems():\n",
    "        \n",
    "        # build a subtree and add it\n",
    "        subtrees[attribute_value] = buildTreeID3(subset,new_candidates)\n",
    "    \n",
    "    # return the default case, if we don't have a \n",
    "    # particular instance of an attribute in our \n",
    "    # tree, then return the majority case\n",
    "    subtrees[None] = num_trues > num_falses\n",
    "    \n",
    "    # return the tree\n",
    "    return (best_attribute, subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outlook',\n",
       " {None: True,\n",
       "  'overcast': True,\n",
       "  'rainy': ('wind', {None: True, 'FALSE': True, 'TRUE': False}),\n",
       "  'sunny': ('humidity', {None: False, 'high': False, 'normal': True})})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the tree\n",
    "mytree = buildTreeID3(data)\n",
    "buildTreeID3(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a classification for the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, inpt):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "        \n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]: \n",
    "        return tree\n",
    "    \n",
    "    # otherwise this tree consists of an attribute to split on \n",
    "    # and a dictionary whose keys are values of that attribute \n",
    "    # and whose values of are subtrees to consider next\n",
    "    \n",
    "    # grab the first attribute and the \n",
    "    # tree that we start at\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    # find the value for this attribute\n",
    "    # among the data for our first input\n",
    "    subtree_key = inpt.get(attribute) \n",
    "    \n",
    "    # if the subtree_key is not in the labels\n",
    "    # already seen by the tree, then we'll use\n",
    "    # the None key which returns the majority case\n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None\n",
    "        \n",
    "    # we then dig further, into the next\n",
    "    # level of tree by grabbing the subtree \n",
    "    # for that division\n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    \n",
    "    # we'll recursively keep digging until we've\n",
    "    # gotten to a leaf node\n",
    "    return classify(subtree, inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I play tennis if it's sunny...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier\n",
    "print \"I play tennis if it's sunny...\"\n",
    "classify(mytree,{'outlook':'sunny'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW13.1.1 What is the classification accuracy of the tree on the training data?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# create counters to measure accuracy\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# loop through each of the training examples\n",
    "for example,label in data:\n",
    "    \n",
    "    # classify the example\n",
    "    predict = classify(mytree,example)\n",
    "    \n",
    "    # increment the counters\n",
    "    total = total + 1\n",
    "    if predict == label:\n",
    "        correct = correct + 1\n",
    "        \n",
    "# generate the accuracy\n",
    "print \"Model Accuracy:\",float(correct)/float(total)*100,\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The model is eerily 100% accurate on the training data. This implies that we've overfit the training data.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW13.1.2  Is it possible to produce some set of correct training examples that will get the algorihtm\n",
    "to include the attribute Temperature in the learned tree, even though the true target concept is\n",
    "independent of Temperature? if no, explain. If yes, give such a set. __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\">Yes, it is possible to produce some set of correct training examples that include the attribute *temperature* in the learned tree. Even though temperature is independent of the target value in reality, it could happen by chance, that high and low temperatures during some week perfectly correlate with playing or not playing tennis. I have provided one such example below where playing tennis always paired with mild days and not playing tennis is always paired with hot days:<br>\n",
    "```\n",
    "({'outlook': 'sunny', 'temperature': 'hot', 'wind': 'FALSE', 'humidity': 'high'}, False)\n",
    "({'outlook': 'sunny', 'temperature': 'hot', 'wind': 'TRUE', 'humidity': 'high'}, False)\n",
    "({'outlook': 'overcast', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'high'}, True)\n",
    "({'outlook': 'rainy', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'high'}, True)\n",
    "({'outlook': 'rainy', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
    "({'outlook': 'rainy', 'temperature': 'hot', 'wind': 'TRUE', 'humidity': 'normal'}, False)\n",
    "({'outlook': 'overcast', 'temperature': 'mild', 'wind': 'TRUE', 'humidity': 'normal'}, True)\n",
    "({'outlook': 'sunny', 'temperature': 'hot', 'wind': 'FALSE', 'humidity': 'high'}, False)\n",
    "({'outlook': 'sunny', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
    "({'outlook': 'rainy', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
    "({'outlook': 'sunny', 'temperature': 'mild', 'wind': 'TRUE', 'humidity': 'normal'}, True)\n",
    "({'outlook': 'overcast', 'temperature': 'mild', 'wind': 'TRUE', 'humidity': 'high'}, True)\n",
    "({'outlook': 'overcast', 'temperature': 'mild', 'wind': 'FALSE', 'humidity': 'normal'}, True)\n",
    "({'outlook': 'rainy', 'temperature': 'hot', 'wind': 'TRUE', 'humidity': 'high'}, False)\n",
    "```\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW13.1.3  Now, build a tree using only examples D1–D7. What is the classification accuracy for the\n",
    "training set? what is the accuracy for the test set (examples D8–D14)? explain why you think these\n",
    "are the results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outlook',\n",
       " {None: True,\n",
       "  'overcast': True,\n",
       "  'rainy': ('wind', {None: True, 'FALSE': True, 'TRUE': False}),\n",
       "  'sunny': False})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate out the data\n",
    "dataPart1 = data[0:7]\n",
    "dataPart2 = data[7:]\n",
    "\n",
    "# build the tree\n",
    "partD1D7 = buildTreeID3(dataPart1)\n",
    "partD1D7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on training set: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# use the tree to predict examples D8-D14\n",
    "# create counters to measure accuracy\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# loop through each of the training examples\n",
    "for example,label in dataPart1:\n",
    "    \n",
    "    # classify the example\n",
    "    predict = classify(partD1D7,example)\n",
    "    \n",
    "    # increment the counters\n",
    "    total = total + 1\n",
    "    if predict == label:\n",
    "        correct = correct + 1\n",
    "        \n",
    "# generate the accuracy\n",
    "print \"Model Accuracy on training set:\",float(correct)/float(total)*100,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on test set: 71.4285714286 %\n"
     ]
    }
   ],
   "source": [
    "# use the tree to predict examples D8-D14\n",
    "# create counters to measure accuracy\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# loop through each of the training examples\n",
    "for example,label in dataPart2:\n",
    "    \n",
    "    # classify the example\n",
    "    predict = classify(partD1D7,example)\n",
    "    \n",
    "    # increment the counters\n",
    "    total = total + 1\n",
    "    if predict == label:\n",
    "        correct = correct + 1\n",
    "        \n",
    "# generate the accuracy\n",
    "print \"Model Accuracy on test set:\",float(correct)/float(total)*100,\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">My accuracy falls to a mere 70%. I have clearly overfit the few exmaples in the training data (where I had an accuracy of 100%). This is because I extend each leaf node, till I'm 100% certain of the predicted class.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW13.1.4 In this case, and others, there are only a few labelled examples available for training (that\n",
    "is, no additional data is available for testing or validation). Suggest a concrete pruning strategy, that\n",
    "can be readily embedded in the algorithm, to avoid over fitting. Explain why you think this strategy\n",
    "should work.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\">We are overfitting the training data with our model. There are a couple of a quick solutions that would help us not overfit the training data so badly. (1) We could end the subtree construction once we reach a certain number of examples in the node. For example, once we have 3 examples for a given node, we could stop building any further nodes and just take the majority class of that node with the mixed exmaples. (2) We could end the subtree construction once all the entropies fall below a certain value. Rather than continually taking the minimum entropy, once all entropies fall below a certain value, we decide that there is little value to keep splitting and simply take the majority class. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"13.2\"></a>\n",
    " ## HW13.2 Regression Tree (OPTIONAL Homework) \n",
    " \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Implement a decision tree algorithm for regression for two input continous variables and one categorical input variable on a single core computer using Python. \n",
    "\n",
    "- Use the IRIS dataset to evaluate your code, where the input variables are: Petal.Length Petal.Width  Species  and the target or output variable is  Sepal.Length. \n",
    "- Use the same dataset to train and test your implementation. \n",
    "- Stop expanding nodes once you have less than ten (10) examples (along with the usual stopping criteria). \n",
    "- Report the mean squared error for your implementation and contrast that with the MSE from scikit-learn's implementation on this dataset (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"13.3\"></a>\n",
    "## HW13.3 Predict survival on the Titanic using Python (Logistic regression, SVMs, Random Forests)\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, you need to review (and edit the code) in this [notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/kmbgrkhh73931lo/Titanic-EDA-LogisticRegression.ipynb) to do analysis of what sorts of people were likely to survive. In particular, please look at how the tools of machine learning are used to predict which passengers survived the tragedy. Please share any useful graphs/analysis you come up with via the group email. For example, pick the top two most important variables and plot the separating hyperplane in this 2D space that is generated using an SVM (or logistic regression model or plot both; are they similar?) that is learnt using those two features only. Comment on your observations. Please feel free to come up other graphs/analysis (e.g., clustering the passengers). \n",
    "\n",
    "For more details see:\n",
    "\n",
    "* https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"13.4\"></a>\n",
    " ## HW13.4 Heritage Healthcare Prize (Predict # Days in Hospital next year)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "1. Introduction \n",
    "Back to Table of Contents\n",
    "\n",
    "The Heritage Health Prize (HHP) was a data science challenge sponsored by The Heritage Provider Network. It took place from April 4, 2011 to April 4, 2013. For information on the winning entries, please see here.\n",
    "\n",
    "Please see the following notebooks for more background and candidate solutions\n",
    "\n",
    "\n",
    "- Spark Map-Reduce + MMLlib solution (with optional extensions) See [Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/v52cxipe7yftf97/HeritageHealthPrizeUnitTestNotebook_Spark-Map-Reduce.ipynb)\n",
    "\n",
    "- Spark SQL + MLLib solution (with optional extensions): [Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/s2wxg6g982oho5m/HeritageHealthPrizeUnitTestNotebook_SQL_FINAL.ipynb)\n",
    "\n",
    "\n",
    "Please look at section 7 in both notebooks complete any one or more the suggested next steps. Included here are excerpts from section 7 in both of those notebooks. E.g.,\n",
    "\n",
    "* Please complete the EDA extensions using inspiration from the Titanic Notebook from above.\n",
    "* __Complete Section 3.B: EDA-0. Gather information to see what transformations may need to be done on the data.__\n",
    "Answer questions about each raw DataFrame. In general, is the data in good shape? For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers? In the Claims DataFrame (df_claims), how many different ProviderIDs are there? How many different PrimaryConditionGroups are there? What are their values? What values can the CharlesonIndex take on? Are they integers? In the Drug Count DataFrame (df_drug_count), what values can DrugCount take on? Are they all integers? Given this information, what transformations are needed?\n",
    "\n",
    "* __Complete Section 3.D: EDA-1. Create tables and graphs to display information about the transformed DataFrames. __\n",
    "For inspiration, see the Titanic notebook discussed above. Answer questions about each DataFrame. For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what is the minimum, maximum, mean, and standard deviation of DaysInHospital? In the Claims DataFrame, group by MemberID and Year and count the number of records. What is the minimum, maximum, mean, and standard deviation of the count? Do the same for the Drug Count and Lab Count DataFrames, etc.\n",
    "\n",
    "\n",
    "* __ Please generate ensemble of DT model using 100 trees with 8 nodes and report the Loss __\n",
    "Try additional models. See possibilities here (e.g. Decision Tree Regressor, Gradient-Boosted Trees Regressor, Random Forest Regressor). See an example here. Tune their hyperparameters. Try different feature selections. Try a two-step model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
