{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - W261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW11\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Alex Smith  \n",
    "__Class:__ MIDS W261 (Summer 2016 Group 2)     \n",
    "__Email:__ aksmith@iSchool.Berkeley.edu     \n",
    "__Week:__  11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Useful references\n",
    "- [Spark Quick Reference](http://spark.apache.org/docs/latest/quick-start.html)\n",
    "- Synchronous Slides for Week 11\n",
    "- [Quora: L1 & L2 Regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization)\n",
    "- [Andrew Ng's CS229 Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf)\n",
    "- [Sample Logistic Regression Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/r20ff7q0yni5kiu/LogisticRegression-Spark-Notebook.ipynb)\n",
    "- [Sample SVM Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/dm2l73iznde7y4f/SVM-Notebook-Linear-Kernel-2015-06-19.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Start up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f959c1516d0>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f957a1182d0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print sc\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\">HW11.0  Broadcast versus Caching in Spark</span>\n",
    "<span style=\"color:gray\">What is the difference between broadcasting and caching data in Spark? Give an example (in the context of machine learning) of each mechanism (at a highlevel). Feel free to cut and paste code examples from the lectures to support your answer.\n",
    "<br><br>\n",
    "Review the following Spark-notebook-based implementation of KMeans and use the broadcast pattern to make this implementation more efficient. Please describe your changes in English first, implement, comment your code and highlight your changes:\n",
    "- [Notebook](https://www.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb?dl=0)\n",
    "- [Notebook via NBViewer](http://nbviewer.ipython.org/urls/dl.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We broadcast a variable but cache a resilient distributed dataset (RDD). When we broadcast a variable, we send the variable to each node in the cluster and store it there. When we cache an RDD, we store in the RDD in a cluster-wide cache. Both of these methods are useful for storing data that we use repeatedly. However, they are used differently. It's helpful to think about these differences through examples. When performing KMeans in Spark, we'll want to broadcast the intermediary centroid values but cache the data points for which we're attempting to find to clusters. We broadcast the centroid values so that each node in the cluster in the cluster has access to the centroid values and each map function does not need to pull the centroid data fresh. We cache the RDD because we reference the dataset multiple times. If we don't cache the dataset, each time we loop through it to find the nearest centroid for each point, then we need to reload it. If instead, we cache the dataset, then we'll store in the dataset in memory after the first iteration.\n",
    "\n",
    "Reference: [Broadcast](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables), [Cache](http://spark.apache.org/docs/latest/quick-start.html#caching), and [StackOverflow](http://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I'm only submitting a single notebook, I'll only include the code snippet that changed here. For the full code, we can look at HW10 submission which had this KMeans code made more efficient.\n",
    "\n",
    "We broadcast the intermediary centroid values to each node. Because we compare each datapoint to the list of all centroids to identify the closest centroid, we have efficiency gains by storing the centroids with each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of iterations\n",
    "ITERATIONS = 100\n",
    "\n",
    "# set the interesting iterations\n",
    "INTEREST = [1,10,20,100]\n",
    "\n",
    "# set the iterations we want\n",
    "# to record errors for\n",
    "ERROR_INTEREST = [1, 10, 20, 30, 40, 50, 100]\n",
    "\n",
    "# create a list to hold our \n",
    "# error for each iterations\n",
    "errors = []\n",
    "\n",
    "# set the current iteration\n",
    "iteration = 0 \n",
    "\n",
    "# while the iteration is less than \n",
    "# the number of iterations\n",
    "while iteration <= ITERATIONS:\n",
    "    \n",
    "    # BROADCASTING MODIFICATION TO CODE\n",
    "    # broadcast the centroids to each node\n",
    "    broad_centroids = sc.broadcast(centroids)\n",
    "    \n",
    "    # take each line of data and \n",
    "    # find its nearest neighbor\n",
    "    _nearest = parsedData.map(lambda x: findNearest(x,np.array(broad_centroids.value)))\n",
    "    \n",
    "    # and then reduce by key\n",
    "    # to get the sum of all points\n",
    "    # and the number of data points\n",
    "    _summed = _nearest.reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "    # and then reduce by dividing \n",
    "    # the sum of points by the \n",
    "    # total number of points\n",
    "    new_centroids = _summed.map(lambda x: x[1][0]/x[1][1]).collect()\n",
    "    \n",
    "    # convert the new_centroids to \n",
    "    # numpy array\n",
    "    centroids = np.array(new_centroids)\n",
    "    \n",
    "    # calculate the WSSE for this model by computing\n",
    "    # the distance between each point and its assigned\n",
    "    # centroid\n",
    "    \n",
    "    # first calculate the error for each point\n",
    "    _error_each = _nearest.map(lambda x: sqError(x,centroids))\n",
    "\n",
    "    # then calculate the total error\n",
    "    _error_total = _error_each.reduce(lambda x,y: x+y)\n",
    "    \n",
    "    # increment the iterator\n",
    "    iteration = iteration + 1\n",
    "    \n",
    "    # check to see if this is an iteration\n",
    "    # where we want to record the error\n",
    "    # if it is, then record it\n",
    "    if iteration in ERROR_INTEREST:\n",
    "        info = iteration,_error_total\n",
    "        errors.append(info)\n",
    "    \n",
    "    # check to see if this an iteration\n",
    "    # of interest, one that we would\n",
    "    # like to plot\n",
    "    if iteration in INTEREST:\n",
    "        \n",
    "        # print out the iteration \n",
    "        # and the centroids\n",
    "        print \"Iteration\", iteration\n",
    "        print \"WSSSE:\", _error_total\n",
    "        print \"Centroids:\"\n",
    "        print centroids\n",
    "\n",
    "        # plot the centroids and the data\n",
    "        plotKMeans(centroids,data)\n",
    "\n",
    "        print \"\\n\"\n",
    "        \n",
    "# convert the errors to a numpy array\n",
    "# and write them to file\n",
    "errors = np.array(errors)\n",
    "np.savetxt('errors.csv',errors,delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\">HW11.1  Loss Functions</span>\n",
    "<span style=\"color:gray\">In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a L2 penalized logistic regesssion learning algorithm? <br><br>\n",
    "In your reponse, please discuss the loss functions, and the learnt models, and separating surfaces between the two classes.<br><br>\n",
    "In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a perceptron learning algorithm? <br><br>\n",
    "[OPTIONAL]: generate an artifical binary classification dataset with 2 input features and plot the learnt separating surface for both a linear SVM and for  logistic regression. Comment on the learnt surfaces. Please feel free to do this in Python (no need to use Spark).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM versus Logistic Regression\n",
    "In the context of binary classification problems, the linear Support Vector Machine (SVM) will yield a very similar result to the L2 penalized logistic regression learning algorithm. However, it is still important to remember the differences between the two. The SVM calculates a decision boundary to classify each example. The logistic regression algorithm calculates the problability that an example is in one or the other class. With SVM, we care only about the edge cases, those that are close to the boundary. With logistic regression, we care about all cases, even if we decrease teh weight of those examples far from the boundary with ridge regularization. \n",
    "\n",
    "**SVM Loss Function**\n",
    "$$\\sum_{i=1}^{N}[1-y_{i}*W\\vec{x}_{i}]_{+}$$\n",
    "\n",
    "**Logistic Regression L2 Loss Function**\n",
    "$$\\sum_{i=1}^{N}[(y_{i}-W\\vec{x}_{i})^{2}]$$\n",
    "\n",
    "We can see the difference between the loss functions graphically. For logistic regression, we calculate some training error even for correctly classified examples. In the case of hinge loss for SVM, we don't worry about these clearly correctly calculated examples.\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week11/SVMvLogistic.png\" alt=\"Loss\" style=\"width: 500px;\"/>\n",
    "*[Image source:Quora](https://www.quora.com/Is-logistic-regression-with-regularization-similar-in-a-conceptual-sense-to-simple-linear-SVM)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM versus Perceptron\n",
    "In the context of binary classification problems, the linear Support Vector Machine (SVM) algorithms does not yield the same results as a perceptron algorithm. Intuitively, we think of the perceptron algorithm as stopping once it has successfully classified all the data. However, the linear SVM attempts to find the hyperplane that is furtherest from all the datapoints. This means that it creates more of a buffer between the classes. We can also look at the loss functions as a way of understanding this.<br><br>\n",
    "**SVM Loss Function**\n",
    "$$\\sum_{i=1}^{N}[1-y_{i}*W\\vec{x}_{i}]_{+}$$\n",
    "**Perceptron Loss Function**\n",
    "$$\\sum_{i=1}^{N}[max(0,-W\\vec{x}_{i}*y_{i})]$$\n",
    "\n",
    "The SVM loss function is a hinge loss function that is always working. It produces continuous values that we must optimize. However, the perceptron loss function only creates discrete values, 0 or the value of the incorrectly classified point. Once all points are correctly classified, the perceptron loss function stops working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\">HW11.2 Gradient descent</span>\n",
    "<span style=\"color:gray\">In the context of logistic regression describe and define three flavors of penalized loss functions.  Are these all supported in Spark MLLib (include online references to support your answers)?\n",
    "<br><br>\n",
    "Descibe probabilitic interpretations of the L1 and L2 priors for penalized logistic regression (HINT: see synchronous slides for week 11 for details)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of logistic regression, we can think of 3 flavors of penalized loss functions that are **all** supported by Spark's MLLib. The 3 flavors are of the regularizer:\n",
    "- L1 (or Lasso) Regularization\n",
    "- L2 (or Ridge) Regularization\n",
    "- None (no regularization)\n",
    "\n",
    "These three flavors all have different effects and penalize the complexity of the model in different ways. For **L1 (Lasso) Regularization**, we penalize the model by adding to the loss function a component that sums the absolute values of the coefficients. We penalize the axes more (see image below). Therefore, with Lasso regularization we can take the noisy, but useless, features and reduce them to zero. For **L2 (Ridge) Regularization**, we penalize the the complexity of the model by taking the square root of the sum of the squares of the coefficients. With this regularization, we do not bring the noisy, but useless, features to zero, instead we end up bringing them very close to zero. Ultimately, the goal of both types of regularization is to penalize the model-maker for making overly complex that overfit the data. The third flavor is **no regularization** which doesn't provide a penalty for model complexity. The risk with this approach is that the model can get increasingly complex until it overfits the training data.\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week11/LassoVRidge.png\" alt=\"RdigeVLasso\" style=\"width: 500px;\"/>\n",
    "*[Image source](http://gerardnico.com/wiki/data_mining/lasso)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\">HW11.3  Logistic Regression</span>\n",
    "<span style=\"color:gray\">Generate 2 sets of linearly separable data with 100 data points each using the data generation code provided below and plot each in separate plots. Call one the training set and the other the testing set.</span>\n",
    "```\n",
    "def generateData(n):\n",
    " \"\"\" \n",
    "  generates a 2D linearly separable dataset with n samples. \n",
    "  The third element of the sample is the label\n",
    " \"\"\"\n",
    " xb = (rand(n)*2-1)/2-0.5\n",
    " yb = (rand(n)*2-1)/2+0.5\n",
    " xr = (rand(n)*2-1)/2+0.5\n",
    " yr = (rand(n)*2-1)/2-0.5\n",
    " inputs = []\n",
    " for i in range(len(xb)):\n",
    "  inputs.append([xb[i],yb[i],1])\n",
    "  inputs.append([xr[i],yr[i],-1])\n",
    " return inputs\n",
    "```\n",
    "<span style=\"color:gray\">Modify this data generation code to generating non-linearly separable training and testing datasets (with approximately 10% of the data falling on the wrong side of the separating hyperplane. Plot the resulting datasets.<br>\n",
    "<br>\n",
    "NOTE: For the remainder of this problem please use the non-linearly separable training and testing datasets.<br>\n",
    "<br>\n",
    "Using MLLib  train up a LASSO logistic regression model with the training dataset and evaluate with the testing set. What a good number of iterations for training the logistic regression model? Justify with plots and words.<br>\n",
    "<br>\n",
    "Derive and implement in Spark a weighted  LASSO logistic regression. Implement a convergence test of your choice to check for termination within your training algorithm.<br>\n",
    "<br>\n",
    "Weight the above training dataset as follows:  Weight each example using the inverse vector length (Euclidean norm):<br>\n",
    "weight(X)= 1/||X||,<br>\n",
    "where ||X|| = SQRT(X.X)= SQRT(X1^2 + X2^2)<br>\n",
    "Here X is vector made up of X1 and X2.<br>\n",
    "<br>\n",
    "Evaluate your homegrown weighted  LASSO logistic regression on the test dataset. Report misclassification error (1 - Accuracy) and how many iterations does it took to converge.<br>\n",
    "<br>\n",
    "Does Spark MLLib have a weighted LASSO logistic regression implementation? If so, use it and report your findings on the weighted training set and test set. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function generate linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "def generateData(n):\n",
    "    \"\"\"generates a 2D linearly separable dataset with n samples. \n",
    "    The third element of the sample is the label\"\"\"\n",
    "\n",
    "    xb = (rand(n)*2-1)/2-0.5\n",
    "    yb = (rand(n)*2-1)/2+0.5\n",
    "    xr = (rand(n)*2-1)/2+0.5\n",
    "    yr = (rand(n)*2-1)/2-0.5\n",
    "    inputs = []\n",
    "    for i in range(len(xb)):\n",
    "        inputs.append([xb[i],yb[i],1])\n",
    "        inputs.append([xr[i],yr[i],-1])\n",
    "    return np.array(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate two samples of linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.123969270731652426e-01,4.876178687364305375e-01,1.000000000000000000e+00\r\n",
      "7.263460494316192895e-01,-3.348179321123132546e-01,-1.000000000000000000e+00\r\n",
      "-4.508612046355497949e-02,5.753135195168691540e-01,1.000000000000000000e+00\r\n",
      "9.143251338039184439e-01,-2.963493722042721545e-01,-1.000000000000000000e+00\r\n",
      "-8.051971018716123085e-01,4.383609086082024264e-01,1.000000000000000000e+00\r\n",
      "6.253994429442213399e-01,-2.339894688503001419e-01,-1.000000000000000000e+00\r\n",
      "-5.630718827020542161e-02,7.435254412477608765e-01,1.000000000000000000e+00\r\n",
      "5.584572001614525050e-01,-7.010231059007312471e-01,-1.000000000000000000e+00\r\n",
      "-5.343047817040319503e-01,3.224654342907674609e-01,1.000000000000000000e+00\r\n",
      "8.408044493923740870e-01,-4.418302514143426230e-01,-1.000000000000000000e+00\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate the two sets and save them\n",
    "linear_test = generateData(50)\n",
    "linear_train = generateData(50)\n",
    "np.savetxt('linear_test.csv',linear_test,delimiter=',')\n",
    "np.savetxt('linear_train.csv',linear_train,delimiter=',')\n",
    "\n",
    "# preview one of the files\n",
    "!head linear_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to graph the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graphLabelled(data,title,color1='red',color2='blue'):\n",
    "    \"\"\"function that takes an input data, 2D, with\n",
    "    a third column for the label. splits the data\n",
    "    into two data sets and plots the two data sets\n",
    "    with two different colors\"\"\"\n",
    "    \n",
    "    # split the data\n",
    "    set1 = data[data[:,2]==-1]\n",
    "    set2 = data[data[:,2]==1]\n",
    "    \n",
    "    # plot each set\n",
    "    plt.scatter(set1[:,0],set1[:,1],color=color1)\n",
    "    plt.scatter(set2[:,0],set2[:,1],color=color2)\n",
    "    \n",
    "    # set the title\n",
    "    plt.title(title)\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957a0a2e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSIMpCIiINJiCgIhIg/1/uwxGffLKOYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957a0a2d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphLabelled(linear_test,'Test Data')\n",
    "graphLabelled(linear_train,'Train Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify function to generate mostly linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "def generateDataMix(n,mix=0.2):\n",
    "    \"\"\"generates a 2D linearly separable dataset with n samples. \n",
    "    The third element of the sample is the label\n",
    "    we add the mix variable that controls what percentage of\n",
    "    datapoints are mixed into the other set\"\"\"\n",
    "\n",
    "    xb = (rand(n)*2-1)/2-0.5\n",
    "    yb = (rand(n)*2-1)/2+0.5\n",
    "    xr = (rand(n)*2-1)/2+0.5\n",
    "    yr = (rand(n)*2-1)/2-0.5\n",
    "    inputs = []\n",
    "    for i in range(len(xb)):\n",
    "        inputs.append([xb[i],yb[i],1])\n",
    "        inputs.append([xr[i],yr[i],-1])\n",
    "    \n",
    "    # convert the inputs to a numpy\n",
    "    # array\n",
    "    inputs = np.array(inputs)\n",
    "    \n",
    "    # generate the indexes of the examples\n",
    "    # that we would like to swap\n",
    "    swap = np.random.randint(0, n, size=mix*n)\n",
    "    \n",
    "    # loop through all those we wish to swap \n",
    "    # and swap their classifications\n",
    "    for i in swap:\n",
    "        \n",
    "        # reset the current class by multiplying\n",
    "        # it by negative 1\n",
    "        inputs[i][2] = inputs[i][2] * -1\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate two samples of non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.618683910779338042e-01,1.077429623389888347e-01,1.000000000000000000e+00\r\n",
      "5.939108962681916015e-01,-7.458136787099975962e-01,-1.000000000000000000e+00\r\n",
      "-3.439924272428942498e-01,7.103026844058518696e-01,1.000000000000000000e+00\r\n",
      "6.988877821006941327e-01,-1.956489679893546052e-01,-1.000000000000000000e+00\r\n",
      "-4.011297665342470342e-01,3.331525315676561183e-01,1.000000000000000000e+00\r\n",
      "8.110004700803218336e-01,-5.077755052062663532e-01,-1.000000000000000000e+00\r\n",
      "-8.589843557665196583e-01,1.022412635392887514e-01,1.000000000000000000e+00\r\n",
      "2.002743001569864401e-01,-3.494729676869504642e-01,-1.000000000000000000e+00\r\n",
      "-6.230482830164917329e-01,2.561716713378526800e-01,1.000000000000000000e+00\r\n",
      "1.066077316613961434e-01,-3.126209406904745247e-01,-1.000000000000000000e+00\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:25: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate the two sets and save them\n",
    "test_nl = generateDataMix(50)\n",
    "train_nl = generateDataMix(50)\n",
    "np.savetxt('test_nl.csv',test_nl,delimiter=',')\n",
    "np.savetxt('train_nl.csv',train_nl,delimiter=',')\n",
    "\n",
    "# preview one of the files\n",
    "!head test_nl.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwhQEREQq7P8DnPLf8XfCvFEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957a17fb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957a17f910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphLabelled(test_nl,'Test Data')\n",
    "graphLabelled(train_nl,'Train Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write function to turn data into Spark's labelled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(line):\n",
    "    \"\"\"takes each line in the format [x,y,label] and \n",
    "    converts it to Spark labelled points RDD\"\"\"\n",
    "    \n",
    "    # grab the values from the line\n",
    "    values = [float(x) for x in line.split(',')]\n",
    "    \n",
    "    # if the classification is -1, then\n",
    "    # convert it to zero\n",
    "    if values[2] == -1:\n",
    "        values[2] = 0\n",
    "    \n",
    "    return LabeledPoint(values[2], values[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data to a format readable by Spark's library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [-0.666776511004,0.629922738339]), LabeledPoint(0.0, [0.327655008854,-0.566204527638]), LabeledPoint(1.0, [-0.20250519138,0.0516129445782]), LabeledPoint(1.0, [0.729045899887,-0.421574894025]), LabeledPoint(0.0, [-0.343943255331,0.758416700556])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert the data to an RDD\n",
    "trainRDD = sc.textFile('train_nl.csv')\n",
    "\n",
    "# turn the data into labelled points\n",
    "trainRDD = trainRDD.map(parsePoint).cache()\n",
    "\n",
    "# print out a sample of what we've done\n",
    "print trainRDD.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(model,data):\n",
    "    \"\"\"takes two arguments, model and data\n",
    "    where the model is the predictive model and\n",
    "    data is the testing data. computes the \n",
    "    predictions for each example in the test\n",
    "    data and computes the accuracy as the \n",
    "    portion of correctly classified examples\n",
    "    over all examples\"\"\"\n",
    "    \n",
    "    # number of examples\n",
    "    total = len(data)\n",
    "    \n",
    "    # set a counter for the number of correctly\n",
    "    # classified examples\n",
    "    correct = 0\n",
    "    \n",
    "    # predict each examples and see if its\n",
    "    # accurate\n",
    "    for example in data:\n",
    "        \n",
    "        # set the true class and the coordinates\n",
    "        truth = example[2]\n",
    "        coord = list(example[0:2])\n",
    "        \n",
    "        # conver the truth to 0,1\n",
    "        if truth == -1: \n",
    "            truth = 0\n",
    "        \n",
    "        # get the prediction\n",
    "        prediction = model.predict(coord)\n",
    "        \n",
    "        # if its correct increment our correct counter\n",
    "        if prediction == truth:\n",
    "            correct = correct + 1\n",
    "            \n",
    "    # return the accuracy\n",
    "    accur = float(correct) / float(total)\n",
    "    return accur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a logistic regression model using Spark's library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Accuracy: 0.48\n",
      "Coefficients: [0.0,0.0]\n",
      "\n",
      "\n",
      "Iteration 1\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-1.2040139379,1.27252761354]\n",
      "\n",
      "\n",
      "Iteration 2\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-1.72329825785,1.77836145634]\n",
      "\n",
      "\n",
      "Iteration 3\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.1905942677,2.18007800153]\n",
      "\n",
      "\n",
      "Iteration 4\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.45433288323,2.34884573967]\n",
      "\n",
      "\n",
      "Iteration 5\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.57776425633,2.37246864483]\n",
      "\n",
      "\n",
      "Iteration 10\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.71507842894,2.2345208461]\n",
      "\n",
      "\n",
      "Iteration 20\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.71507842894,2.2345208461]\n",
      "\n",
      "\n",
      "Iteration 35\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.71507842894,2.2345208461]\n",
      "\n",
      "\n",
      "Iteration 60\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.71507842894,2.2345208461]\n",
      "\n",
      "\n",
      "Iteration 100\n",
      "Accuracy: 0.92\n",
      "Coefficients: [-2.71507842894,2.2345208461]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# some test data that we can use based on the documentation\n",
    "#data = sc.parallelize([LabeledPoint(0.0, [0.0, 1.0]),LabeledPoint(1.0, [1.0, 0.0])])\n",
    "\n",
    "# set the number of iterations\n",
    "ITERATIONS = 100\n",
    "\n",
    "# set the current iteration\n",
    "iteration = 0\n",
    "\n",
    "# set the iterations of interest for plotting\n",
    "interest = [0,1,2,3,4,5,10,20,35,60,100]\n",
    "\n",
    "# create an array to store the accuracies\n",
    "accuracies = []\n",
    "\n",
    "# set an array of weights that we'll update\n",
    "weights = []\n",
    "\n",
    "# loop through all the intersting iterations\n",
    "for i in interest:\n",
    "    \n",
    "    # set the model\n",
    "    model = LogisticRegressionWithLBFGS.train(trainRDD,regType='l1',iterations=i)\n",
    "    \n",
    "    # compute the accuracy\n",
    "    accur = accuracy(model,test_nl)\n",
    "    \n",
    "    # add the accuracy to the list\n",
    "    accuracies.append(accur)\n",
    "    \n",
    "    # print out the accuracy and the iteration\n",
    "    print \"Iteration\",i\n",
    "    print \"Accuracy:\",accur\n",
    "    print \"Coefficients:\",model.weights\n",
    "    print \"\\n\"\n",
    "\n",
    "#### OLD APPROACH\n",
    "# # keep going till we have 10 iterations\n",
    "# while iteration <= ITERATIONS:\n",
    "    \n",
    "#     # OLD APPROACH\n",
    "# #     # set the model with no weights if its\n",
    "# #     # the first iteration\n",
    "# #     if iteration == 0: \n",
    "# #         model = LogisticRegressionWithLBFGS.train(trainRDD,regType='l1',iterations=1)\n",
    "    \n",
    "# #     # else begin the model with the\n",
    "# #     # previous weights\n",
    "# #     else:\n",
    "# #         model = LogisticRegressionWithLBFGS.train(trainRDD,regType='l1',iterations=1,initialWeights=weights)\n",
    "    \n",
    "# #     # set the weights based on the model\n",
    "# #     weights = model.weights\n",
    "    \n",
    "#     # set the model\n",
    "#     model = LogisticRegressionWithLBFGS.train(trainRDD,regType='l1',iterations=i)\n",
    "    \n",
    "#     # calculate the accuracy\n",
    "#     accur = accuracy(model,test_nl)\n",
    "    \n",
    "#     # if this one of the iterations of interest\n",
    "#     # plot it and compute the accuracy\n",
    "#     if iteration in interest:\n",
    "#         print \"\\n\"\n",
    "#         print \"Iteration\",iteration\n",
    "#         print \"Accuracy:\",accur\n",
    "    \n",
    "#     # increment the iteration\n",
    "#     iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMyucA72ZWeEc6M3MCudAb2ZWuP8DteNbl0NvmQoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957029eb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the evolving accuracies\n",
    "plt.plot(interest,accuracies)\n",
    "plt.title(\"Accuracy for Logistic Regression Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:teal\">We can see that after even a single iteration, we have stabilized in terms of accuracy at around 92%. However, the actual coefficients don't stabilize till around 10 iterations.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to calculate each example's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calcWeight(line):\n",
    "    \"\"\"takes a point and returns that point\n",
    "    with its classification and the weight\"\"\"\n",
    "    \n",
    "    # grab the values from the line\n",
    "    values = [float(x) for x in line.split(',')]\n",
    "    \n",
    "    # get the class and the coordinates\n",
    "    x = float(values[0])\n",
    "    y = float(values[1])\n",
    "    truth = int(values[2])\n",
    "    \n",
    "    # define the weight function\n",
    "    def weight(x,y):\n",
    "        return math.sqrt(x**(2) + y **(2))\n",
    "    \n",
    "    # get the weight for the point\n",
    "    wgt = weight(x,y)\n",
    "    \n",
    "    # return the point with all the\n",
    "    # data\n",
    "    return [x,y,truth,wgt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the weights for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.66677651  0.62992274  1.          0.91727508]\n",
      " [ 0.32765501 -0.56620453 -1.          0.65417534]\n",
      " [-0.20250519  0.05161294  1.          0.20897906]\n",
      " [ 0.7290459  -0.42157489  1.          0.84215991]\n",
      " [-0.34394326  0.7584167  -1.          0.83276218]\n",
      " [ 0.68047352 -0.08827704 -1.          0.68617566]\n",
      " [-0.26157763  0.52208947  1.          0.58395228]\n",
      " [ 0.29162002 -0.35207817  1.          0.45716657]\n",
      " [-0.96350059  0.74657127  1.          1.21889378]\n",
      " [ 0.66888864 -0.08600763 -1.          0.67439552]]\n"
     ]
    }
   ],
   "source": [
    "# load in the training data\n",
    "trainRDD = sc.textFile('train_nl.csv')\n",
    "\n",
    "# weight it\n",
    "train_wgt = trainRDD.map(calcWeight)\n",
    "\n",
    "# print out a sample\n",
    "print np.array(train_wgt.collect())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a named tuple to help us deal with all our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "# create a named tuple type of Point\n",
    "# that has the values: features,\n",
    "# class, and weight\n",
    "Point = namedtuple('Point', 'x y wgt')\n",
    "\n",
    "# write a function to convert a data \n",
    "# to the this format \n",
    "def readPoint(line):\n",
    "    \n",
    "    # set a blank array to hold\n",
    "    # the features\n",
    "    x = []\n",
    "    \n",
    "    # loop through all but the last \n",
    "    # two elements of the array, and\n",
    "    # append it to our array\n",
    "    for i in line[:-2]:\n",
    "        x.append(float(i))\n",
    "    \n",
    "    # append the bias term\n",
    "    x.append(1.0)\n",
    "    \n",
    "    # get the classification\n",
    "    y = line[-2]\n",
    "    \n",
    "    # get the weight\n",
    "    wgt = line[-1]\n",
    "    \n",
    "    return Point(x, y, wgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training data into our homegrown point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Point(x=[-0.6667765110042355, 0.6299227383386319, 1.0], y=1, wgt=0.9172750797350934), Point(x=[0.3276550088540289, -0.5662045276381993, 1.0], y=-1, wgt=0.6541753373103653), Point(x=[-0.20250519138016065, 0.0516129445782425, 1.0], y=1, wgt=0.20897906254922338), Point(x=[0.7290458998871743, -0.4215748940248314, 1.0], y=1, wgt=0.84215991083306), Point(x=[-0.34394325533068537, 0.7584167005560385, 1.0], y=-1, wgt=0.8327621836813778), Point(x=[0.6804735163940775, -0.08827703698126466, 1.0], y=-1, wgt=0.6861756639315566), Point(x=[-0.2615776286653765, 0.5220894651654919, 1.0], y=1, wgt=0.5839522801179827), Point(x=[0.29162001661399173, -0.35207816936229186, 1.0], y=1, wgt=0.4571665685846325), Point(x=[-0.963500593763634, 0.7465712664634533, 1.0], y=1, wgt=1.218893781299962), Point(x=[0.6688886354181288, -0.08600763046501081, 1.0], y=-1, wgt=0.6743955212556889)]\n"
     ]
    }
   ],
   "source": [
    "# convert the training data to named tuple points\n",
    "train = train_wgt.map(readPoint).cache()\n",
    "\n",
    "# print a sample\n",
    "print train.collect()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown Logistic Regression Function (unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logisticRegressionGDUW(data, wInitial=None, learningRate=0.05, iterations=50, regParam=0.01, regType=None):\n",
    "    \"\"\"Logistic Regression function that takes as input\n",
    "    the data, an initial set of weights, a learning rate,\n",
    "    a number of iterations, a regularization hyperparameter,\n",
    "    and the type of regularization.\n",
    "    It outputs the coefficients for the model.\"\"\"\n",
    "    \n",
    "    # set the feature length by taking the length\n",
    "    # of the features in the first point\n",
    "    featureLen = len(data.take(1)[0].x)\n",
    "    \n",
    "    # count the number of data points\n",
    "    n = data.count()\n",
    "    \n",
    "    # if the initial coefficients are none,\n",
    "    # then initalize a random set of coefficients\n",
    "    # that are equal to the feature length\n",
    "    if wInitial is None:\n",
    "        w = np.random.normal(size=featureLen)\n",
    "    \n",
    "    # otherwise, set the coefficients to be the \n",
    "    # coefficients provided by the user\n",
    "    else:\n",
    "        w = wInitial\n",
    "        \n",
    "    # loop through the number of iterations \n",
    "    # specified by the user\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # broadcast the coefficients so that \n",
    "        # we can hold the coefficients in memory\n",
    "        wBroadcast = sc.broadcast(w)\n",
    "        \n",
    "        # calculate the gradient for each point\n",
    "        # and then reduce by summing all the \n",
    "        # partial gradients \n",
    "        \n",
    "        # the partial gradient is 1 calculated as \n",
    "        # using the formula here\n",
    "        # https://work.caltech.edu/library/093.pdf\n",
    "        partial_gradient = data.map(lambda point: (1 / \\\n",
    "        (1 + np.exp(-point.y*np.dot(wBroadcast.value, point.x)))-1) * \\\n",
    "                                    point.y * np.array(point.x))\n",
    "        \n",
    "        gradient = partial_gradient.reduce(lambda a, b: a + b)\n",
    "        \n",
    "        # if the regularization type is 'ridge'\n",
    "        # then use the \n",
    "        if regType == \"Ridge\":\n",
    "            \n",
    "            # multiply the weight vector by 1 to make\n",
    "            # a copy of the array\n",
    "            wReg = w * 1\n",
    "            \n",
    "            # ignore the last value of weight vector\n",
    "            # because it is the bias term, \n",
    "            # ignored in regularization\n",
    "            wReg[-1] = 0 \n",
    "            \n",
    "        # else if we're using a 'lasso' regularization\n",
    "        # type\n",
    "        elif regType == \"Lasso\":\n",
    "            \n",
    "            # multiply the weight vector by 1 to make \n",
    "            # a copy of the array\n",
    "            wReg = w * 1\n",
    "            \n",
    "            # ignore the last value of coefficient vector\n",
    "            # because it is the bias term, \n",
    "            # ignored in regularization\n",
    "            wReg[-1] = 0 \n",
    "            \n",
    "            # if the coefficient is greater than 0,\n",
    "            # make it 1, otherwise make it zero\n",
    "            wReg = (wReg>0).astype(int) * 2-1\n",
    "        \n",
    "        # if no regularlization type is provided,\n",
    "        # just don't regularize at all\n",
    "        else:\n",
    "            wReg = np.zeros(w.shape[0])\n",
    "            \n",
    "        # set the gradient equal to itself plus\n",
    "        # the regularization parameter times the\n",
    "        # current weights\n",
    "        # gradient:  gradient of Sqaured Error +\n",
    "        # gradient of regularized term \n",
    "        gradient = gradient + regParam * wReg\n",
    "        \n",
    "        # the new set of coefficients is the \n",
    "        # current set of coefficients minus\n",
    "        # the gradient times the learning rate\n",
    "        # all over the number of exmaples\n",
    "        w = w - learningRate * gradient / n\n",
    "    \n",
    "    # return the coefficients\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homebrew logistic regression with 100 iterations:\n",
      "[-0.10165159  0.55552201 -0.33177561]\n"
     ]
    }
   ],
   "source": [
    "# test it out by running a simple\n",
    "# regression and getting the weights\n",
    "print \"Homebrew logistic regression with 100 iterations:\"\n",
    "print logisticRegressionGDUW(train,regType='Lasso',iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown Logistic Regression Function (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logisticRegressionGDW(data, wInitial=None, learningRate=0.05, iterations=50, regParam=0.01, regType=None):\n",
    "    \"\"\"Logistic Regression function that takes as input\n",
    "    the data, an initial set of weights, a learning rate,\n",
    "    a number of iterations, a regularization hyperparameter,\n",
    "    and the type of regularization.\n",
    "    It outputs the coefficients for the model.\"\"\"\n",
    "    \n",
    "    # set the feature length by taking the length\n",
    "    # of the features in the first point\n",
    "    featureLen = len(data.take(1)[0].x)\n",
    "    \n",
    "    # instead of counting the number of datapoints\n",
    "    # sum all the weights\n",
    "    n = data.map(lambda point: point.wgt).reduce(lambda a,b: a + b)\n",
    "    \n",
    "    # if the initial coefficients are none,\n",
    "    # then initalize a random set of coefficients\n",
    "    # that are equal to the feature length\n",
    "    if wInitial is None:\n",
    "        w = np.random.normal(size=featureLen)\n",
    "    \n",
    "    # otherwise, set the coefficients to be the \n",
    "    # coefficients provided by the user\n",
    "    else:\n",
    "        w = wInitial\n",
    "        \n",
    "    # loop through the number of iterations \n",
    "    # specified by the user\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # broadcast the coefficients so that \n",
    "        # we can hold the coefficients in memory\n",
    "        wBroadcast = sc.broadcast(w)\n",
    "        \n",
    "        # calculate the gradient for each point\n",
    "        # and then reduce by summing all the \n",
    "        # partial gradients \n",
    "        \n",
    "        # the partial gradient is 1 calculated as \n",
    "        # using the formula here\n",
    "        # https://work.caltech.edu/library/093.pdf\n",
    "        \n",
    "        # we update the formula to add a term\n",
    "        # to weight each point\n",
    "        partial_gradient = data.map(lambda point: (((1 / \\\n",
    "        (1 + np.exp(-point.y*np.dot(wBroadcast.value, point.x)))-1) * \\\n",
    "                                    point.y * np.array(point.x))) * \\\n",
    "                                   point.wgt)\n",
    "        # we calculate the total gradient by summing across\n",
    "        # the partial gradients\n",
    "        gradient = partial_gradient.reduce(lambda a, b: a + b)\n",
    "        \n",
    "        # if the regularization type is 'ridge'\n",
    "        # then use the \n",
    "        if regType == \"Ridge\":\n",
    "            \n",
    "            # multiply the weight vector by 1 to make\n",
    "            # a copy of the array\n",
    "            wReg = w * 1\n",
    "            \n",
    "            # ignore the last value of weight vector\n",
    "            # because it is the bias term, \n",
    "            # ignored in regularization\n",
    "            wReg[-1] = 0 \n",
    "            \n",
    "        # else if we're using a 'lasso' regularization\n",
    "        # type\n",
    "        elif regType == \"Lasso\":\n",
    "            \n",
    "            # multiply the weight vector by 1 to make \n",
    "            # a copy of the array\n",
    "            wReg = w * 1\n",
    "            \n",
    "            # ignore the last value of coefficient vector\n",
    "            # because it is the bias term, \n",
    "            # ignored in regularization\n",
    "            wReg[-1] = 0 \n",
    "            \n",
    "            # if the coefficient is greater than 0,\n",
    "            # make it 1, otherwise make it zero\n",
    "            wReg = (wReg>0).astype(int) * 2-1\n",
    "        \n",
    "        # if no regularlization type is provided,\n",
    "        # just don't regularize at all\n",
    "        else:\n",
    "            wReg = np.zeros(w.shape[0])\n",
    "            \n",
    "        # set the gradient equal to itself plus\n",
    "        # the regularization parameter times the\n",
    "        # current weights\n",
    "        # gradient:  gradient of Sqaured Error +\n",
    "        # gradient of regularized term \n",
    "        gradient = gradient + regParam * wReg\n",
    "        \n",
    "        # the new set of coefficients is the \n",
    "        # current set of coefficients minus\n",
    "        # the gradient times the learning rate\n",
    "        # all over the number of exmaples\n",
    "        w = w - learningRate * gradient / n\n",
    "    \n",
    "    # return the coefficients\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to help compute the accuracy of the home grown model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracyHome(coefficients,test,thresh=0.5): \n",
    "    \"\"\"a simple function that takes as inputs,\n",
    "    the coefficients calculated by a logistic\n",
    "    regression model, and the test data, and \n",
    "    uses both to compute the accuracy of the\n",
    "    model\"\"\"\n",
    "    \n",
    "    # keep the count of correct and the total\n",
    "    # count\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # write a subfunction to calculate the\n",
    "    # the actual predictions\n",
    "    def predictLog(point):\n",
    "        \n",
    "        # get the prediction from the log odds by \n",
    "        # converting the log-odds back to calculate\n",
    "        # the prediction\n",
    "        prediction = 1 / (1+np.exp(-np.dot(coefficients,point)))\n",
    "        \n",
    "        # use the threshold to establish the prediction\n",
    "        predict = -1.0\n",
    "        if prediction > 0.5:\n",
    "            predict = 1.0\n",
    "        \n",
    "        # return the prediction\n",
    "        return predict\n",
    "    \n",
    "    # loop through each test example\n",
    "    for i in test: \n",
    "        \n",
    "        # get the x (features) from the point\n",
    "        # and append a 1 for the bias term\n",
    "        x = list(i[0:2])\n",
    "        x.append(1.0)\n",
    "        \n",
    "        # get the prediction for this point\n",
    "        predict = predictLog(x)\n",
    "        \n",
    "        # if the prediction is right, then \n",
    "        # increment correct. always increment \n",
    "        # total\n",
    "        total = total + 1\n",
    "        if predict == i[2]:\n",
    "            correct = correct + 1\n",
    "    \n",
    "    # return the accuracy\n",
    "    accur = float(correct) / float(total)\n",
    "    return accur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the homegrown model for many iterations and look for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Accuracy: 0.87\n",
      "Coefficients: [ 0.13130022  1.39845874 -0.07275296]\n",
      "\n",
      "\n",
      "Iteration 1\n",
      "Accuracy: 0.14\n",
      "Coefficients: [ 0.56990619  0.03133693  0.12013405]\n",
      "\n",
      "\n",
      "Iteration 2\n",
      "Accuracy: 0.52\n",
      "Coefficients: [-0.5228003  -0.15189114  1.25087885]\n",
      "\n",
      "\n",
      "Iteration 3\n",
      "Accuracy: 0.35\n",
      "Coefficients: [ 1.12528197 -0.74934186 -0.98882026]\n",
      "\n",
      "\n",
      "Iteration 4\n",
      "Accuracy: 0.52\n",
      "Coefficients: [ 1.10565593 -0.43907647  1.52753988]\n",
      "\n",
      "\n",
      "Iteration 5\n",
      "Accuracy: 0.44\n",
      "Coefficients: [-0.42033692 -0.58857714 -0.26308081]\n",
      "\n",
      "\n",
      "Iteration 10\n",
      "Accuracy: 0.59\n",
      "Coefficients: [-0.88010529 -0.19830175 -0.61124347]\n",
      "\n",
      "\n",
      "Iteration 20\n",
      "Accuracy: 0.65\n",
      "Coefficients: [-0.14017454  0.98315338 -0.77679797]\n",
      "\n",
      "\n",
      "Iteration 35\n",
      "Accuracy: 0.88\n",
      "Coefficients: [-1.28399497  0.17235407  0.28482233]\n",
      "\n",
      "\n",
      "Iteration 60\n",
      "Accuracy: 0.85\n",
      "Coefficients: [ 0.2019071   2.33761949  0.17697976]\n",
      "\n",
      "\n",
      "Iteration 100\n",
      "Accuracy: 0.84\n",
      "Coefficients: [-1.91936521 -0.53283814 -0.11461893]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the iterations of interest for plotting\n",
    "interest = [0,1,2,3,4,5,10,20,35,60,100]\n",
    "\n",
    "# create an array to store the accuracies\n",
    "accuracies = []\n",
    "\n",
    "# loop through all the intersting iterations\n",
    "for i in interest:\n",
    "    \n",
    "    # grab the coefficients\n",
    "    coefficients = logisticRegressionGDW(train, regType='Lasso',iterations=i)\n",
    "    \n",
    "    # compute the accuracy\n",
    "    accur = accuracyHome(coefficients,test_nl)\n",
    "    \n",
    "    # add the accuracy to the list\n",
    "    accuracies.append(accur)\n",
    "    \n",
    "    # print out the accuracy and the iteration\n",
    "    print \"Iteration\",i\n",
    "    print \"Accuracy:\",accur\n",
    "    print \"Coefficients:\",coefficients\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x/YaQNo7CECFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95701fcd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the evolving accuracies\n",
    "plt.plot(interest,accuracies)\n",
    "plt.title(\"Accuracy for Homegrown Logistic Regression Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear that my coefficients are **converging**, even though my accuracies improve drastically with more iterations (before stabilizing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark machine learning library does not have a native way to weight examples in Python. There does appear to be an [option](https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/mllib/optimization/L1Updater.html) to use weights when using Spark in Scala. However, I could find no such option in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\">HW11.4 SVMs</span>\n",
    "<span style=\"color:gray\">Use the non-linearly separable training and testing datasets from HW11.3 in this problem.\n",
    "<br><br>\n",
    "Use the non-linearly separable training and testing datasets from HW11.3 in this problem.\n",
    "<br><br>\n",
    "Using MLLib train up a soft SVM model with the training dataset and evaluate with the testing set. What is a good number of iterations for training the SVM model? Justify with plots and words.\n",
    "<br><br>\n",
    "HW11.4.1 [Optional] Derive and Implement in Spark a weighted hard linear svm classification learning algorithm. Feel free to use the following notebook as a starting point SVM Notebook.<br>\n",
    "Evaluate your homegrown weighted linear svm classification learning algorithm on the weighted training dataset and test dataset from HW11.3 (linearly separable dataset). Report misclassification error (1 - Accuracy) and how many iterations does it took to converge? How many support vectors do you end up with?<br>\n",
    "Does Spark MLLib have a weighted soft SVM learner. If so use it and report your findings on the weighted training set and test set.\n",
    "<br><br>\n",
    "HW11.4.2 [Optional] Repeat HW11.4.2 using a soft SVM and a nonlinearly separable datasets. Compare the error rates that you get here with the error rates you achieve using MLLib's soft SVM. Report the number of support vectors in both cases (may not be available the MLLib implementation).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write function to turn data into Spark's labelled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(line):\n",
    "    \"\"\"takes each line in the format [x,y,label] and \n",
    "    converts it to Spark labelled points RDD\"\"\"\n",
    "    \n",
    "    # grab the values from the line\n",
    "    values = [float(x) for x in line.split(',')]\n",
    "    \n",
    "    # if the classification is -1, then\n",
    "    # convert it to zero\n",
    "    if values[2] == -1:\n",
    "        values[2] = 0\n",
    "    \n",
    "    return LabeledPoint(values[2], values[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data to a format readable by Spark's library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [-0.666776511004,0.629922738339]), LabeledPoint(0.0, [0.327655008854,-0.566204527638]), LabeledPoint(1.0, [-0.20250519138,0.0516129445782]), LabeledPoint(1.0, [0.729045899887,-0.421574894025]), LabeledPoint(0.0, [-0.343943255331,0.758416700556])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert the data to an RDD\n",
    "trainRDD = sc.textFile('train_nl.csv')\n",
    "\n",
    "# turn the data into labelled points\n",
    "trainRDD = trainRDD.map(parsePoint).cache()\n",
    "\n",
    "# print out a sample of what we've done\n",
    "print trainRDD.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SVM model using Spark's library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Misclassification: 0.52\n",
      "Coefficients: [0.0,0.0]\n",
      "\n",
      "\n",
      "Iteration 1\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-0.447709021287,0.397602060893]\n",
      "\n",
      "\n",
      "Iteration 2\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-0.764287106238,0.678749174364]\n",
      "\n",
      "\n",
      "Iteration 3\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-0.922558334871,0.81955765388]\n",
      "\n",
      "\n",
      "Iteration 4\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.01043935566,0.897405969077]\n",
      "\n",
      "\n",
      "Iteration 5\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.07440503792,0.952233063804]\n",
      "\n",
      "\n",
      "Iteration 10\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.26227188448,1.10911905912]\n",
      "\n",
      "\n",
      "Iteration 20\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.4094900117,1.25685853017]\n",
      "\n",
      "\n",
      "Iteration 35\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.51991317389,1.37253485294]\n",
      "\n",
      "\n",
      "Iteration 60\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.63457455404,1.44165575892]\n",
      "\n",
      "\n",
      "Iteration 75\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.66839254922,1.46713553171]\n",
      "\n",
      "\n",
      "Iteration 90\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.67152715048,1.46975541981]\n",
      "\n",
      "\n",
      "Iteration 100\n",
      "Misclassification: 0.08\n",
      "Coefficients: [-1.67152715048,1.46975541981]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "# some test data that we can use based on the documentation\n",
    "#data = sc.parallelize([LabeledPoint(0.0, [0.0, 1.0]),LabeledPoint(1.0, [1.0, 0.0])])\n",
    "\n",
    "# set the iterations of interest for plotting\n",
    "interest = [0,1,2,3,4,5,10,20,35,60,75,90,100]\n",
    "\n",
    "# create an array to store the accuracies\n",
    "accuracies = []\n",
    "\n",
    "# loop through all the intersting iterations\n",
    "for i in interest:\n",
    "    \n",
    "    # set the model\n",
    "    model = SVMWithSGD.train(trainRDD,regType='l1',iterations=i)\n",
    "    \n",
    "    # compute the accuracy\n",
    "    accur = accuracy(model,test_nl)\n",
    "    \n",
    "    # append the accuracy to accuracies\n",
    "    accuracies.append(accur)\n",
    "    \n",
    "    # compute the misclassification rate\n",
    "    misclas = 1.0 - accur\n",
    "    \n",
    "    # print out the accuracy and the iteration\n",
    "    print \"Iteration\",i\n",
    "    print \"Misclassification:\",misclas\n",
    "    print \"Coefficients:\",model.weights\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6TGGfSS1DiDXpIaZ9BLUuMMeklq3P8H2KFF+6MEm/8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95704c8c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the evolving accuracies\n",
    "plt.plot(interest,accuracies)\n",
    "plt.title(\"Accuracy for SVM Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:teal\">We can see that after even a single iteration, we have stabilized in terms of accuracy at around 92%. However, the coefficients don't actually converge till about 90 iterations.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:sienna\">Optional: Write our own homegrown version of SVM (unweighted)</span>\n",
    "We borrow from the [sample notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/dm2l73iznde7y4f/SVM-Notebook-Linear-Kernel-2015-06-19.ipynb) and start with an unweighted implementation before moving to a fully weighted implementation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# gradient descent (and with NO stochasticity!)\n",
    "# Objective Function\n",
    "# minw   λ/2   w’w   +   1/m Σi(1   –    yi(w’xi – b))+\n",
    "# gradient \n",
    "#   λw               if      yi(w’xi – b))  > 1    #correctly classified\n",
    "#   λw  + yi xi      Otherwise                    #incorrectly classified\n",
    "#--------------------------------\n",
    "# Wt+1  = wt + average(gradient)\n",
    "# Wt+1  = wt + average(regularization + hinge loss)\n",
    "def SVM_GDUW(data,w=None,eta=0.01,iter_num=1000,regPara=0.01,stopCriteria=0.0001):\n",
    "    \"\"\"a function that produces a support vector machine\n",
    "    from an inputted set of data,\n",
    "    an optional initial set of coefficients, a learning\n",
    "    rate (eta), a number of iterations, a regularization\n",
    "    parameter and a stopping criteria\"\"\"\n",
    "\n",
    "    # this line of code prepends the classification\n",
    "    # to the data. however, for our data, its already\n",
    "    # all combined so we comment out this line\n",
    "    # dataRDD = sc.parallelize(np.append(y[:,None],data,axis=1)).cache() #prepend y to X\n",
    "    \n",
    "    # grab the feature length\n",
    "    featureLen = len(data.take(1)[0].x)\n",
    "    \n",
    "    # set the number of data points\n",
    "    n = data.count()\n",
    "    \n",
    "    # if the user did not provide an initial set\n",
    "    # of coefficients, then we generate a random\n",
    "    # set\n",
    "    if w==None:\n",
    "        w = np.random.normal(size=featureLen)\n",
    "        \n",
    "    # loop through all the iterations\n",
    "    for i in range(iter_num):       #label * margin\n",
    "        \n",
    "        # create the support vector by taking the\n",
    "        # classification and multiplying it by the\n",
    "        # dot product of the weights and the x value\n",
    "        sv = data.filter(lambda point: point.y * np.dot(w,point.x)<1)\n",
    "        \n",
    "        # if the support vector is empty (that means\n",
    "        # that it has converged), then go ahead and\n",
    "        # break out\n",
    "        if sv.isEmpty(): \n",
    "            break       \n",
    "        \n",
    "        # set the gradient using the hing loss function\n",
    "        # to first elements multiplied by the rest of\n",
    "        # the elements and all summed together\n",
    "        g = -sv.map(lambda point: point.y*np.array(point.x)).reduce(lambda x,y:x+y)/n\n",
    "        \n",
    "        # make a copy of the coefficients\n",
    "        wreg = w*1\n",
    "        \n",
    "        # set the bias term to 0 so that we\n",
    "        # ignore it during regularization\n",
    "        wreg[-1] = 0 \n",
    "        \n",
    "        # calculate the change in the \n",
    "        # gradient as the learning rate\n",
    "        # times the gradient plus the \n",
    "        # the regularization parameter \n",
    "        # times the new coefficients\n",
    "        wdelta = eta*(g+regPara*wreg)\n",
    "        \n",
    "        # if the amount of change we're\n",
    "        # about to use is less than the\n",
    "        # stopping criteria that we've\n",
    "        # set, then we we break out of\n",
    "        # the loop and stop doing any\n",
    "        # further iterations\n",
    "        if sum(abs(wdelta))<=stopCriteria*sum(abs(w)):\n",
    "            break\n",
    "        \n",
    "        # otherwise, if we're still going,\n",
    "        # go ahead and update the coefficients\n",
    "        w = w - wdelta\n",
    "        \n",
    "    # return the coefficients\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20659619, -1.07756549, -1.71936608])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it out\n",
    "SVM_GDUW(train,iter_num=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:sienna\">Optional: Write our own homegrown version of SVM (weighted)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# gradient descent (and with NO stochasticity!)\n",
    "# Objective Function\n",
    "# minw   λ/2   w’w   +   1/m Σi(1   –    yi(w’xi – b))+\n",
    "# gradient \n",
    "#   λw               if      yi(w’xi – b))  > 1    #correctly classified\n",
    "#   λw  + yi xi      Otherwise                    #incorrectly classified\n",
    "#--------------------------------\n",
    "# Wt+1  = wt + average(gradient)\n",
    "# Wt+1  = wt + average(regularization + hinge loss)\n",
    "def SVM_GDW(data,w=None,eta=0.01,iter_num=1000,regPara=0.01,stopCriteria=0.0001):\n",
    "    \"\"\"a function that produces a support vector machine\n",
    "    from an inputted set of data,\n",
    "    an optional initial set of coefficients, a learning\n",
    "    rate (eta), a number of iterations, a regularization\n",
    "    parameter and a stopping criteria\"\"\"\n",
    "\n",
    "    # this line of code prepends the classification\n",
    "    # to the data. however, for our data, its already\n",
    "    # all combined so we comment out this line\n",
    "    # dataRDD = sc.parallelize(np.append(y[:,None],data,axis=1)).cache() #prepend y to X\n",
    "    \n",
    "    # grab the feature length\n",
    "    featureLen = len(data.take(1)[0].x)\n",
    "    \n",
    "    # instead of counting the number of datapoints\n",
    "    # sum all the weights\n",
    "    n = data.map(lambda point: point.wgt).reduce(lambda a,b: a + b)\n",
    "    \n",
    "    # if the user did not provide an initial set\n",
    "    # of coefficients, then we generate a random\n",
    "    # set\n",
    "    if w==None:\n",
    "        w = np.random.normal(size=featureLen)\n",
    "        \n",
    "    # loop through all the iterations\n",
    "    for i in range(iter_num):       #label * margin\n",
    "        \n",
    "        # create the support vector by taking the\n",
    "        # classification and multiplying it by the\n",
    "        # dot product of the weights and the x value\n",
    "        sv = data.filter(lambda point: point.y * np.dot(w,point.x)<1)\n",
    "        \n",
    "        # if the support vector is empty (that means\n",
    "        # that it has converged), then go ahead and\n",
    "        # break out\n",
    "        if sv.isEmpty(): \n",
    "            break       \n",
    "        \n",
    "        # set the gradient using the hinge loss function\n",
    "        # to first elements multiplied by the rest of\n",
    "        # the elements and all summed together\n",
    "        g = -sv.map(lambda point: point.y*np.array(point.x)*point.wgt).reduce(lambda x,y:x+y)/n\n",
    "        \n",
    "        # make a copy of the coefficients\n",
    "        wreg = w*1\n",
    "        \n",
    "        # set the bias term to 0 so that we\n",
    "        # ignore it during regularization\n",
    "        wreg[-1] = 0 \n",
    "        \n",
    "        # calculate the change in the \n",
    "        # gradient as the learning rate\n",
    "        # times the gradient plus the \n",
    "        # the regularization parameter \n",
    "        # times the new coefficients\n",
    "        wdelta = eta*(g+regPara*wreg)\n",
    "        \n",
    "        # if the amount of change we're\n",
    "        # about to use is less than the\n",
    "        # stopping criteria that we've\n",
    "        # set, then we we break out of\n",
    "        # the loop and stop doing any\n",
    "        # further iterations\n",
    "        if sum(abs(wdelta))<=stopCriteria*sum(abs(w)):\n",
    "            break\n",
    "        \n",
    "        # otherwise, if we're still going,\n",
    "        # go ahead and update the coefficients\n",
    "        w = w - wdelta\n",
    "        \n",
    "    # return the coefficients\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74760737, -1.78180941, -0.93883008])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it out\n",
    "SVM_GDW(train,iter_num=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to compute the accuracy of our homegrown SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracySVM(coefficients,test,thresh=0.5):\n",
    "    \"\"\"a simple function that takes the coefficients\n",
    "    of a support vector machine and uses them\n",
    "    to predict the classifications of test data\"\"\"\n",
    "    \n",
    "    # set up counters for the total and the\n",
    "    # number of correct examples\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # sub functiont that predicts \n",
    "    # each individual point\n",
    "    def prediction(point):\n",
    "        prod = np.dot(point,coefficients)\n",
    "        \n",
    "        # compare it to \n",
    "        predict = -1.0\n",
    "        if prod > 0.5: \n",
    "            predict = 1.0\n",
    "        \n",
    "        # return the class that we predicted\n",
    "        return predict\n",
    "    \n",
    "    # loop through each test example\n",
    "    for i in test: \n",
    "        \n",
    "        # get the x (features) from the point\n",
    "        # and append a 1 for the bias term\n",
    "        x = list(i[0:2])\n",
    "        x.append(1.0)\n",
    "        \n",
    "        # get the prediction for this point\n",
    "        predict = prediction(x)\n",
    "        \n",
    "        # if the prediction is right, then \n",
    "        # increment correct. always increment \n",
    "        # total\n",
    "        total = total + 1\n",
    "        if predict == i[2]:\n",
    "            correct = correct + 1\n",
    "        \n",
    "    # compute the actual accuracy\n",
    "    accur = float(correct)/float(total)\n",
    "    \n",
    "    # return the accuracy\n",
    "    return accur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our homegrown model for a number of iterations to test convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Accuracy: 0.48\n",
      "Coefficients: [-0.74759883 -0.07484864 -1.07809815]\n",
      "\n",
      "\n",
      "Iteration 1\n",
      "Accuracy: 0.68\n",
      "Coefficients: [ 0.81155744  2.61991941 -0.38742666]\n",
      "\n",
      "\n",
      "Iteration 2\n",
      "Accuracy: 0.21\n",
      "Coefficients: [ 0.04803298 -1.9637259  -0.19864101]\n",
      "\n",
      "\n",
      "Iteration 3\n",
      "Accuracy: 0.25\n",
      "Coefficients: [ 0.76904408  0.03922179  0.7571654 ]\n",
      "\n",
      "\n",
      "Iteration 4\n",
      "Accuracy: 0.48\n",
      "Coefficients: [-0.0823882  -0.10741338 -0.90256635]\n",
      "\n",
      "\n",
      "Iteration 5\n",
      "Accuracy: 0.91\n",
      "Coefficients: [-0.83038039  0.49219454  0.63233672]\n",
      "\n",
      "\n",
      "Iteration 10\n",
      "Accuracy: 0.52\n",
      "Coefficients: [-0.14639041  0.53860456  1.29556984]\n",
      "\n",
      "\n",
      "Iteration 20\n",
      "Accuracy: 0.79\n",
      "Coefficients: [-0.38940474  0.59443796  0.89203248]\n",
      "\n",
      "\n",
      "Iteration 35\n",
      "Accuracy: 0.48\n",
      "Coefficients: [-0.31948492 -0.52346544 -1.11545924]\n",
      "\n",
      "\n",
      "Iteration 60\n",
      "Accuracy: 0.48\n",
      "Coefficients: [-0.29943391  1.6402978  -1.37812231]\n",
      "\n",
      "\n",
      "Iteration 100\n",
      "Accuracy: 0.67\n",
      "Coefficients: [-1.25080583 -0.56100595 -0.10016891]\n",
      "\n",
      "\n",
      "Iteration 250\n",
      "Accuracy: 0.73\n",
      "Coefficients: [-1.18131614  0.45598356 -0.30530493]\n",
      "\n",
      "\n",
      "Iteration 500\n",
      "Accuracy: 0.87\n",
      "Coefficients: [-1.74355006  0.09984     0.22732044]\n",
      "\n",
      "\n",
      "Iteration 1000\n",
      "Accuracy: 0.9\n",
      "Coefficients: [-0.93350784  1.82832282  0.07988053]\n",
      "\n",
      "\n",
      "Iteration 1500\n",
      "Accuracy: 0.9\n",
      "Coefficients: [-1.44206363  1.22644465  0.07012011]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the iterations of interest for plotting\n",
    "interest = [0,1,2,3,4,5,10,20,35,60,100, 250, 500, 1000, 1500]\n",
    "\n",
    "# create an array to store the accuracies\n",
    "accuracies = []\n",
    "\n",
    "# loop through all the intersting iterations\n",
    "for i in interest:\n",
    "    \n",
    "    # grab the coefficients\n",
    "    coefficients = SVM_GDW(train,iter_num=i)\n",
    "    \n",
    "    # compute the accuracy\n",
    "    accur = accuracySVM(coefficients,test_nl)\n",
    "    \n",
    "    # add the accuracy to the list\n",
    "    accuracies.append(accur)\n",
    "    \n",
    "    # print out the accuracy and the iteration\n",
    "    print \"Iteration\",i\n",
    "    print \"Accuracy:\",accur\n",
    "    print \"Coefficients:\",coefficients\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95704d9690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the evolving accuracies\n",
    "plt.plot(interest,accuracies)\n",
    "plt.title(\"Accuracy for Homegrown Logistic Regression Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we see an increasing level of accuracy (that stabilizes eventually), we *unfortunately* don't see the convergence of the weights for the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
