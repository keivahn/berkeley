{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \\#1\n",
    "**Student: Alex Smith** <br>\n",
    "Course: W261 - Machine Learning at Scale <br>\n",
    "Professor: Jimi Shanahan <br>\n",
    "Due Date: May 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Useful resources\n",
    "When completing this notebook, I found the following resources particularly useful:\n",
    "- [Model Selection: Underfitting, Overfitting, and the Bias-Variance Tradeoff](https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/)\n",
    "- [Bayes' Theorem: statement of theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem#Statement_of_theorem)\n",
    "- [An introduction to information retrieval, chapter 13](http://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)\n",
    "- [SKLearn's Explanation for Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "The following libraries must be installed before running the below code. They can all be installed through [Pip](https://github.com/pypa/pip).\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "- [Regular Expression](https://docs.python.org/2/library/re.html)\n",
    "- [Pretty Table](https://pypi.python.org/pypi/PrettyTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW0.0 \n",
    "*Prepare your bio and include it in this HW submission. Please limit to 100 words. Count the words in your bio and print the length of your bio (in terms of words) in a separate cell.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing my undergrad in Middle Eastern Studies, I moved to Atlanta to be part of a management program at an industrial supply company, McMaster-Carr. I’m currently the Billings and control manager for the branch. I had never really analyzed data before starting work at McMaster. At McMaster, I got to do some really fun data analysis and build some really nifty models in Excel (more business intelligence than data science). I realized that I find playing around with data to be pretty fun. I decided that I want to explore this more and found this program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to count the number of words in my bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bioWordCount(text,limit=100):\n",
    "    \"\"\"a word count function that counts the \n",
    "    number of words in the inputed text and\n",
    "    compares it a limit, by default set to 100\"\"\"\n",
    "    \n",
    "    # calculate the length of my bio by \n",
    "    # splitting the words using python's native split\n",
    "    # function which splits by spaces\n",
    "    text_length = len(text.split())\n",
    "    \n",
    "    # if the text length is greater than the \n",
    "    # number of words allowed, warn the user\n",
    "    # and print out the length of the text\n",
    "    if text_length > limit:\n",
    "        \n",
    "        return \"Uh-oh, my bio has more words than allowed. \\\n",
    "        My bio has \" + str(text_length) + \" words, but \\\n",
    "        I am only allowed \" + str(limit) + \" words.\"\n",
    "    \n",
    "    # else if the text length is less than \n",
    "    # or equal to the limit, tell the user how\n",
    "    # many they words they used\n",
    "    else:\n",
    "        \n",
    "        return \"The length of my bio is \" + \\\n",
    "    str(text_length) + \" which is \" + \\\n",
    "    str(limit-text_length) + \\\n",
    "        \" less than the bio limit of \" + str(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of my bio is 97 which is 3 less than the bio limit of 100\n"
     ]
    }
   ],
   "source": [
    "# write my bio\n",
    "bio_my = \"\"\"After completing my undergrad in Middle Eastern Studies, \n",
    "I moved to Atlanta to be part of a management program at an \n",
    "industrial supply company, McMaster-Carr. I’m currently the Billings \n",
    "and control manager for the branch. I had never really analyzed \n",
    "data before starting work at McMaster. At McMaster, I got to \n",
    "do some really fun data analysis and build some really nifty \n",
    "models in Excel (more business intelligence than data \n",
    "science). I realized that I find playing around with data \n",
    "to be pretty fun. I decided that I want to explore this \n",
    "more and found this program.\"\"\"\n",
    "\n",
    "# print out my word count using the function defined in the cell above\n",
    "print bioWordCount(bio_my)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.0.0.\n",
    "*Define big data. Provide an example of a big data problem in your domain of expertise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful framework to think about big data is the 3 V's: volume, velocity, and variety. In layman's terms, this means data is huge, comes very quickly, and is composed of different types. To define these V's more specifically, it is useful to think of a standard laptop with 1 TB hard drive and 8-16 GB of memory. Big data would be data for which it would simply be impractical to use a single, standard laptop.<br>\n",
    "1. **Volume**: We cannnot hold more than 1 TB of data on the drive, and so analyzing anything larger than this becomes an impossibility. Rather than thinking about terabytes, we think of big data in zettabytes. \n",
    "1. **Velocity**: To read a whole terabyte of data on our standard laptop would require three hours. This makes it impractical to analyze data generated in real time. Rather than thinking about data as batches or dumps, big data requires us to think of streaming data that must be analyzed in real time. \n",
    "1. **Variety**: Previous to concept of big data, we might think of data as a table. All of the data would be homogeneous and standardized. Big data instead requires us to think about data as a compliation of different types of information. For example, we might combine texts of tweets with geolocation with pictures posted to facebook at that same location. \n",
    "\n",
    "As a side note, IBM also mentions a fourth V of big data: **veracity**. Big data analysis must sometimes deal with data of suspect quality. A big data analyst must make decisions on how to handle data of poor quality (e.g. missing records, inaccuracies). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.0.1.  Bias Variance\n",
    "*In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any model is going to two types of error, **reducible** and **irreducible** error, when attempting to estimate some true funtion. We can think of the **irreducible error** as the error that arise from noise that surrounds that surrounds the true function. As its name implies, we cannot minimize or remove this error because the noise varies randomly from the true function. In the image below, we can see the true function as the solid black line and the points generated with random noise as the white circles close to the line.<br>\n",
    "<img src=\"https://theclevermachine.files.wordpress.com/2013/04/bias-variance-f_x-y.png\" width=\"400\">\n",
    "*Image source: dustinstansbury. \"Model Selection: Underfitting, Overfitting, and the Bias-Variance Tradeoff.\" The Clever Machine. (21 April 2013)*\n",
    "<br>\n",
    "<br>\n",
    "On the other hand, we have **reducible error**. This type of error we want to minimize. Reducible error is further broken up into bias and variance. We can think of **bias** as the difference between the estimate of a model and the true value. For example, a model will have higher bias if the estimates it produces are further from the true values than another model. We measure the bias as the *error due to squared bias* which is the difference between the model's predicted value and the true value over the training data. In the image below, we can see the model as the red lines and the true function as the black line. For most points, these models have very high bias (the red lines are not close to the true values demarcted by the black line).<br>\n",
    "<img src=\"https://theclevermachine.files.wordpress.com/2013/04/bias-variance-simulation1.png\" width=\"400\">\n",
    "*Image source: dustinstansbury, 2013.*\n",
    "<br>\n",
    "<br>\n",
    "The second part of the reducible error is the **variance**. We can think of variance as the difference that results from running multiple models from samples of the same data. A model would have higher variance if multiple iterations of the model over different samples of the training data are widely different. We measure the variance as the *error due to variance* which is amount by which the prediction from one training sets differs from the predictions of all other training sets. In the image below, we can see the model as purple lines and the true function as the black line. The lines are very inconsistent and appear to wildly fluctuate to match the specific sample of training data that was pulled. This is a clear case of *overfitting* the model to a particular training set.<br>\n",
    "<img src=\"https://theclevermachine.files.wordpress.com/2013/04/bias-variance-simulation10.png\" width=\"400\">\n",
    "*Image source: dustinstansbury, 2013.*\n",
    "<br>\n",
    "<br>\n",
    "When using polynomial models of increasing degrees, I would divide up my data in multiple training sets and a test set. I would then run multiple iterations of each polynomial degree model on the training sets. I would test each model on the test sets and calculate the error due to squared bias and the error due to variance as defined in the preceding paragraphs. I would expect to find that increasing the complexity of my model increases my error due to variance but decreases my error due to squared bias. I would choose the model with the least total error. For an example plot that shows the relationship between the error due to variance and the error due to squared bias with the model complexity, see the image below: <br>\n",
    "<img src=\"https://theclevermachine.files.wordpress.com/2013/04/bias-variance-tradeoff.png\" width=\"400\">\n",
    "*Image source: dustinstansbury, 2013.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### HW1.1. \n",
    "*Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a  \"done\" (print \"done\") string will suffice here. (dont forget to include the Question Number and the question in the cell as a markdown multiline comment!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the shell script that runs our mappers and reducers\n",
    "Author: Jake Williams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, \n",
    "##        e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of \n",
    "##               each command,\n",
    "##               and focus on how arguments are \n",
    "##               supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python\n",
    "##               scripts take input.\n",
    "##               When you are comfortable with the \n",
    "##               unix code below,\n",
    "##               answer the questions on the LMS for \n",
    "##               HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\"\n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\"\n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number \n",
    "## of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and \n",
    "    ## redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python \n",
    "## reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.2. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.*\n",
    "\n",
    "*To do so, make sure that*\n",
    "   - *mapper.py counts all occurrences of a single word, and*\n",
    "   - *reducer.py collates the counts of the single word.*\n",
    "\n",
    "*CROSSCHECK: <br>\n",
    "    grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l* <br>\n",
    "    *8 <br>\n",
    "    \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!*\n",
    "\n",
    "*Here, mapper.py will read in a portion (i.e., a single record corresponding to a row) of the email data,\n",
    "count the number of occurences of the word in question and print/emit a count to the output stream. While the utility of the reducer responsible for reading in  counts of the word and summarizing them before printing that summary to the output stream.*\n",
    "\n",
    "*See example in: http://nbviewer.ipython.org/urls/dl.dropbox.com/s/ujz9w7d2a73b80o/DivideAndConquer2-python-Incomplete.ipynb\n",
    "See video section 1.12.1 1.12.1 Poor Man's MapReduce Using Command Line (Part 2) located at: \n",
    "https://learn.datascience.berkeley.edu/mod/page/view.php?id=10961\n",
    "NOTE in your python notebook create a cell to save your mapper/reducer to disk using magic commands (see example here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "This function takes a text and given word and counts the occurences of that given word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments; initalize the count\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "# collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# open the file as readable\n",
    "with open (filename, \"r\") as myfile:\n",
    "    \n",
    "    # loop through each line in the file\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # convert the line into a list of words without punctuation\n",
    "        # also lowercase all the words\n",
    "        words = wordify.findall(line.lower())\n",
    "        \n",
    "        # loop through each word in each line\n",
    "        for word in words:\n",
    "            \n",
    "            # if the word equals the word we're searching for\n",
    "            # increment the counter by one\n",
    "            if word == findwords[0]:\n",
    "                count = count + 1\n",
    "\n",
    "print findwords[0], \"\\t\", count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function combines the multiple counts generated by the mapper function into a single count for all occurences of a given word in the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "# import the system libraries to read from the input and\n",
    "# initalize the sum variable and the find word\n",
    "import sys\n",
    "sums = 0\n",
    "findword = \"\"\n",
    "\n",
    "# grab and store the count files\n",
    "count_files = sys.argv[1:]\n",
    "\n",
    "# loop through each file in the list of count files\n",
    "for filename in count_files:\n",
    "    \n",
    "    # open the file as readable\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        \n",
    "        # loop through each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            \n",
    "            # split the line by tabs and take the \n",
    "            # second element (the count)\n",
    "            line = line.split(\"\\t\")\n",
    "            \n",
    "            # add the value found by each chunk to the sum\n",
    "            sums = sums + int(line[1])\n",
    "            \n",
    "            # if the find word has not been set yet, set it\n",
    "            if findword == \"\":\n",
    "                findword = line[0]\n",
    "\n",
    "print findword, \"\\t\", sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Magic functions to set the appropriate permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the right permissions for the python files mapper and reducer\n",
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modifies the permission to make the shell \n",
    "# command executable from the notebook\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run our counter and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word assistance occurs 10 times.\n"
     ]
    }
   ],
   "source": [
    "# run the Naive Bayes counter with the word assistance\n",
    "!./pNaiveBayes.sh 2 \"assistance\"\n",
    "\n",
    "# grab the output file and open it\n",
    "filename = \"enronemail_1h.txt.output\"\n",
    "with open(filename, \"r\") as myfile:\n",
    "    \n",
    "    # pull out the count\n",
    "    word_count = myfile.read().split()[1]\n",
    "\n",
    "# print out our solution\n",
    "print \"The word assistance occurs %s times.\" % word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.3. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that*\n",
    "   - *mapper.py and*\n",
    "   - *reducer.py*\n",
    "\n",
    "*that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:*\n",
    "\n",
    "- *the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM*\n",
    "\n",
    "*NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding Naive Bayes Classifier\n",
    "We want to build a Naive Bayes classifier that predicts the probability that some email is spam given the number of times some word (in this case, \"assistance\") occurs in the email. So that we can better understand this problem, let's go over some basic Bayesian probability and simplify our terms. Let A be the probability that an email is spam. Let B be the probability that an email has the word, assistance, in it. In essance, we want to find the probability of A given B, P(A|B). Using Bayes' theorem, we know that: \n",
    "$$P(A|B) = \\frac{P(B|A)\\cdot P(A)}{P(B)}$$\n",
    "We divide this up to see the variables we must calculate:\n",
    "- P(A|B) = probability of spam given \"assistance\" \n",
    "- P(B|A) = probability of \"assistance\" given spam (calculated as the number of times assistance occurs in spam labeled documents over the number of words in documents labeled spam)\n",
    "- P(A)   = probability of spam (calculated as the number of times an email is labeled spam over the total number of emails)\n",
    "- P(B)   = probability of assistance (calculated as the number of times assistances appears over the total number of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "The goal is for this function to output the information neccessary for the mapper to build the probabilities for the Naive Bayes classifier. This function loops through each word of each email in the text and outputs:\n",
    "- email id\n",
    "- word of email\n",
    "- spam indicator \n",
    "- word of interest indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# initalize the outputs that we'll be using\n",
    "email_id = None\n",
    "word_act = None\n",
    "spam_ind = None\n",
    "word_ind = None\n",
    "\n",
    "# collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# open the file as readable\n",
    "with open (filename, \"r\") as myfile:\n",
    "    \n",
    "    # loop through each line in the file\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # separate out each line based on the tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the email id and spam indicator fields\n",
    "        email_id = line[0]\n",
    "        spam_ind = line[1]\n",
    "        \n",
    "        # grab the text from the body and subject and concatenate it\n",
    "        text = line[2] + \" \" + line[3]\n",
    "        \n",
    "        # convert the text into a list of words without punctuation\n",
    "        # also lowercase all the words\n",
    "        words = wordify.findall(text.lower())\n",
    "        \n",
    "        # loop through each word\n",
    "        for word in words:\n",
    "            \n",
    "            # if the word is one of the words we're searching for, set\n",
    "            # the word indicator to 1; otherwise, set it to 0\n",
    "            if word in findwords:\n",
    "                word_ind = 1\n",
    "            else:\n",
    "                word_ind = 0\n",
    "                \n",
    "            # set the word field\n",
    "            word_act = word\n",
    "            \n",
    "            # collect the line that we want to print out\n",
    "            info = email_id + \"\\t\" + word_act + \"\\t\" + \\\n",
    "            str(spam_ind) + \"\\t\" + str(word_ind)\n",
    "            \n",
    "            # let's print each email id, word, spam indicator, \n",
    "            # and word of interest indicator\n",
    "            # we'll separate each value with a tab character\n",
    "            print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function takes the outputs from the mapper function and uses it to calculate the probabilities for the classifier. It then loops back through the outputs and classifies each email ID as spam or not spam. To calculate the probabilities for the classifier, it gathers the following information:\n",
    "- number of all words (does not exclude duplicates)\n",
    "- number of emails\n",
    "- number of spam emails\n",
    "- number of times words of interest in appear in both spam and not spam emails\n",
    "- number of words in emails with the findword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "# import the system libraries to read from the input\n",
    "import sys\n",
    "\n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "\n",
    "# number of words in spam emails\n",
    "spam_ewords = 0 \n",
    "\n",
    "# number of times the findword appears in spam emails\n",
    "findword_spam = 0 \n",
    "\n",
    "# number of times the findword appears in not spam emails\n",
    "findword_notspam = 0 \n",
    "\n",
    "# create a dictionary to store each email id \n",
    "# and whether it contains the find word\n",
    "emails = {}\n",
    "\n",
    "# grab and store the count files\n",
    "count_files = sys.argv[1:]\n",
    "\n",
    "# loop through each file in the list of count files\n",
    "for filename in count_files:\n",
    "    \n",
    "    # open the file as readable\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        \n",
    "        # loop through each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            \n",
    "            # split the line by tabs\n",
    "            line = line.split(\"\\t\")\n",
    "            \n",
    "            # pull out my values for each part of the line\n",
    "            email_id = line[0]\n",
    "            word_act = line[1]\n",
    "            spam_ind = int(line[2])\n",
    "            word_ind = int(line[3])\n",
    "            \n",
    "            # let's update the number of words and \n",
    "            # the spam words (if it's spam)\n",
    "            all_words = all_words + 1\n",
    "            if spam_ind == 1:\n",
    "                spam_ewords = spam_ewords + 1\n",
    "            \n",
    "            # if it's a findword, let's update the count \n",
    "            # respective to whether or not it's in \n",
    "            # a spam email\n",
    "            if word_ind == 1:\n",
    "                if spam_ind == 1:\n",
    "                    findword_spam = findword_spam + 1\n",
    "                else:\n",
    "                    findword_notspam = findword_notspam + 1\n",
    "            \n",
    "            # check to see if this email is already \n",
    "            # in the dictionary, and if\n",
    "            # its not already there, initalize it\n",
    "            if email_id not in emails:\n",
    "                \n",
    "                # create a sub-dictionary within the \n",
    "                # email dictionary that is initalized\n",
    "                # to not containing the find word\n",
    "                emails[email_id] = {\"spam\":spam_ind, \"findword\":0}\n",
    "                \n",
    "                # if it's not already there, let's \n",
    "                # also increment the email counter\n",
    "                # and the spam counter if it's spam\n",
    "                all_emails = all_emails + 1\n",
    "                if spam_ind == 1:\n",
    "                    spam_emails = spam_emails + 1\n",
    "            \n",
    "            # if the word is a findword, update \n",
    "            # the find word indicator in the dictionary\n",
    "            if word_ind == 1:\n",
    "                emails[email_id][\"findword\"] = 1\n",
    "\n",
    "# now that we have the summary statistics, we \n",
    "# can calculate the probability that an email \n",
    "# is spam given that a find word appears; we \n",
    "# use the bayesian probability formula from above\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# probability of the find word, given the label\n",
    "# spam is the the number of times the findword\n",
    "# occurs in spam labeled documents over the \n",
    "# number of words in documents labeled spam\n",
    "prob_findGIVspam = float(findword_spam) / float(spam_ewords)\n",
    "\n",
    "# probability of the find word\n",
    "prob_findword = float(findword_spam) / \\\n",
    "float(findword_notspam+findword_spam)\n",
    "\n",
    "# probability of spam given findword using the \n",
    "# Bayesian probability formula from above\n",
    "prob_spamGIVfind = (prob_findGIVspam * prob_spam) / prob_findword\n",
    "\n",
    "# we use the same bayesian probability formula to \n",
    "# calculate the probability of not spam\n",
    "# given the find word\n",
    "prob_findGIVnspam = float(findword_notspam) / \\\n",
    "float(all_words - spam_ewords)\n",
    "\n",
    "prob_nspamGIVfind = (prob_findGIVnspam * prob_nspam) / prob_findword\n",
    "\n",
    "# now let's loop through each email in the \n",
    "# dictionary and classify it as spam or not spam\n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no find word, then \n",
    "    # set the prediction based on the posterior\n",
    "    # probabilities\n",
    "    if emails[email][\"findword\"] == 0:\n",
    "        if(prob_spam > prob_nspam):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # else if the email has a find word, then set \n",
    "    # the prediction based on the conditional probability\n",
    "    else:\n",
    "        if(prob_spamGIVfind > prob_nspamGIVfind):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, \n",
    "    # the actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + str(_prediction)\n",
    "    print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Magic functions to set the appropriate permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the right permissions for the python \n",
    "# files mapper and reducer\n",
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modifies the permission to make the \n",
    "# shell command executable from the notebook\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run our model and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Check the file enronemail_1h.txt.output\n"
     ]
    }
   ],
   "source": [
    "# run the Naive Bayes counter with the word assistance\n",
    "!./pNaiveBayes.sh 3 \"assistance\"\n",
    "\n",
    "print \"Done. Check the file enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.4. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy)\n",
    "To do so, make sure that*\n",
    "\n",
    "- *mapper.py counts all occurrences of a list of words, and*\n",
    "- *reducer.py*\n",
    "\n",
    "*performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "The goal of this function is to spit out the occurences for each word. The input will be a chunk of email messages. The output will be a tab delimited file:\n",
    "- email id\n",
    "- spam indicator\n",
    "- word\n",
    "- word of interest indicator\n",
    "\n",
    "We modify our mapper from 1.3 to print the words of interest at the top of each output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# initalize the outputs that we'll be using\n",
    "email_id = None\n",
    "word_act = None\n",
    "spam_ind = None\n",
    "word_ind = None\n",
    "\n",
    "# collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2:]\n",
    "\n",
    "# let's lowercase the find words\n",
    "new_findwords = []\n",
    "for findword in findwords:\n",
    "    new_findwords.append(findword.lower())\n",
    "findwords = new_findwords\n",
    "    \n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# print the words of interest as the first line\n",
    "words_of_interest=\"\"\n",
    "for word in findwords:\n",
    "    words_of_interest = words_of_interest + word + \"\\t\"\n",
    "print words_of_interest\n",
    "\n",
    "# open the file as readable\n",
    "with open (filename, \"r\") as myfile:\n",
    "    \n",
    "    # loop through each line in the file\n",
    "    for line in myfile.readlines()[1:]:\n",
    "        \n",
    "        # separate out each line based on the tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the email id and spam indicator fields\n",
    "        email_id = line[0]\n",
    "        spam_ind = line[1]\n",
    "        \n",
    "        # grab the text from the body and subject and concatenate it\n",
    "        text = line[2] + \" \" + line[3]\n",
    "        \n",
    "        # convert the text into a list of words without punctuation\n",
    "        # also lowercase all the words\n",
    "        words = wordify.findall(text.lower())\n",
    "        \n",
    "        # loop through each word\n",
    "        for word in words:\n",
    "            \n",
    "            # if the word is one of the words we're searching for, set\n",
    "            # the word indicator to 1; otherwise, set it to 0\n",
    "            if word in findwords:\n",
    "                word_ind = 1\n",
    "            else:\n",
    "                word_ind = 0\n",
    "                \n",
    "            # set the word field\n",
    "            word_act = word\n",
    "            \n",
    "            # collect the line that we want to print out\n",
    "            info = email_id + \"\\t\" + word_act + \"\\t\" + \\\n",
    "            str(spam_ind) + \"\\t\" + str(word_ind)\n",
    "            \n",
    "            # let's print each email id, word, spam \n",
    "            # indicator, and word of interest indicator\n",
    "            # we'll separate each value with a tab character\n",
    "            print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function takes the outputs from the mapper and combines them to build a Naive Bayes classifier. It then runs the Naive Bayes classifier on the training data. We modify this from 1.3 because the reducer in 1.3 is not generalizable enough. It cannot take into account multiple words of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "# import the system libraries to read from the input, from the \n",
    "# math library import the log function\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "# great a dictionary to store each email id, spam indciator, \n",
    "# list of words of interest, and spam prediction\n",
    "emails = {}\n",
    "\n",
    "# grab and store the count files\n",
    "count_files = sys.argv[1:]\n",
    "\n",
    "# let's grab the find words from the first file\n",
    "with open (count_files[0], \"r\") as myfile:\n",
    "    findwords = myfile.readlines()[0].split(\"\\t\")\n",
    "# we also need to remove the new line character that populates\n",
    "if '\\n' in findwords: findwords.remove('\\n')\n",
    "    \n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "spam_ewords = 0 # number of words in spam emails\n",
    "\n",
    "# let's initalize a dictionary for the find words\n",
    "# each find word will have it's own dictionary with the counts\n",
    "# of how many times it appears in spam emails and how many in\n",
    "# not spam emails\n",
    "findword_prob = {}\n",
    "for findword in findwords:\n",
    "    findword_prob[findword] = {\"spam_count\":0,\"not_spam_count\":0}\n",
    "\n",
    "# loop through each file in the list of count files\n",
    "for filename in count_files:\n",
    "    \n",
    "    # open the file as readable\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        \n",
    "        # loop through each line in the file, \n",
    "        # we ignore the first line because\n",
    "        # that is the line with the find words\n",
    "        for line in myfile.readlines()[1:]:\n",
    "            \n",
    "            # split the line by tabs\n",
    "            line = line.split(\"\\t\")\n",
    "   \n",
    "            # pull out my values for each part of the line\n",
    "            email_id = line[0]\n",
    "            word_act = line[1]\n",
    "            spam_ind = int(line[2])\n",
    "            word_ind = int(line[3])\n",
    "            \n",
    "            # let's update the number of words \n",
    "            # and the spam words (if it's spam)\n",
    "            all_words = all_words + 1\n",
    "            if spam_ind == 1:\n",
    "                spam_ewords = spam_ewords + 1\n",
    "            \n",
    "            # if it's a findword, let's update \n",
    "            # the count respective to whether\n",
    "            # or not it's in a spam email\n",
    "            if word_ind == 1:\n",
    "                \n",
    "                # let's grab the words dictionary\n",
    "                word_dict = findword_prob[word_act]\n",
    "                \n",
    "                if spam_ind == 1:\n",
    "                    word_dict[\"spam_count\"] = \\\n",
    "                    word_dict[\"spam_count\"] + 1\n",
    "                else:\n",
    "                    word_dict[\"not_spam_count\"] = \\\n",
    "                    word_dict[\"not_spam_count\"] + 1\n",
    "            \n",
    "            # check to see if this email is already \n",
    "            # in the dictionary, and if\n",
    "            # its not already there, initalize it\n",
    "            if email_id not in emails:\n",
    "                \n",
    "                # create a sub-dictionary within \n",
    "                # the email dictionary for each email\n",
    "                emails[email_id] = {\"spam\":spam_ind, \"words\":[]}\n",
    "                \n",
    "                # if it's not already there, let's \n",
    "                # also increment the email counter\n",
    "                # and the spam counter if it's spam\n",
    "                all_emails = all_emails + 1\n",
    "                if spam_ind == 1:\n",
    "                    spam_emails = spam_emails + 1\n",
    "            \n",
    "            # if the word is a findword, add \n",
    "            # it to the list of find words\n",
    "            if word_ind == 1:\n",
    "                emails[email_id][\"words\"].append(word_act)\n",
    "\n",
    "# now that we have the summary statistics, \n",
    "# we can calculate the probability that an email \n",
    "# is spam given that a find word appears; \n",
    "# we use the bayesian probability formula from above\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# for each find word, let's calculate the \n",
    "# conditional probability of spam given the find word\n",
    "# and not spam given the find word\n",
    "\n",
    "# look at each findword\n",
    "for findword in findword_prob.keys():\n",
    "    \n",
    "    # set the find word that we'll be calculating \n",
    "    # the probabilities for\n",
    "    findword = findword_prob[findword]\n",
    "    \n",
    "    # calculate the probability of the word\n",
    "    findword['total'] = findword['spam_count'] + \\\n",
    "    findword['not_spam_count']\n",
    "    findword['probs'] = float(findword['total']) / \\\n",
    "    float(all_words)\n",
    "    \n",
    "    # calculate the probability of word given spam\n",
    "    findword['wordGIVspam'] = float(findword['spam_count']) \\\n",
    "    / float(spam_ewords)\n",
    "    \n",
    "    # calculate the probability of the word given not spam\n",
    "    findword['wordGIVnspam'] = \\\n",
    "    float(findword['not_spam_count']) / \\\n",
    "    float(all_words-spam_ewords)\n",
    "    \n",
    "    # calculate the probability of spam given the \n",
    "    # word (here, we use the probability formula \n",
    "    # provided in the explanation of the Naive Bayes\n",
    "    # classifier from Section 1.2)\n",
    "    findword['spamGIVword'] = \\\n",
    "    (float(findword['wordGIVspam']) * float(prob_spam)) \\\n",
    "    / float(findword['probs'])\n",
    "    \n",
    "    # calculate the probability of not spam given the word\n",
    "    findword['nspamGIVword'] = \\\n",
    "    (float(findword['wordGIVnspam']) * float(prob_nspam)) \\\n",
    "    / float(findword['probs'])\n",
    "\n",
    "# now let's loop through each email in \n",
    "# the dictionary and classify it as spam or not\n",
    "# spam\n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no find word, then set the\n",
    "    # prediction based on the posterior\n",
    "    # probabilities\n",
    "    if len(emails[email][\"words\"]) == 0:\n",
    "        if(prob_spam > prob_nspam):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # else if the email has a find word, \n",
    "    # then set the prediction based on the\n",
    "    # conditional probability\n",
    "    else:\n",
    "        \n",
    "        # initalize a set of spam and not \n",
    "        # spam probabilities\n",
    "        spam_prob = 1\n",
    "        nspam_prob = 1\n",
    "        \n",
    "        # loop through each of the find words \n",
    "        # in the list of words and \n",
    "        # update the spam and not spam probabilities\n",
    "        for word in emails[email]['words']:\n",
    "            \n",
    "            spam_prob = spam_prob * \\\n",
    "            findword_prob[word]['spamGIVword']\n",
    "            nspam_prob = nspam_prob * \\\n",
    "            findword_prob[word]['nspamGIVword']\n",
    "        \n",
    "        if(spam_prob > nspam_prob):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, the \n",
    "    # actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + str(_prediction)\n",
    "    print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the right permissions for the python \n",
    "# files mapper and reducer\n",
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modifies the permission to make the shell\n",
    "# command executable from the notebook\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run our model and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Check the file enronemail_1h.txt.output\n"
     ]
    }
   ],
   "source": [
    "# run the Naive Bayes counter with the word assistance\n",
    "!./pNaiveBayes.sh 3 assistance valium enlargementWithATypo\n",
    "\n",
    "print \"Done. Check the file enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.5. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that*\n",
    "- *mapper.py counts all occurrences of all words, and*\n",
    "- *reducer.py performs a word-distribution-wide Naive Bayes classification.*\n",
    "\n",
    "*In this problem  you should apply a Laplace (add-1) smoothing to the classifier (always on the reducer side) to safeguard code against low-data. <br> \n",
    "For a quick reference on the construction of the classifier that you will code, please consult the \"Document Classification\" section of the following wikipedia page: <br>\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "<br>the original paper by our curators of the Enron email data:<br>\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf <br>\n",
    "or the recording of this week's live lecture that you will find on the LMS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error function\n",
    "We write a function to calculate the training error of our model. We define the training error as the number of incorrectly classified records over the total number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainingerror(class_file):\n",
    "    \"\"\"a function that takes a tab delimited\n",
    "    classification file with 3 entries per row \n",
    "    that correspond to: record id, true class,\n",
    "    and predicted class\"\"\"\n",
    "    \n",
    "    # initalize some counters so that we can \n",
    "    # keep track of how many are wrong and\n",
    "    # and how many total records there are\n",
    "    records_wrong = 0\n",
    "    records_total = 0\n",
    "    \n",
    "    # open the file\n",
    "    with open (class_file, \"r\") as myfile:\n",
    "        \n",
    "        # read every line in the file\n",
    "        for line in myfile.readlines():\n",
    "            \n",
    "            # separate each line by the tabs\n",
    "            line = line.split(\"\\t\")\n",
    "            \n",
    "            # get the truth and predicted values\n",
    "            _truth = int(line[1])\n",
    "            _predicted = int(line[2])\n",
    "        \n",
    "            # add to the wrong records if \n",
    "            # the prediction is wrong\n",
    "            if _predicted != _truth:\n",
    "                records_wrong = records_wrong + 1\n",
    "                \n",
    "            # add to the total records\n",
    "            records_total = records_total + 1\n",
    "    \n",
    "    # calculate the error rate as wrong over total\n",
    "    error = float(records_wrong) / float(records_total)\n",
    "    \n",
    "    # return this error rate\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "We re-use the mapper function from 1.4. We make a modification to disregard any inputs after the number of chunks because we are counting all words. Similarily, we don't print out any findwords to the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# initalize the outputs that we'll be using\n",
    "email_id = None\n",
    "word_act = None\n",
    "spam_ind = None\n",
    "word_ind = None\n",
    "\n",
    "# collect user input, we are no longer \n",
    "# collecting any information \n",
    "# other than the number of chunks\n",
    "filename = sys.argv[1]\n",
    "    \n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# open the file as readable\n",
    "with open (filename, \"r\") as myfile:\n",
    "    \n",
    "    # loop through each line in the file\n",
    "    for line in myfile.readlines()[1:]:\n",
    "        \n",
    "        # separate out each line based on the tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the email id and spam indicator fields\n",
    "        email_id = line[0]\n",
    "        spam_ind = line[1]\n",
    "        \n",
    "        # grab the text from the body and \n",
    "        # subject and concatenate it\n",
    "        text = line[2] + \" \" + line[3]\n",
    "        \n",
    "        # convert the text into a list of \n",
    "        # words without punctuation\n",
    "        # also lowercase all the words\n",
    "        words = wordify.findall(text.lower())\n",
    "        \n",
    "        # loop through each word\n",
    "        for word in words:\n",
    "            \n",
    "            # collect the line that we want to print out\n",
    "            info = email_id + \"\\t\" + word + \"\\t\" + str(spam_ind)\n",
    "            \n",
    "            # let's print each email id, word, \n",
    "            # spam indicator, and word of interest indicator\n",
    "            # we'll separate each value with a tab character\n",
    "            print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer Function (with smoothing)\n",
    "We modify our reducer function from 1.4 to calculate the probabilities for every word. We add a LaPlace smoother and log the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "# import the system libraries to read \n",
    "# from the input, from the \n",
    "# math library import the log function\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "# great a dictionary to store each email id, \n",
    "# spam indciator, list of words of \n",
    "# interest, and spam prediction\n",
    "emails = {}\n",
    "\n",
    "# grab and store the count files\n",
    "count_files = sys.argv[1:]\n",
    "\n",
    "# let's grab the find words from the first file\n",
    "with open (count_files[0], \"r\") as myfile:\n",
    "    findwords = myfile.readlines()[0].split(\"\\t\")\n",
    "# we also need to remove the new line character \n",
    "# that populates\n",
    "if '\\n' in findwords: findwords.remove('\\n')\n",
    "    \n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "spam_ewords = 0 # number of words in spam emails\n",
    "\n",
    "# let's initalize a dictionary that will \n",
    "# hold the probabilities for \n",
    "# each and every word in the corpus\n",
    "words_probs = {}\n",
    "\n",
    "# loop through each file in the list of count files\n",
    "for filename in count_files:\n",
    "    \n",
    "    # open the file as readable\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        \n",
    "        # loop through each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            \n",
    "            # split the line by tabs\n",
    "            line = line.split(\"\\t\")\n",
    "   \n",
    "            # pull out my values for each part of the line\n",
    "            email_id = line[0]\n",
    "            word_act = line[1]\n",
    "            spam_ind = int(line[2])\n",
    "            \n",
    "            # let's update the number of words and \n",
    "            # the spam words (if it's spam)\n",
    "            all_words = all_words + 1\n",
    "            if spam_ind == 1:\n",
    "                spam_ewords = spam_ewords + 1\n",
    "            \n",
    "            # if we don't already have the word in \n",
    "            # our dictionary of words, let's\n",
    "            # add it and initalize counts of zero\n",
    "            if word_act not in words_probs:\n",
    "                words_probs[word_act] = \\\n",
    "                {\"spam_count\":0,\"not_spam_count\":0}\n",
    "            \n",
    "            # let's grab the dictionary for the word\n",
    "            word_dict = words_probs[word_act]\n",
    "            \n",
    "            # let's increment the word counts \n",
    "            # for the word\n",
    "            if spam_ind == 1:\n",
    "                word_dict['spam_count'] = \\\n",
    "                word_dict['spam_count'] + 1\n",
    "            else:\n",
    "                word_dict['not_spam_count'] = \\\n",
    "                word_dict['not_spam_count'] + 1\n",
    "            \n",
    "            # check to see if this email is already \n",
    "            # in the dictionary, and if\n",
    "            # its not already there, initalize it\n",
    "            if email_id not in emails:\n",
    "                \n",
    "                # create a sub-dictionary within \n",
    "                # the email dictionary for each email\n",
    "                emails[email_id] = \\\n",
    "                {\"spam\":spam_ind, \"words\":[]}\n",
    "                \n",
    "                # if it's not already there, \n",
    "                # let's also increment the email counter\n",
    "                # and the spam counter if it's spam\n",
    "                all_emails = all_emails + 1\n",
    "                if spam_ind == 1:\n",
    "                    spam_emails = spam_emails + 1\n",
    "            \n",
    "            # let's add the word to our list \n",
    "            # of words for this email\n",
    "            emails[email_id][\"words\"].append(word_act)\n",
    "\n",
    "# now that we have the summary statistics, \n",
    "# we can calculate the probability that an email \n",
    "# is spam given the words it contains; \n",
    "# we use the bayesian probability formula from above\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# let's also take the log of these probabilities\n",
    "log_prob_spam = log(prob_spam)\n",
    "log_prob_nspam = log(prob_nspam)\n",
    "\n",
    "# for each word, let's calculate the \n",
    "# conditional probability of spam given the word\n",
    "# and not spam given the word\n",
    "\n",
    "# let's define our LaPlace smoother\n",
    "SMOOTHER = 1.0\n",
    "VOCAB = len(words_probs)\n",
    "\n",
    "# look at each word\n",
    "for word in words_probs.keys():\n",
    "    \n",
    "    # set the find word that we'll be \n",
    "    # calculating the probabilities for\n",
    "    word = words_probs[word]\n",
    "    \n",
    "    # calculate the probability of the word\n",
    "    word['total'] = word['spam_count'] + \\\n",
    "    word['not_spam_count']\n",
    "    word['probs'] = float(word['total']) / \\\n",
    "    float(all_words)\n",
    "    \n",
    "    # calculate the probability of word \n",
    "    # given spam and add the smoother\n",
    "    word['wordGIVspam'] = \\\n",
    "    float(word['spam_count'] + SMOOTHER) / \\\n",
    "    float(spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of the word \n",
    "    # given not spam and add the smoother\n",
    "    word['wordGIVnspam'] = \\\n",
    "    float(word['not_spam_count'] + SMOOTHER) / \\\n",
    "    float(all_words-spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of spam given \n",
    "    # the word (here, we use the probability formula \n",
    "    # provided in the explanation of the Naive \n",
    "    # Bayes classifier from Section 1.2)\n",
    "    word['spamGIVword'] = \\\n",
    "    (float(word['wordGIVspam']) * float(prob_spam)) / \\\n",
    "    float(word['probs'])\n",
    "    \n",
    "    # calculate the probability of not \n",
    "    # spam given the word\n",
    "    word['nspamGIVword'] = \\\n",
    "    (float(word['wordGIVnspam']) * float(prob_nspam)) / \\\n",
    "    float(word['probs'])\n",
    "    \n",
    "    # let's log the probabilities of interest\n",
    "    word['log_spamGIVword'] = log(word['spamGIVword'])\n",
    "    word['log_nspamGIVword'] = log(word['nspamGIVword'])\n",
    "    \n",
    "# now let's print our model out to a file\n",
    "# this will be useful because it will make it\n",
    "# easier to load it in future functions\n",
    "with open(\"NaiveBayesSmoothing.txt\",\"w\") as myfile:\n",
    "\n",
    "    # set the header\n",
    "    myfile.write(\"Word \\tCount \\tP(Spam|word) \\tP(Not Spam|word)\\n\")\n",
    "    \n",
    "    # loop through each word\n",
    "    for word in words_probs.keys():\n",
    "\n",
    "        # set the word name\n",
    "        word_name = word\n",
    "        \n",
    "        # set the word that we'll be \n",
    "        # printing the probabilities for\n",
    "        word = words_probs[word]\n",
    "        \n",
    "        # set each line as tab delimited\n",
    "        info = str(word_name) + \"\\t\" +\\\n",
    "        str(word['total']) + \"\\t\" +\\\n",
    "        str(word['spamGIVword']) + \"\\t\" +\\\n",
    "        str(word['nspamGIVword']) + \"\\n\"\n",
    "        \n",
    "        # print each line\n",
    "        myfile.write(info)\n",
    "    \n",
    "# now let's loop through each email in \n",
    "# the dictionary and classify it as spam or not\n",
    "# spam\n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no words, then set the \n",
    "    # prediction based on the posterior\n",
    "    # probabilities\n",
    "    if len(emails[email][\"words\"]) == 0:\n",
    "        if(log_prob_spam > log_prob_nspam):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # else if the email has a word, then set \n",
    "    # the prediction based on the\n",
    "    # conditional probability\n",
    "    else:\n",
    "        \n",
    "        # initalize a set of spam and not spam \n",
    "        # probabilities\n",
    "        spam_prob = 0\n",
    "        nspam_prob = 0\n",
    "        \n",
    "        # loop through each of the words in \n",
    "        # the list of words and \n",
    "        # update the spam and not spam probabilities\n",
    "        for word in emails[email]['words']:\n",
    "            \n",
    "            spam_prob = spam_prob + \\\n",
    "            words_probs[word]['log_spamGIVword']\n",
    "            nspam_prob = nspam_prob + \\\n",
    "            words_probs[word]['log_nspamGIVword']\n",
    "        \n",
    "        if(spam_prob > nspam_prob):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, the \n",
    "    # actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + \\\n",
    "    str(_prediction)\n",
    "    print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modifies the permission to make the \n",
    "# shell command executable from the notebook\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the right permissions for the \n",
    "# python files mapper and reducer\n",
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run our model and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Check the file enronemail_1h.txt.output\n"
     ]
    }
   ],
   "source": [
    "# run the Naive Bayes counter \n",
    "# with the word assistance\n",
    "!./pNaiveBayes.sh 3 *\n",
    "\n",
    "print \"Done. Check the file enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the function defined at the start of this section to\n",
    "# calculate and store the training error for the model\n",
    "# with LaPlace smoothing\n",
    "smoothing_error = trainingerror(\"enronemail_1h.txt.output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer Function (without smoothing)\n",
    "We modify our reducer function from 1.4 to calculate the probabilities for every word. We remove the LaPlace smoother and return to regular (non-logged probabilities). We can easily remove the smoother because we have it as a variable, and we simply set it to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "# import the system libraries to read \n",
    "# from the input, from the \n",
    "# math library import the log function\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "# great a dictionary to store each email id, \n",
    "# spam indciator, list of words of \n",
    "# interest, and spam prediction\n",
    "emails = {}\n",
    "\n",
    "# grab and store the count files\n",
    "count_files = sys.argv[1:]\n",
    "\n",
    "# let's grab the find words from the first file\n",
    "with open (count_files[0], \"r\") as myfile:\n",
    "    findwords = myfile.readlines()[0].split(\"\\t\")\n",
    "# we also need to remove the new line character \n",
    "# that populates\n",
    "if '\\n' in findwords: findwords.remove('\\n')\n",
    "    \n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "spam_ewords = 0 # number of words in spam emails\n",
    "\n",
    "# let's initalize a dictionary that will \n",
    "# hold the probabilities for \n",
    "# each and every word in the corpus\n",
    "words_probs = {}\n",
    "\n",
    "# loop through each file in the list of count files\n",
    "for filename in count_files:\n",
    "    \n",
    "    # open the file as readable\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        \n",
    "        # loop through each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            \n",
    "            # split the line by tabs\n",
    "            line = line.split(\"\\t\")\n",
    "   \n",
    "            # pull out my values for each part of the line\n",
    "            email_id = line[0]\n",
    "            word_act = line[1]\n",
    "            spam_ind = int(line[2])\n",
    "            \n",
    "            # let's update the number of words and \n",
    "            # the spam words (if it's spam)\n",
    "            all_words = all_words + 1\n",
    "            if spam_ind == 1:\n",
    "                spam_ewords = spam_ewords + 1\n",
    "            \n",
    "            # if we don't already have the word in \n",
    "            # our dictionary of words, let's\n",
    "            # add it and initalize counts of zero\n",
    "            if word_act not in words_probs:\n",
    "                words_probs[word_act] = \\\n",
    "                {\"spam_count\":0,\"not_spam_count\":0}\n",
    "            \n",
    "            # let's grab the dictionary for the word\n",
    "            word_dict = words_probs[word_act]\n",
    "            \n",
    "            # let's increment the word counts \n",
    "            # for the word\n",
    "            if spam_ind == 1:\n",
    "                word_dict['spam_count'] = \\\n",
    "                word_dict['spam_count'] + 1\n",
    "            else:\n",
    "                word_dict['not_spam_count'] = \\\n",
    "                word_dict['not_spam_count'] + 1\n",
    "            \n",
    "            # check to see if this email is already \n",
    "            # in the dictionary, and if\n",
    "            # its not already there, initalize it\n",
    "            if email_id not in emails:\n",
    "                \n",
    "                # create a sub-dictionary within \n",
    "                # the email dictionary for each email\n",
    "                emails[email_id] = \\\n",
    "                {\"spam\":spam_ind, \"words\":[]}\n",
    "                \n",
    "                # if it's not already there, \n",
    "                # let's also increment the email counter\n",
    "                # and the spam counter if it's spam\n",
    "                all_emails = all_emails + 1\n",
    "                if spam_ind == 1:\n",
    "                    spam_emails = spam_emails + 1\n",
    "            \n",
    "            # let's add the word to our list \n",
    "            # of words for this email\n",
    "            emails[email_id][\"words\"].append(word_act)\n",
    "\n",
    "# now that we have the summary statistics, \n",
    "# we can calculate the probability that an email \n",
    "# is spam given the words it contains; \n",
    "# we use the bayesian probability formula from above\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# for each word, let's calculate the \n",
    "# conditional probability of spam given the word\n",
    "# and not spam given the word\n",
    "\n",
    "# let's define our LaPlace smoother \n",
    "# (in this case, 0)\n",
    "SMOOTHER = 0\n",
    "VOCAB = 0\n",
    "\n",
    "# look at each word\n",
    "for word in words_probs.keys():\n",
    "    \n",
    "    # set the find word that we'll be \n",
    "    # calculating the probabilities for\n",
    "    word = words_probs[word]\n",
    "    \n",
    "    # calculate the probability of the word\n",
    "    word['total'] = word['spam_count'] + \\\n",
    "    word['not_spam_count']\n",
    "    word['probs'] = float(word['total']) / \\\n",
    "    float(all_words)\n",
    "    \n",
    "    # calculate the probability of word \n",
    "    # given spam and add the smoother\n",
    "    word['wordGIVspam'] = \\\n",
    "    float(word['spam_count'] + SMOOTHER) / \\\n",
    "    float(spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of the word \n",
    "    # given not spam and add the smoother\n",
    "    word['wordGIVnspam'] = \\\n",
    "    float(word['not_spam_count'] + SMOOTHER) / \\\n",
    "    float(all_words-spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of spam given \n",
    "    # the word (here, we use the probability formula \n",
    "    # provided in the explanation of the Naive \n",
    "    # Bayes classifier from Section 1.2)\n",
    "    word['spamGIVword'] = \\\n",
    "    (float(word['wordGIVspam']) * float(prob_spam)) / \\\n",
    "    float(word['probs'])\n",
    "    \n",
    "    # calculate the probability of not \n",
    "    # spam given the word\n",
    "    word['nspamGIVword'] = \\\n",
    "    (float(word['wordGIVnspam']) * float(prob_nspam)) / \\\n",
    "    float(word['probs'])\n",
    "\n",
    "# now let's print our model out to a file\n",
    "# this will be useful because it will make it\n",
    "# easier to load it in future functions\n",
    "with open(\"NaiveBayesNoSmoothing.txt\",\"w\") as myfile:\n",
    "\n",
    "    # set the header\n",
    "    myfile.write(\"Word \\tCount \\tP(Spam|word) \\tP(Not Spam|word)\\n\")\n",
    "    \n",
    "    # loop through each word\n",
    "    for word in words_probs.keys():\n",
    "\n",
    "        # set the word name\n",
    "        word_name = word\n",
    "        \n",
    "        # set the word that we'll be \n",
    "        # printing the probabilities for\n",
    "        word = words_probs[word]\n",
    "        \n",
    "        # set each line as tab delimited\n",
    "        info = str(word_name) + \"\\t\" +\\\n",
    "        str(word['total']) + \"\\t\" +\\\n",
    "        str(word['spamGIVword']) + \"\\t\" +\\\n",
    "        str(word['nspamGIVword']) + \"\\n\"\n",
    "        \n",
    "        # print each line\n",
    "        myfile.write(info)   \n",
    "\n",
    "# now let's loop through each email in \n",
    "# the dictionary and classify it as spam or not\n",
    "# spam\n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no words, then set the \n",
    "    # prediction based on the posterior\n",
    "    # probabilities\n",
    "    if len(emails[email][\"words\"]) == 0:\n",
    "        if(prob_spam > prob_nspam):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # else if the email has a word, then set \n",
    "    # the prediction based on the\n",
    "    # conditional probability\n",
    "    else:\n",
    "        \n",
    "        # initalize a set of spam and not spam \n",
    "        # probabilities\n",
    "        spam_prob = 1\n",
    "        nspam_prob = 1\n",
    "        \n",
    "        # loop through each of the words in \n",
    "        # the list of words and \n",
    "        # update the spam and not spam probabilities\n",
    "        for word in emails[email]['words']:\n",
    "            \n",
    "            spam_prob = spam_prob * \\\n",
    "            words_probs[word]['spamGIVword']\n",
    "            nspam_prob = nspam_prob * \\\n",
    "            words_probs[word]['nspamGIVword']\n",
    "        \n",
    "        if(spam_prob > nspam_prob):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, the \n",
    "    # actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + \\\n",
    "    str(_prediction)\n",
    "    print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modifies the permission to make the \n",
    "# shell command executable from the notebook\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the right permissions for the \n",
    "# python files mapper and reducer\n",
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run our model and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Check the file enronemail_1h.txt.output\n"
     ]
    }
   ],
   "source": [
    "# run the Naive Bayes counter \n",
    "# with the word assistance\n",
    "!./pNaiveBayes.sh 3 *\n",
    "\n",
    "print \"Done. Check the file enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the function defined at the start of this section to\n",
    "# calculate and store the training error for the model\n",
    "# with LaPlace smoothing\n",
    "no_smoothing_error = trainingerror(\"enronemail_1h.txt.output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare the training errors\n",
    "We compare the training errors between the model with LaPlace smoothing and the model without. We can see from the table below that the not smoothed model has a higher error rate than the smoothed model. This is because the not smoothed model fails to account for the possibility that a word could be in a class that we did not see. This will cause us to zero out a class unnecessarily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|       Model        | Training Error Rate |\n",
      "+--------------------+---------------------+\n",
      "|   Smoothed model   |         0.0         |\n",
      "| Not smoothed model |         0.03        |\n",
      "+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# import the python library pretty table; \n",
    "# this wil help us print out the comparison \n",
    "# very clearly\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# create the pretty table and add a row for both models\n",
    "prettySmooth = PrettyTable([\"Model\",\"Training Error Rate\"])\n",
    "prettySmooth.add_row([\"Smoothed model\", \n",
    "                      round(smoothing_error,2)])\n",
    "prettySmooth.add_row([\"Not smoothed model\", \n",
    "                      round(no_smoothing_error,2)])\n",
    "\n",
    "# print out the table\n",
    "print prettySmooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW1.6 \n",
    "\n",
    "*Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes. It always a good idea to test your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html. \n",
    "<br>Lets define  Training error = misclassification rate with respect to a training set. It is more formally defined here:*\n",
    "- *Let DF represent the training set in the following:*\n",
    "- *Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|*\n",
    "- *Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”*\n",
    "\n",
    "*In this exercise, please complete the following:*\n",
    "- *Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)*\n",
    "- *Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error*\n",
    "- *Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error*\n",
    "- *Please prepare a table to present your results*\n",
    "- *Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)*\n",
    "- *Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preparation for Sklearn\n",
    "Before putting the data through the sklearn algorithms, we have to prepare it. We do this by using the Sklearn's countvectorizers to transform the data into a bag of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our bag of words has a shape of: (100, 5375)\n",
      "Our traiing labels have a shape of: (100,)\n"
     ]
    }
   ],
   "source": [
    "# import the count vectorizer function from the \n",
    "# appropriate Sklearn library and import numpy to reshape\n",
    "# the labels array into a 100 x 1 array\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# create arrays to hold the data\n",
    "email_ids = []\n",
    "train_labels = []\n",
    "email_text = []\n",
    "\n",
    "# begin by opening the file\n",
    "with open(\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "    \n",
    "    # loop through each line\n",
    "    for record in myfile.readlines():\n",
    "        \n",
    "        # split each line based on the tabs\n",
    "        record = record.split(\"\\t\")\n",
    "        \n",
    "        # generate the text as a combination \n",
    "        # of the subject and body\n",
    "        text = record[2] + \" \" + record[3]\n",
    "        \n",
    "        # add to the email_ids, to the labels, \n",
    "        # and to the email text\n",
    "        email_ids.append(record[0])\n",
    "        train_labels.append(record[1])\n",
    "        email_text.append(text)\n",
    "        \n",
    "# genearate a bag of words on the email texts \n",
    "# by using count vectorizer\n",
    "bag = CountVectorizer()\n",
    "train_data = bag.fit_transform(email_text)\n",
    "\n",
    "# convert email labels into a numpy array of size 100x1\n",
    "train_labels = np.array(train_labels).reshape(-1)\n",
    "\n",
    "# to verify that we've got the right thing, \n",
    "# let's print out the shape\n",
    "# of our training data and training labels. \n",
    "# it should be 100 rows \n",
    "# have a column for every word\n",
    "print \"Our bag of words has a shape of:\",train_data.shape\n",
    "print \"Our traiing labels have a shape of:\", train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outputing SKLearn Findings\n",
    "We create a function that will output the results from running SKLearn's algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputSK(filename,record_ids,truths,predictions):\n",
    "    \"\"\"Takes three arrays (record id, truth, \n",
    "    prediction) and a file name and arranges \n",
    "    them as a tab delimited file with columns: \n",
    "    record id, truth, and prediction and\n",
    "    outputs the file with specified file name\"\"\"\n",
    "    \n",
    "    # open the file as write-able\n",
    "    with open(filename, \"w\") as myfile:\n",
    "        \n",
    "        # loop through every record ids, truth, \n",
    "        # and prediction\n",
    "        for index,record in enumerate(record_ids):\n",
    "            \n",
    "            # create the line that has the information \n",
    "            # we'll write to the file\n",
    "            new_line = record_ids[index] + \"\\t\" + \\\n",
    "            truths[index] + \"\\t\" + \\\n",
    "            predictions[index] + \"\\n\"\n",
    "            \n",
    "            # write the line to the file\n",
    "            myfile.write(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SKLearn's Multinomial Naive Bayes Algorithm\n",
    "We use SKLearn's multinomial naive bayes algorithm on our email data to generate predictions of spam or not spam. We then use our output function to save the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out the output file 'sk_multinomial.txt' \n",
      "to see the results from the SK Learn\n",
      " Multinomial Naive Bayes algorithm.\n"
     ]
    }
   ],
   "source": [
    "# import the multinomial naive bayes algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create and train the model on our data\n",
    "sk_multinomial = MultinomialNB()\n",
    "sk_multinomial.fit(train_data,train_labels)\n",
    "\n",
    "# use our model to predict the training data\n",
    "sk_predictions = sk_multinomial.predict(train_data)\n",
    "\n",
    "# output the results to a file that we \n",
    "# can use for comparison\n",
    "outputSK(\"sk_multinomial.txt\",email_ids,\\\n",
    "         train_labels,sk_predictions)\n",
    "\n",
    "print \"Check out the output file \\\n",
    "'sk_multinomial.txt' \\nto see the results from \\\n",
    "the SK Learn\\n Multinomial Naive Bayes algorithm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SKLearn's Bernoulli Naive Bayes Algorithm\n",
    "We use SKLearn's bernouilli naive bayes algorithm on our email data to generate predictions of spam or not spam. We then use our output function to save the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out the output file 'sk_bernoulli.txt' \n",
      "to see the results from the SK Learn\n",
      " Multinomial Naive Bayes algorithm.\n"
     ]
    }
   ],
   "source": [
    "# import the multinomial naive bayes algorithm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# create and train the model on our data\n",
    "sk_bernoulli = BernoulliNB()\n",
    "sk_bernoulli.fit(train_data,train_labels)\n",
    "\n",
    "# use our model to predict the training data\n",
    "sk_predictions = sk_bernoulli.predict(train_data)\n",
    "\n",
    "# output the results to a file that we can \n",
    "# use for comparison\n",
    "outputSK(\"sk_bernoulli.txt\",email_ids,\\\n",
    "         train_labels,sk_predictions)\n",
    "\n",
    "print \"Check out the output file \\\n",
    "'sk_bernoulli.txt' \\nto see the results from \\\n",
    "the SK Learn\\n Multinomial Naive Bayes algorithm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reporting the training error\n",
    "Now let's report the training error across the 3 models (1.5, SKLearn's Multinomial Naive Bayes, and SKLearn's Bernoulli Naive Bayes. Remember that we can use the training error function we created for 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------+\n",
      "|            Model            | Training Error Rate |\n",
      "+-----------------------------+---------------------+\n",
      "|     My Naive Bayes Model    |         0.0         |\n",
      "| SKLearn's Multinomial Model |         0.0         |\n",
      "|  SKLearn's Bernoulli Model  |         0.16        |\n",
      "+-----------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# import the python library pretty table; \n",
    "# this wil help us print out the comparison \n",
    "# very clearly\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# get the training errors for each algorithm\n",
    "my_error = smoothing_error # already calculated in 1.5\n",
    "sk_multi_error = trainingerror(\"sk_multinomial.txt\")\n",
    "sk_berno_error = trainingerror(\"sk_bernoulli.txt\")\n",
    "\n",
    "\n",
    "# create the pretty table and add a row for each model\n",
    "pretty = PrettyTable([\"Model\",\"Training Error Rate\"])\n",
    "pretty.add_row([\"My Naive Bayes Model\", my_error])\n",
    "pretty.add_row([\"SKLearn's Multinomial Model\", sk_multi_error])\n",
    "pretty.add_row([\"SKLearn's Bernoulli Model\", sk_berno_error])\n",
    "\n",
    "# print out the table\n",
    "print pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our Naive Bayes Model and SKLearn's Multinomial Naive Bayes model have the same error rates. This make sense because they use the same probability calculations. They even both use the same smoother of 1. They are both designed to handle this type of data. They can handle multiple values for a given feature. However, the Bernoulli model relies on the assumption that each feature is a binary value. We know this to be false for our data here because a given word can appear at different frequencies. The Bernoulli model attemps to binarize the data and results in losing information. As a side note, it's interesting that our error rate is zero for my and SKLearn's multinomial naive bayes classifier. This is because we trained the model on the training data and then tested it on the same training data. We should have tested it on separate test data. We got such a low error rate, in part, because the overlapp between words in the ham and not ham classes was pretty small, combined with a small training set and a large feature set (every vocab word) meant that we likely overfit our training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
