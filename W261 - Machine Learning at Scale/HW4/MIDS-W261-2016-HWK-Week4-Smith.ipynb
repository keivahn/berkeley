{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \\#4\n",
    "**Student: Alex Smith** <br>\n",
    "Course: W261 - Machine Learning at Scale <br>\n",
    "Professor: Jimi Shanahan <br>\n",
    "Due Date: June 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Libraries\n",
    "The following libraries must be installed before running the below code. They can all be installed through [Pip](https://github.com/pypa/pip).\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "- [Regular Expression](https://docs.python.org/2/library/re.html)\n",
    "- [Pretty Table](https://pypi.python.org/pypi/PrettyTable)\n",
    "- [Random](https://docs.python.org/2/library/random.html)\n",
    "- [Datetime](https://docs.python.org/2/library/datetime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 4.0. \n",
    "*What is MrJob? How is it different to Hadoop MapReduce?<br>\n",
    "What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MrJob is a MapReduce framework. It is a python package for running streaming Hadoop jobs. It was developed by Yelp to assist with producing multi-step jobs. MrJob provides a pythonic way to deal with Hadoop streaming. It's main advantage over Hadoop MapReduce is that it can schedule multiple jobs in succession. It's major disadvantage over Hadoop MapReduce is that it does not serialization of inputs/outputs in binary.<br>\n",
    "We now go over a variety of the mrjob functions:<br>\n",
    "- mapper_init: defines an action to be run before the mapper processes any data\n",
    "- mapper_final: defines an action to be run after the mapper process the input\n",
    "- combiner_final: defines an action for the combiner after it reaches the end of its input\n",
    "- reducer_final: defines an action to be run when the reducer finishes processing its data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "### HW 4.1\n",
    "*What is serialization in the context of MrJob or Hadoop? When it used in these frameworks? What is the default serialization mode for input and outputs for MrJob?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of serialization as the format of the input and output data. Formally, \"serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage\" (Async 4.9).By default, MrJob supports a number of protocols: RawProtocol, JSONProtocol, PickleProtocol, and ReprProtocol. It accepts as input raw text and JSON files. MrJob does not support a binary serialization scheme. Binary serialization schemes can be helpful in reducing the amount of data transferred between nodes. This can make text processing slow as data is serialized and deserialized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 4.2: \n",
    "*Recall the Microsoft logfiles data from the async lecture. The logfiles described are located at:<br> \n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html <br>\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/ <br>\n",
    "<br>\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.<br>\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:<br>\n",
    "C,\"10001\",10001   #Visitor id 10001<br>\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000<br>\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001<br>\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002<br>\n",
    "C,\"10002\",10002   #Visitor id 10001<br>\n",
    "V<br>\n",
    "<br>\n",
    "to the format:<br>\n",
    "V,1000,1,C, 10001<br>\n",
    "V,1001,1,C, 10001<br>\n",
    "V,1002,1,C, 10001<br>\n",
    "Write the python code to accomplish this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to consolidate a Microsoft log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Consolidate(filepath):\n",
    "    \"\"\"Function takes as input a file path.\n",
    "    Returns a modified file in the same directory.\n",
    "    Consolidates the information so that each \n",
    "    record includes both the visitor id and\n",
    "    the page id\"\"\"\n",
    "    \n",
    "    # open the file\n",
    "    with open(filepath,\"r\") as myfile:\n",
    "        \n",
    "        # create a new file name for where\n",
    "        # we will return our output\n",
    "        filepath_new = filepath + \"_mod\"\n",
    "        \n",
    "        # open this new file\n",
    "        with open(filepath_new,\"w\") as mynewfile:\n",
    "        \n",
    "            # set the current visitor\n",
    "            visitor = None\n",
    "\n",
    "            # loop through each line\n",
    "            for line in myfile.readlines():\n",
    "\n",
    "                # split the line by the commas\n",
    "                line = line.split(\",\")\n",
    "                category = line[0].strip()\n",
    "                \n",
    "                # if the category is a vistor id\n",
    "                # or a visit id, then grab the\n",
    "                # rest of the info\n",
    "                if category == \"C\" or \\\n",
    "                category == \"V\":\n",
    "                    record_id = int(line[1].replace(\"\\\"\",\"\"))\n",
    "                    simple = int(line[2].strip())\n",
    "                \n",
    "                # if this is the line that \n",
    "                # identifies the visitor\n",
    "                if category == \"C\":\n",
    "\n",
    "                    # set the visitor\n",
    "                    visitor = record_id\n",
    "\n",
    "                # else we are dealing with a \n",
    "                # page visit\n",
    "                elif category == \"V\":\n",
    "                    \n",
    "                    # write to the new file with\n",
    "                    # visit id and the visitor id\n",
    "                    info = \"V,\" + str(record_id) \\\n",
    "                    + \",\" + str(simple) + \",C,\" \\\n",
    "                    + str(visitor)+\"\\n\"\n",
    "                    mynewfile.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put our log file through this function\n",
    "Consolidate(\"anonymous-msweb.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\r\n",
      "V,1001,1,C,10001\r\n",
      "V,1002,1,C,10001\r\n",
      "V,1001,1,C,10002\r\n",
      "V,1003,1,C,10002\r\n",
      "V,1001,1,C,10003\r\n",
      "V,1003,1,C,10003\r\n",
      "V,1004,1,C,10003\r\n",
      "V,1005,1,C,10004\r\n",
      "V,1006,1,C,10005\r\n"
     ]
    }
   ],
   "source": [
    "# sample the top of the output file to gut \n",
    "# check if our program worked\n",
    "!head anonymous-msweb.data_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 4.3\n",
    "*Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the MRJob class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_pagevisit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_pagevisit.py\n",
    "# import MrJob\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "# create the class\n",
    "class MRPageVisit(MRJob):\n",
    "    \"\"\"A page visit class implemented \n",
    "    in MRJob\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # gather the arguments (i.e. the files\n",
    "        # we want to perform the function on)\n",
    "        super(MRPageVisit, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"takes the words from the input where\n",
    "        the value is the text of the line\"\"\"\n",
    "        \n",
    "        # split the line based on commas\n",
    "        line = line.split(\",\")\n",
    "        \n",
    "        # grab the page visited\n",
    "        page = int(line[1])\n",
    "        \n",
    "        # yield the page with a simple count\n",
    "        # of 1\n",
    "        yield page, 1\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        \"\"\"outputs the sum of visits for each\n",
    "        page visited\"\"\"\n",
    "        \n",
    "        # output the sum of page views\n",
    "        yield key, sum(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a runner to run the MRJob within the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the MRJob that we created\n",
    "from mr_pagevisit import MRPageVisit \n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRPageVisit(args=['anonymous-msweb.data_mod']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run() \n",
    "    \n",
    "    # create a file to write to\n",
    "    with open(\"HW4.3_Output\",\"w\") as myfile: \n",
    "    \n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output(): \n",
    "\n",
    "            # write the output to a file\n",
    "            info=str(mr_job.parse_output_line(line))+\"\\n\"\n",
    "            myfile.write(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the top 5 most frequently visited pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 10836)\r\n",
      "(1034, 9383)\r\n",
      "(1004, 8463)\r\n",
      "(1018, 5330)\r\n",
      "(1017, 5108)\r\n"
     ]
    }
   ],
   "source": [
    "!cat HW4.3_Output | sort -k2nr > temp\n",
    "!head -5 temp\n",
    "!rm temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 4.4\n",
    "*Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to gather the webpage urls based on the webpage IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GatherWeb(filename):\n",
    "    \"\"\"Takes as input the file path to a \n",
    "    Microsoft log file. Gather the URLs and\n",
    "    webpage id combinations. Returns a file\n",
    "    that matches each webpage to its id\"\"\"\n",
    "    \n",
    "    # open the file\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        \n",
    "        # the name of the new file \n",
    "        newfile = \"MS_webpages\"\n",
    "        \n",
    "        # open the new file to write\n",
    "        with open(newfile,\"w\") as mynewfile:\n",
    "        \n",
    "            # loop through each line in the file\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line by commas\n",
    "                line = line.split(\",\")\n",
    "                \n",
    "                # set the category\n",
    "                category = line[0]\n",
    "                \n",
    "                # if the category is the description\n",
    "                # of the webpage\n",
    "                if category == \"A\":\n",
    "                    \n",
    "                    # set the web_id and the \n",
    "                    # web_url\n",
    "                    web_id = line[1]\n",
    "                    web_url = line[3].replace(\"\\\"\",\"\")\n",
    "                    \n",
    "                    # write to the new file\n",
    "                    info = str(web_id) + \",\" \\\n",
    "                    + str(web_url) + \"\\n\"\n",
    "                    mynewfile.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287,International AutoRoute\r\n",
      "1288,library\r\n",
      "1289,Master Chef Product Information\r\n",
      "1297,Central America\r\n",
      "1215,For Developers Only Info\r\n",
      "1279,Multimedia Golf\r\n",
      "1239,Microsoft Consulting\r\n",
      "1282,home\r\n",
      "1251,Reference Support\r\n",
      "1121,Microsoft Magazine\r\n"
     ]
    }
   ],
   "source": [
    "GatherWeb(\"anonymous-msweb.data\")\n",
    "!head MS_webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRJob class to calculate the most frequent visitor for each webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_freqvisit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_freqvisit.py\n",
    "# import MRJob\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "# create the class\n",
    "class MRFreqVisit(MRJob):\n",
    "    \"\"\"MRJob class that identifies the most\n",
    "    frequent visitor for each webpage\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        # allow us to take a file as input\n",
    "        super(MRFreqVisit,self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # create a dictionary to hold the information\n",
    "        # that matches each web id to it's url\n",
    "        self.urls = {}\n",
    "        \n",
    "        # gather the webpage name data\n",
    "        with open('MS_webpages','r') as myfile:\n",
    "            \n",
    "            # read through each line\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # gather the id and url\n",
    "                line = line.split(\",\")\n",
    "                web_id = line[0].strip()\n",
    "                web_url = line[1].strip()\n",
    "                \n",
    "                # add the id and url to\n",
    "                # the dictionary\n",
    "                self.urls[web_id] = web_url\n",
    "                \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # break the line up\n",
    "        line = line.split(\",\")\n",
    "        \n",
    "        # gather the website and the visitor\n",
    "        site = line[1]\n",
    "        visitor = line[4]\n",
    "        \n",
    "        # yield the site with the visitor\n",
    "        # and a count of 1\n",
    "        yield site,(visitor,1)\n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        # create a dictionary for this site\n",
    "        visitor_counts = {}\n",
    "        \n",
    "        # convert the values to a tuple\n",
    "        visitors = tuple(values)\n",
    "        \n",
    "        # loop through the values for each site\n",
    "        for item in visitors:\n",
    "            \n",
    "            # split into the visitor id and\n",
    "            # the count\n",
    "            visitor_id = item[0]\n",
    "            visitor_count = item[1]\n",
    "            \n",
    "            # check to see if this visitor is \n",
    "            # already in the dictionary, if \n",
    "            # it's not, add it\n",
    "            if visitor_id not in \\\n",
    "            visitor_counts.keys():\n",
    "                visitor_counts[visitor_id] = 0\n",
    "            \n",
    "            # add the count to the dictionary\n",
    "            visitor_counts[visitor_id] = \\\n",
    "            visitor_counts[visitor_id] + \\\n",
    "            visitor_count\n",
    "        \n",
    "        # set a max place holder\n",
    "        max_visitor = None\n",
    "        max_count = 0\n",
    "        \n",
    "        # loop through the keys and update the\n",
    "        # max visitor\n",
    "        for visitor in visitor_counts:\n",
    "            \n",
    "            # check to see if it's a new max\n",
    "            if visitor_counts[visitor] > max_count:\n",
    "                max_count = visitor_counts[visitor]\n",
    "                max_visitor = visitor\n",
    "                \n",
    "        \n",
    "        # let's format it nicely by grabbing \n",
    "        # everything we need\n",
    "        url = self.urls[key]\n",
    "        info = url + \"    \" + max_visitor + \"    \" \\\n",
    "        + str(max_count)\n",
    "        \n",
    "        # yield the page, the visitor, and \n",
    "        # the count\n",
    "        yield key,info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a runner to run the MRJob within the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the MRJob that we created\n",
    "from mr_freqvisit import MRFreqVisit \n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRFreqVisit(args=['anonymous-msweb.data_mod','--file=MS_webpages']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run() \n",
    "    \n",
    "    # create a file to write to\n",
    "    with open(\"HW4.4_Output\",\"w\") as myfile: \n",
    "    \n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output(): \n",
    "\n",
    "            # write the output to a file\n",
    "            info=str(mr_job.parse_output_line(line))+\"\\n\"\n",
    "            myfile.write(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the most frequent visitors for a couple of webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage.ID___URL___Visitor.ID___Visits\n",
      "('1000', 'regwiz    36585    1')\n",
      "('1001', 'Support Desktop    23995    1')\n",
      "('1002', 'End User Produced View    35235    1')\n",
      "('1003', 'Knowledge Base    22469    1')\n",
      "('1004', 'Microsoft.com Search    35540    1')\n",
      "('1005', 'Norway    10004    1')\n",
      "('1006', 'misc    27495    1')\n",
      "('1007', 'International IE content    19492    1')\n",
      "('1008', 'Free Downloads    35236    1')\n",
      "('1009', 'Windows Family of OSs    22504    1')\n"
     ]
    }
   ],
   "source": [
    "!echo Webpage.ID___URL___Visitor.ID___Visits\n",
    "!head HW4.4_Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 4.5 Clustering Tweet Dataset\n",
    "*Here you will use a different dataset consisting of word-frequency distributions for 1,000 Twitter users. These Twitter users use language in very different ways, and were classified by hand according to the criteria:<br>\n",
    "0: Human, where only basic human-human communication is observed.<br>\n",
    "1: Cyborg, where language is primarily borrowed from other sources<br>\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).<br>\n",
    "2: Robot, where language is formulaically derived from unrelated sources<br>\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).<br>\n",
    "3: Spammer, where language is replicated to high multiplicity<br>\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )<br>\n",
    "<br>\n",
    "Check out the preprints of  recent research, which spawned this dataset:*<br>\n",
    "- *http://arxiv.org/abs/1505.04342*\n",
    "- *http://arxiv.org/abs/1508.01843*\n",
    "<br>\n",
    "\n",
    "*The main data lie in the accompanying file:<br>\n",
    "topUsers_Apr-Jul_2014_1000-words.txt<br>\n",
    "<br>\n",
    "and are of the form:<br>\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...<br>\n",
    "<br>\n",
    "where<br>\n",
    "USERID = unique user identifier<br>\n",
    "CODE = 0/1/2/3 class code<br>\n",
    "TOTAL = sum of the word counts<br>\n",
    "<br>\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users by their 1000-dimensional word stripes/vectors using several centroid initializations and values of K.<br>\n",
    "<br>\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that word-frequency distributions are generally heavy-tailed power-laws (often called Zipf distributions), and are very rare in the larger class of discrete, random distributions. For each user you will have to normalize by its \"TOTAL\" column. Try several parameterizations and initializations:<br>\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)<br>\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution <br>\n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution <br>\n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes. Use the (row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!). <br>\n",
    "<br>\n",
    "Note that you do not have to compute the aggregated distribution or the class-aggregated distributions, which are rows in the auxiliary file:<br>\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt*<br>\n",
    "- *Row 1: Words*<br>\n",
    "- *Row 2: Aggregated distribution across all classes*<br>\n",
    "- *Row 3-6 class-aggregated distributions for clases 0-3*<br>\n",
    "\n",
    "*For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]<br>\n",
    "For (B), (C), and (D)  you will have to use data from the auxiliary file: <br>\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt<br>\n",
    "This file contains 5 special word-frequency distributions:<br>\n",
    "(1) The 1000-user-wide aggregate, which you will perturb for initializations in parts (B) and (C), and <br>\n",
    "(2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)<br>\n",
    "<br>\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate (after initially normalizing by its sum, which is also provided). So if in (B) you want to create 2 perturbations of the aggregate, start with (1), normalize, and generate 1000 random numbers uniformly from the unit interval (0,1) twice (for two centroids), using:*<br>\n",
    "```\n",
    "from numpy import random\n",
    "numbers = random.sample(1000)\n",
    "```\n",
    "*Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate, and then renormalize to obtain one of your aggregate-perturbed initial centroids.*<br>\n",
    "<br>\n",
    "\n",
    "```\n",
    "###########################################\n",
    "## Geneate random initial centroids around the global aggregate\n",
    "## Part (B) and (C) of this question\n",
    "###########################################\n",
    "\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "```\n",
    "<br>\n",
    "*For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached. After convergence, print out a summary of the classes present in each cluster. In particular, report the composition as measured by the total portion of each class type (0-3) contained in each cluster, and discuss your findings and any differences in outcomes across parts A-D.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A. K=4 uniform random centroid-distributions over the 1000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize the data\n",
    "Every part of the question requires us to normalize the data. Rather than doing this for each step, let's do it once at the beginning and write to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1        2         3         4         5         6     \\\n",
      "0  1180025371     2  1724608  0.043808  0.000480  0.033401  0.004133   \n",
      "1   284534859     2   827765  0.120714  0.000000  0.017442  0.034623   \n",
      "2  1602852614     2   987334  0.000000  0.002734  0.000000  0.000000   \n",
      "3  2361533634     2   416584  0.134612  0.000132  0.000007  0.000000   \n",
      "4   485013829     1   530484  0.102629  0.000019  0.000000  0.000000   \n",
      "\n",
      "       7         8         9     ...       993   994   995   996   997   \\\n",
      "0  0.002483  0.026484  0.038740  ...   0.000102   0.0   0.0   0.0   0.0   \n",
      "1  0.009022  0.031469  0.033303  ...   0.000000   0.0   0.0   0.0   0.0   \n",
      "2  0.000000  0.000000  0.000000  ...   0.000000   0.0   0.0   0.0   0.0   \n",
      "3  0.000000  0.000000  0.000007  ...   0.000000   0.0   0.0   0.0   0.0   \n",
      "4  0.000000  0.000000  0.015812  ...   0.000000   0.0   0.0   0.0   0.0   \n",
      "\n",
      "       998   999   1000  1001  1002  \n",
      "0  0.000070   0.0   0.0   0.0   0.0  \n",
      "1  0.000000   0.0   0.0   0.0   0.0  \n",
      "2  0.000000   0.0   0.0   0.0   0.0  \n",
      "3  0.001359   0.0   0.0   0.0   0.0  \n",
      "4  0.000000   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 1003 columns]\n"
     ]
    }
   ],
   "source": [
    "# import pandas to allow us to act efficiently\n",
    "# with this data set\n",
    "import pandas as pd\n",
    "\n",
    "# read in the twitter data\n",
    "raw_data = \\\n",
    "pd.read_csv(\\\n",
    "            \"topUsers_Apr-Jul_2014_1000-words.txt\",\\\n",
    "            header=None)\n",
    "\n",
    "# divide each word count by the total and rename\n",
    "# the file to reflect it's normalized state\n",
    "raw_data.ix[:,3:] = raw_data.ix[:,3:].\\\n",
    "div(raw_data[2],'index')\n",
    "norm_data = raw_data\n",
    "\n",
    "# write the file to the local drive and\n",
    "# show the first couple lines\n",
    "norm_data.to_csv('twitter_users_norm.txt',header=False,index=False)\n",
    "print norm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the initial centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0                 1                  2                  3    \\\n",
      "0  0.00573363807452  0.00695318771852    0.0125332853702    0.0256851193009   \n",
      "1   0.0920348952663     0.01482843389  2.73903188918e-05  6.16282175065e-05   \n",
      "2    0.039549886789   0.0524525253445   0.00240075549841   0.00323134863293   \n",
      "3  0.00573363807452  0.00695318771852    0.0125332853702    0.0256851193009   \n",
      "\n",
      "               4                  5                 6                  7    \\\n",
      "0  0.0480756998153     0.016718358609  0.00196970428115   0.00099581931363   \n",
      "1              0.0  2.73903188918e-05   0.0155405821812    0.0307319377966   \n",
      "2  0.0204348667069  0.000785081181945  0.00225284165254  0.000614411359783   \n",
      "3  0.0480756998153     0.016718358609  0.00196970428115   0.00099581931363   \n",
      "\n",
      "               8                9    ...                  990  991  \\\n",
      "0              0.0  0.0455400894044  ...                  0.0  0.0   \n",
      "1  0.0920348952663              0.0  ...    2.05427391688e-05  0.0   \n",
      "2              0.0  0.0313577353252  ...    3.41339644324e-05  0.0   \n",
      "3              0.0  0.0455400894044  ...                  0.0  0.0   \n",
      "\n",
      "                 992                993                994                995  \\\n",
      "0  4.38686922304e-06  6.14161691226e-05  0.000127219207468  1.75474768922e-05   \n",
      "1                0.0                0.0                0.0                0.0   \n",
      "2  1.13779881441e-05                0.0  1.13779881441e-05                0.0   \n",
      "3  4.38686922304e-06  6.14161691226e-05  0.000127219207468  1.75474768922e-05   \n",
      "\n",
      "                 996  997                998    999  \n",
      "0  1.75474768922e-05  0.0                0.0  0.0\\n  \n",
      "1                0.0  0.0                0.0  0.0\\n  \n",
      "2                0.0  0.0  1.13779881441e-05  0.0\\n  \n",
      "3  1.75474768922e-05  0.0                0.0  0.0\\n  \n",
      "\n",
      "[4 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "# import libraries to help us get started\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# begin by creating the centroids\n",
    "# define how many centroids we need\n",
    "# and create a list to store them\n",
    "K = 4\n",
    "centroid_index = []\n",
    "\n",
    "# grab the number of users\n",
    "with open('twitter_users_norm.txt','r') as myfile:\n",
    "    lines = myfile.readlines()\n",
    "    num_users = len(lines)\n",
    "\n",
    "# loop through each possible centroid\n",
    "for point in range(K):\n",
    "    \n",
    "    # get the number of a random user\n",
    "    _user = \\\n",
    "    np.random.randint(0,num_users-1)\n",
    "    \n",
    "    # add that user to our list\n",
    "    centroid_index.append(_user)\n",
    "\n",
    "\n",
    "# create an array to hold the centroid values\n",
    "centroids = []\n",
    "    \n",
    "# pull the centroid values from the randomly\n",
    "# selected users and write it to a local file\n",
    "with open('centroids.txt','w') as myfile:\n",
    "\n",
    "    # loop through our indexes    \n",
    "    for index in centroid_index:\n",
    "        \n",
    "        # set the centroid as the line from \n",
    "        # the 3rd element on\n",
    "        centroid = lines[index].split(\",\")[3:]\n",
    "        centroids.append(centroid)\n",
    "\n",
    "# convert our array to a pandas data frame\n",
    "# and write that data frame to an output file\n",
    "centroids = pd.DataFrame(centroids)\n",
    "centroids.to_csv('centroids.txt',header=False,index=False)\n",
    "\n",
    "# print the first couple lines of our data frame\n",
    "print centroids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count the members in each class and write to a file\n",
    "This is useful for when we have to calculate the purities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    752\n",
      "3    103\n",
      "1     91\n",
      "2     54\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import pandas to help us process the data\n",
    "import pandas as pd\n",
    "\n",
    "# open the file as a pandas dataframe\n",
    "data = \\\n",
    "pd.read_csv(\\\n",
    "            \"topUsers_Apr-Jul_2014_1000-words.txt\",\\\n",
    "            header=None)\n",
    "\n",
    "# store the counts for each class\n",
    "classes = data.ix[:,1].value_counts()\n",
    "\n",
    "# print the classes\n",
    "print classes\n",
    "\n",
    "# create a blank array to hold the classes\n",
    "classes_out = []\n",
    "\n",
    "# loop through and add to the array each element\n",
    "for i in range(len(classes)):\n",
    "    classes_out.append(classes[i])\n",
    "\n",
    "# save the classes as a pandas dataframe and write\n",
    "# the file to the disk\n",
    "classes = classes_out\n",
    "classes = pd.DataFrame(classes)\n",
    "classes.to_csv('class_counts.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Create the MRJob class that finds the next closest centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_kmeans.py\n",
    "# import MRJob and some other libraries\n",
    "# to help us get started\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# define a function that will find which centroid\n",
    "# is closest to a given point\n",
    "def ClosestCentroid(point,centroid_points):\n",
    "    \"\"\"takes a point, a list of coordinates, and \n",
    "    compares that point to each of a number of \n",
    "    centroids stored in a list of lists. returns\n",
    "    the index of the centroid closest to the data\n",
    "    point\"\"\"\n",
    "    \n",
    "    # convert our inputs into numpy arrays\n",
    "    point = np.array(point)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    \n",
    "    # calculate the difference between the point\n",
    "    # and each of the centroid points\n",
    "    difference = point - centroid_points\n",
    "    \n",
    "    # square the difference, this will help us\n",
    "    # calculate distance regardless of direction\n",
    "    diff_sq = difference * difference\n",
    "    \n",
    "    # get the index of the centroid that is \n",
    "    # closest to the data point\n",
    "    closest_index = \\\n",
    "    np.argmin(list(diff_sq.sum(axis=1)))\n",
    "    \n",
    "    # return the closest index\n",
    "    return int(closest_index)\n",
    "    \n",
    "\n",
    "# create the MRJob class\n",
    "class MRKmeans(MRJob):\n",
    "    \"\"\"class responsible for find the nearest centroid\n",
    "    to a number of data points\"\"\"\n",
    "    \n",
    "    # create an array to hold our centroid\n",
    "    # points and set a value for K, number\n",
    "    # of centroids\n",
    "    centroid_points = []\n",
    "    K=4\n",
    "    \n",
    "    # read in the class count file\n",
    "    classes_pd = pd.read_csv('class_counts.txt',\\\n",
    "                         header=None)\n",
    "\n",
    "    # set the class counts\n",
    "    class_counts = map(float,classes_pd.values)\n",
    "    \n",
    "    # set the number of true classifications\n",
    "    # and set the number of dimensions in our \n",
    "    # data\n",
    "    TRUTHS = 4\n",
    "    DIMS = 1000\n",
    "    \n",
    "    # create an empty array to hold the counts\n",
    "    # for each class\n",
    "    classes = [0] * TRUTHS\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                      mapper=self.mapper,\\\n",
    "                      combiner=self.combiner,\\\n",
    "                      reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # load the initial centroids from a \n",
    "    # data file passed in\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # read in the centroids data\n",
    "        centroids = pd.read_csv('centroids.txt',\\\n",
    "                                header=None)\n",
    "        \n",
    "        # set the centroid points based on the \n",
    "        # inputted file\n",
    "        self.centroid_points = map(list,centroids.values)\n",
    "\n",
    "    \n",
    "    # takes a line of the twitter data and \n",
    "    # returns the index of the closest centroid\n",
    "    # and the coordinates of this point, \n",
    "    # along with this point's true class\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # get all the information for the point\n",
    "        point = map(float,line.split(','))\n",
    "        \n",
    "        # get the point's true classification\n",
    "        # and simplify the point to just it's\n",
    "        # coordinates\n",
    "        truth = int(point[1])\n",
    "        point = point[3:]\n",
    "        \n",
    "        # grab the closest centroid\n",
    "        closest = \\\n",
    "        ClosestCentroid(point,self.centroid_points)\n",
    "        \n",
    "        # create an array of zeros of the\n",
    "        # length of the true classifications\n",
    "        classify = [0] * self.TRUTHS\n",
    "        \n",
    "        # set the index of the truth to be 1\n",
    "        classify[truth] = 1\n",
    "        \n",
    "        # yield:\n",
    "        # key: the index of the closest cluster\n",
    "        # value: the coordinates of the point &\n",
    "        # the classification\n",
    "        yield closest, (point, classify)\n",
    "        \n",
    "        \n",
    "    # takes the output of the mapper and combines\n",
    "    # the coordinate positions and updates the \n",
    "    # count of points for this centroid\n",
    "    def combiner(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # convert the numpy arrays back to\n",
    "        # regular arrays for the combiner's\n",
    "        # output\n",
    "        coordinates = list(coordinates)\n",
    "        truths = list(truths)\n",
    "        \n",
    "        # yield the key as the centroid and the \n",
    "        # sum of the coordinates and the sum of\n",
    "        # the classifications\n",
    "        yield centroid, (coordinates,truths)\n",
    "        \n",
    "    # takes the outputs of the mappers and \n",
    "    # combiners and computes the aggregate\n",
    "    # sums for each centroid and uses these\n",
    "    # sums to calculate new centroids at\n",
    "    # the centers of the clusters\n",
    "    def reducer(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # gather the complete count for the\n",
    "        # centroid\n",
    "        num_points = float(sum(truths))\n",
    "        \n",
    "        # calculate the new centroid and \n",
    "        # convert it back to a regular list\n",
    "        new_centroid = coordinates / num_points\n",
    "        new_centroid = list(new_centroid)\n",
    "        \n",
    "        # print out the class breakdown\n",
    "        print \"Cluster #\",centroid\n",
    "        for index,item in enumerate(truths):\n",
    "            proportion = float(item) /\\\n",
    "            self.class_counts[index]\n",
    "            \n",
    "            print \"\\tClass\",index,\"\\t\",proportion\n",
    "        \n",
    "        # yield the centroid index and the \n",
    "        # coordinates of the new centroid\n",
    "        yield centroid, new_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a test file that we used to test\n",
    "# each step of the MRJob\n",
    "!head -50 twitter_users_norm.txt > test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a stop function to tell us when we have achieved sufficient convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the chain tool to combine lists\n",
    "from itertools import chain\n",
    "\n",
    "def stop_reached(centroids_old,\\\n",
    "                 centroids_new,thresh=0.5):\n",
    "    \"\"\"a function that compares two lists of \n",
    "    centroids to determine if coordinate has\n",
    "    moved a greater distance than the\n",
    "    threshold, by default set to 0.5\"\"\"\n",
    "    \n",
    "    # convert the lists of centroids into a \n",
    "    # single list because we don't care about \n",
    "    # the context of the coordinates\n",
    "    centroids_old = list(chain(*centroids_old))\n",
    "    centroids_new = list(chain(*centroids_new))\n",
    "    \n",
    "    # calculate the difference between each\n",
    "    # of the coordinates\n",
    "    difference = [abs(old-new) for old,new in\\\n",
    "                 zip(centroids_old,centroids_new)]\n",
    "    \n",
    "    # set the flag for stopping to true\n",
    "    # by default\n",
    "    stopping = True\n",
    "    \n",
    "    # loop through each difference\n",
    "    for diff in difference:\n",
    "        \n",
    "        # if the difference is greater\n",
    "        # than the threshold, then break\n",
    "        # out of the loop and set the\n",
    "        # indicator for stopping to \n",
    "        # false\n",
    "        if diff > thresh:\n",
    "            stopping = False\n",
    "            break\n",
    "    \n",
    "    # return whether or not we reached the\n",
    "    # threshold or we need to keep going\n",
    "    return stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the MRJob in the notebook and print out the answer to part (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 0\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.797872340426\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.0740740740741\n",
      "\tClass 3 \t0.368932038835\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.202127659574\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.888888888889\n",
      "\tClass 3 \t0.631067961165\n",
      "Index: 0\n",
      "Coordinates sample: [0.011329639554444023, 0.04460399573929679, 0.02673750226515437, 0.02788322375644552]\n",
      "Index: 1\n",
      "Coordinates sample: [0.12472895964830379, 0.002487320646417646, 0.0008340059164346096, 0.0008403320409112254]\n",
      "Index: 2\n",
      "Coordinates sample: [0.05272958802582364, 0.041476597372466326, 0.02219005224033261, 0.02146342433378848]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 1\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.957446808511\n",
      "\tClass 1 \t0.021978021978\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.572815533981\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.604395604396\n",
      "\tClass 2 \t0.0555555555556\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.0425531914894\n",
      "\tClass 1 \t0.373626373626\n",
      "\tClass 2 \t0.907407407407\n",
      "\tClass 3 \t0.427184466019\n",
      "Index: 0\n",
      "Coordinates sample: [0.012123849884720756, 0.04754885431536065, 0.02553344950506735, 0.027376907364399735]\n",
      "Index: 1\n",
      "Coordinates sample: [0.13380382094462412, 0.0025006046736551417, 0.0012037710242803847, 0.0017795507220235507]\n",
      "Index: 2\n",
      "Coordinates sample: [0.079176049580193, 0.02548144435592435, 0.024709291630817858, 0.018690809192939747]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 2\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.990691489362\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.844660194175\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.593406593407\n",
      "\tClass 2 \t0.0555555555556\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.0093085106383\n",
      "\tClass 1 \t0.373626373626\n",
      "\tClass 2 \t0.907407407407\n",
      "\tClass 3 \t0.155339805825\n",
      "Index: 0\n",
      "Coordinates sample: [0.013745081574133258, 0.04776693858166882, 0.025157575324386033, 0.02697235890854639]\n",
      "Index: 1\n",
      "Coordinates sample: [0.13085062239030174, 0.0025295667372873375, 0.0008059181608256544, 0.0008011263368923851]\n",
      "Index: 2\n",
      "Coordinates sample: [0.1026365019243255, 0.012285139204803517, 0.02724961480658368, 0.017826815681354538]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 3\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.148148148148\n",
      "\tClass 3 \t0.922330097087\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0185185185185\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.833333333333\n",
      "\tClass 3 \t0.0776699029126\n",
      "Index: 0\n",
      "Coordinates sample: [0.014397571611987205, 0.047388046524136675, 0.02486042764029234, 0.026662811484961094]\n",
      "Index: 1\n",
      "Coordinates sample: [0.1139868247017923, 0.002217108452177485, 0.00021826160449335208, 0.0005053564866081719]\n",
      "Index: 2\n",
      "Coordinates sample: [0.12721488133856046, 0.0076977158449778645, 0.02939066989024009, 0.01796552846114821]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 4\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.240740740741\n",
      "\tClass 3 \t0.95145631068\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.759259259259\n",
      "\tClass 3 \t0.0485436893204\n",
      "Index: 0\n",
      "Coordinates sample: [0.014566847899000618, 0.04706320932510353, 0.02476222185169291, 0.026499363912542147]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.1388927254530536, 0.007172275985086444, 0.030483518405095462, 0.01860646128841889]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 5\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.95145631068\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0485436893204\n",
      "Index: 0\n",
      "Coordinates sample: [0.014600613913121866, 0.04700941754673868, 0.02477219692305493, 0.0264735368331374]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.1400383219509458, 0.00725291148778457, 0.03044837243827492, 0.01878083897685731]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 6\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.961165048544\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0388349514563\n",
      "Index: 0\n",
      "Coordinates sample: [0.014694596354347216, 0.04700762291410899, 0.024756628910398697, 0.026451644460645893]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.14057435770089027, 0.0067870510052356, 0.0306821973466673, 0.01891849738044094]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 7\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.961165048544\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0388349514563\n",
      "Index: 0\n",
      "Coordinates sample: [0.014694596354347216, 0.04700762291410899, 0.024756628910398697, 0.026451644460645893]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.14057435770089027, 0.0067870510052356, 0.0306821973466673, 0.01891849738044094]\n"
     ]
    }
   ],
   "source": [
    "# import the MRJob that we created\n",
    "from mr_kmeans import MRKmeans \n",
    "\n",
    "# import pandas to help us save and load \n",
    "# the centroids\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRKmeans(args=['twitter_users_norm.txt',\\\n",
    "                        '--file=centroids.txt',\\\n",
    "                       '--file=class_counts.txt']) \n",
    "\n",
    "# read in the centroids data to get the original\n",
    "# centroids and convert it to a list\n",
    "centroids = pd.read_csv('centroids.txt',\\\n",
    "                        header=None)\n",
    "centroids = map(list,centroids.values)\n",
    "\n",
    "# create a counter to count our iterations\n",
    "# and an initial stopping indicator\n",
    "iteration = 0\n",
    "stopping = False\n",
    "\n",
    "# set up a loop that runs until we tell\n",
    "# it to stop\n",
    "while stopping == False:\n",
    "    \n",
    "    # set the old centroids\n",
    "    old_centroids = centroids[:]\n",
    "    \n",
    "    # create a new array to hold the \n",
    "    # new centroid points\n",
    "    new_centroids = []\n",
    "    \n",
    "    # print the iteration we are on\n",
    "    print \"\\n*~*~*~*~*~*~*~*\\n\"\n",
    "    print \"Iteration:\", iteration\n",
    "\n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run() \n",
    "\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output(): \n",
    "\n",
    "            # set the centroid\n",
    "            index,coordinates = mr_job.parse_output_line(line)\n",
    "\n",
    "            # update the current centroid\n",
    "            new_centroids.append(coordinates)\n",
    "\n",
    "            # print out the centroid values\n",
    "            print \"Index:\", index\n",
    "            print \"Coordinates sample:\", coordinates[0:4]\n",
    "\n",
    "        # set the new centroids as a regular list\n",
    "        new_centroids = new_centroids[:]\n",
    "        centroids = new_centroids[:]\n",
    "        \n",
    "        # convert our array to a pandas data frame\n",
    "        # and write that data frame to an output file\n",
    "        # and update the centroids file\n",
    "        centroids_pd = pd.DataFrame(centroids)\n",
    "        centroids_pd.to_csv('HW4.5_A_Output',\\\n",
    "                         header=False,index=False)\n",
    "        centroids_pd.to_csv('centroids.txt',\\\n",
    "                         header=False,index=False)\n",
    "        \n",
    "        # check the stopping condition with our\n",
    "        # new and old centroids\n",
    "        stopping = stop_reached(old_centroids,\\\n",
    "                                new_centroids,thresh=0.001)\n",
    "        \n",
    "        # iterate the iteration count by 1\n",
    "        iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. K=2, Perturbation centroids from the aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the function to generate the random centroids from the aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the numpy library to help us \n",
    "# with randomization\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# generate 1000 random numbers\n",
    "numbers = np.random.sample(1000)\n",
    "\n",
    "def aggregateCentroids(k):\n",
    "    \"\"\"generates k centroid points from the\n",
    "    aggregate data that is randomly perturbed\"\"\"\n",
    "    \n",
    "    # initalize a counter at zero\n",
    "    counter = 0\n",
    "    \n",
    "    # loop through each line in the summary data\n",
    "    for line in \\\n",
    "    open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\")\\\n",
    "    .readlines():\n",
    "        \n",
    "        # if it's the third line\n",
    "        if counter == 1:        \n",
    "            \n",
    "            # split the line by commas\n",
    "            data = re.split(\",\",line)\n",
    "            \n",
    "            # calculate the global aggregate as\n",
    "            # the normalized count for each word\n",
    "            globalAggregate = \\\n",
    "            [float(data[i+3])/float(data[2]) \\\n",
    "             for i in range(1000)]\n",
    "        \n",
    "        # increment our line counter by 1\n",
    "        counter += 1\n",
    "        \n",
    "    # create an empty array to hold the future\n",
    "    # centroid points\n",
    "    centroids = []\n",
    "    \n",
    "    # loop the number of centroids needed\n",
    "    for i in range(k):\n",
    "        \n",
    "        # generate a set of 1000 random points\n",
    "        rndpoints = np.random.sample(1000)\n",
    "        \n",
    "        # peturb the aggregate coordinates by\n",
    "        # the random points generated above\n",
    "        peturpoints = \\\n",
    "        [rndpoints[n]/10+globalAggregate[n] \\\n",
    "         for n in range(1000)]\n",
    "        \n",
    "        # append our preturbed centroid to the\n",
    "        # list of centroids\n",
    "        centroids.append(peturpoints)\n",
    "        \n",
    "        # renormalize, start by initalizing a \n",
    "        # new total\n",
    "        total = 0\n",
    "        \n",
    "        # total up the values of the centroid\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "            \n",
    "        # renormalize the centroids by dividing\n",
    "        # by the total\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "            \n",
    "    # return the new centroids\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the random centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. We got 2 centroids\n"
     ]
    }
   ],
   "source": [
    "# import pandas to help us write to csvs\n",
    "import pandas as pd\n",
    "\n",
    "# generate 2 centroids from the aggregate\n",
    "# data \n",
    "centroids = aggregateCentroids(2)\n",
    "\n",
    "# write the data to a centroids file\n",
    "centroids_pd = pd.DataFrame(centroids)\n",
    "centroids_pd.to_csv('centroids.txt',\\\n",
    "                    header=False,index=False)\n",
    "\n",
    "# read the first couple lines\n",
    "print \"Done. We got\", len(centroids), \"centroids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the MRJob class to find the best centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_kmeans.py\n",
    "# import MRJob and some other libraries\n",
    "# to help us get started\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# define a function that will find which centroid\n",
    "# is closest to a given point\n",
    "def ClosestCentroid(point,centroid_points):\n",
    "    \"\"\"takes a point, a list of coordinates, and \n",
    "    compares that point to each of a number of \n",
    "    centroids stored in a list of lists. returns\n",
    "    the index of the centroid closest to the data\n",
    "    point\"\"\"\n",
    "    \n",
    "    # convert our inputs into numpy arrays\n",
    "    point = np.array(point)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    \n",
    "    # calculate the difference between the point\n",
    "    # and each of the centroid points\n",
    "    difference = point - centroid_points\n",
    "    \n",
    "    # square the difference, this will help us\n",
    "    # calculate distance regardless of direction\n",
    "    diff_sq = difference * difference\n",
    "    \n",
    "    # get the index of the centroid that is \n",
    "    # closest to the data point\n",
    "    closest_index = \\\n",
    "    np.argmin(list(diff_sq.sum(axis=1)))\n",
    "    \n",
    "    # return the closest index\n",
    "    return int(closest_index)\n",
    "    \n",
    "\n",
    "# create the MRJob class\n",
    "class MRKmeans(MRJob):\n",
    "    \"\"\"class responsible for find the nearest centroid\n",
    "    to a number of data points\"\"\"\n",
    "    \n",
    "    # create an array to hold our centroid\n",
    "    # points and set a value for K, number\n",
    "    # of centroids\n",
    "    centroid_points = []\n",
    "    K=2\n",
    "    \n",
    "    # set the number of true classifications\n",
    "    # and set the number of dimensions in our \n",
    "    # data\n",
    "    TRUTHS = 4\n",
    "    DIMS = 1000\n",
    "    \n",
    "    # read in the class count file\n",
    "    classes_pd = pd.read_csv('class_counts.txt',\\\n",
    "                         header=None)\n",
    "\n",
    "    # set the class counts\n",
    "    class_counts = map(float,classes_pd.values)\n",
    "    \n",
    "    # create an empty array to hold the counts\n",
    "    # for each class\n",
    "    classes = [0] * TRUTHS\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                      mapper=self.mapper,\\\n",
    "                      combiner=self.combiner,\\\n",
    "                      reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # load the initial centroids from a \n",
    "    # data file passed in\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # read in the centroids data\n",
    "        centroids = pd.read_csv('Centroids.txt',\\\n",
    "                                header=None)\n",
    "        \n",
    "        # set the centroid points based on the \n",
    "        # inputted file\n",
    "        self.centroid_points = map(list,centroids.values)\n",
    "\n",
    "    \n",
    "    # takes a line of the twitter data and \n",
    "    # returns the index of the closest centroid\n",
    "    # and the coordinates of this point, \n",
    "    # along with this point's true class\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # get all the information for the point\n",
    "        point = map(float,line.split(','))\n",
    "        \n",
    "        # get the point's true classification\n",
    "        # and simplify the point to just it's\n",
    "        # coordinates\n",
    "        truth = int(point[1])\n",
    "        point = point[3:]\n",
    "        \n",
    "        # grab the closest centroid\n",
    "        closest = \\\n",
    "        ClosestCentroid(point,self.centroid_points)\n",
    "        \n",
    "        # create an array of zeros of the\n",
    "        # length of the true classifications\n",
    "        classify = [0] * self.TRUTHS\n",
    "        \n",
    "        # set the index of the truth to be 1\n",
    "        classify[truth] = 1\n",
    "        \n",
    "        # yield:\n",
    "        # key: the index of the closest cluster\n",
    "        # value: the coordinates of the point &\n",
    "        # the classification\n",
    "        yield closest, (point, classify)\n",
    "        \n",
    "        \n",
    "    # takes the output of the mapper and combines\n",
    "    # the coordinate positions and updates the \n",
    "    # count of points for this centroid\n",
    "    def combiner(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # convert the numpy arrays back to\n",
    "        # regular arrays for the combiner's\n",
    "        # output\n",
    "        coordinates = list(coordinates)\n",
    "        truths = list(truths)\n",
    "        \n",
    "        # yield the key as the centroid and the \n",
    "        # sum of the coordinates and the sum of\n",
    "        # the classifications\n",
    "        yield centroid, (coordinates,truths)\n",
    "        \n",
    "    # takes the outputs of the mappers and \n",
    "    # combiners and computes the aggregate\n",
    "    # sums for each centroid and uses these\n",
    "    # sums to calculate new centroids at\n",
    "    # the centers of the clusters\n",
    "    def reducer(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # gather the complete count for the\n",
    "        # centroid\n",
    "        num_points = float(sum(truths))\n",
    "        \n",
    "        # calculate the new centroid and \n",
    "        # convert it back to a regular list\n",
    "        new_centroid = coordinates / num_points\n",
    "        new_centroid = list(new_centroid)\n",
    "        \n",
    "        # print out the class breakdown\n",
    "        print \"Cluster #\",centroid\n",
    "        for index,item in enumerate(truths):\n",
    "            proportion = float(item) /\\\n",
    "            self.class_counts[index]\n",
    "            \n",
    "            print \"\\tClass\",index,\"\\t\",proportion\n",
    "        \n",
    "        # yield the centroid index and the \n",
    "        # coordinates of the new centroid\n",
    "        yield centroid, new_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a copy of the stop function to tell us when we have achieved sufficient convergence\n",
    "We put a copy down here because we don't want to have to scroll each time to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the chain tool to combine lists\n",
    "from itertools import chain\n",
    "\n",
    "def stop_reached(centroids_old,\\\n",
    "                 centroids_new,thresh=0.5):\n",
    "    \"\"\"a function that compares two lists of \n",
    "    centroids to determine if coordinate has\n",
    "    moved a greater distance than the\n",
    "    threshold, by default set to 0.5\"\"\"\n",
    "    \n",
    "    # convert the lists of centroids into a \n",
    "    # single list because we don't care about \n",
    "    # the context of the coordinates\n",
    "    centroids_old = list(chain(*centroids_old))\n",
    "    centroids_new = list(chain(*centroids_new))\n",
    "    \n",
    "    # calculate the difference between each\n",
    "    # of the coordinates\n",
    "    difference = [abs(old-new) for old,new in\\\n",
    "                 zip(centroids_old,centroids_new)]\n",
    "    \n",
    "    # set the flag for stopping to true\n",
    "    # by default\n",
    "    stopping = True\n",
    "    \n",
    "    # loop through each difference\n",
    "    for diff in difference:\n",
    "        \n",
    "        # if the difference is greater\n",
    "        # than the threshold, then break\n",
    "        # out of the loop and set the\n",
    "        # indicator for stopping to \n",
    "        # false\n",
    "        if diff > thresh:\n",
    "            stopping = False\n",
    "            break\n",
    "    \n",
    "    # return whether or not we reached the\n",
    "    # threshold or we need to keep going\n",
    "    return stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a runner to run the MRJob class in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 0\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.803191489362\n",
      "\tClass 1 \t0.582417582418\n",
      "\tClass 2 \t0.185185185185\n",
      "\tClass 3 \t0.388349514563\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.196808510638\n",
      "\tClass 1 \t0.417582417582\n",
      "\tClass 2 \t0.814814814815\n",
      "\tClass 3 \t0.611650485437\n",
      "Index: 0\n",
      "Coordinates sample: [0.019651753250353256, 0.04433178480303183, 0.024602159831210973, 0.025342965897744065]\n",
      "Index: 1\n",
      "Coordinates sample: [0.054432759077732026, 0.034418995893678764, 0.02251726979696211, 0.022504075700255012]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 1\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.974734042553\n",
      "\tClass 1 \t0.021978021978\n",
      "\tClass 2 \t0.12962962963\n",
      "\tClass 3 \t0.621359223301\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0252659574468\n",
      "\tClass 1 \t0.978021978022\n",
      "\tClass 2 \t0.87037037037\n",
      "\tClass 3 \t0.378640776699\n",
      "Index: 0\n",
      "Coordinates sample: [0.012301863884069024, 0.04763117086527928, 0.025192921102346992, 0.02690762951496705]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10271796735678142, 0.0156526491503931, 0.018998931147857532, 0.01455475093204292]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 2\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.240740740741\n",
      "\tClass 3 \t0.922330097087\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.967032967033\n",
      "\tClass 2 \t0.759259259259\n",
      "\tClass 3 \t0.0776699029126\n",
      "Index: 0\n",
      "Coordinates sample: [0.014353908136708856, 0.047148667719523356, 0.024721456271876878, 0.0265098310180887]\n",
      "Index: 1\n",
      "Coordinates sample: [0.1265907184342913, 0.005689754191030542, 0.019430374962450526, 0.012026787915125448]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 3\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.961165048544\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.967032967033\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0388349514563\n",
      "Index: 0\n",
      "Coordinates sample: [0.014694596354347216, 0.04700762291410899, 0.024756628910398697, 0.026451644460645893]\n",
      "Index: 1\n",
      "Coordinates sample: [0.12858927006433232, 0.005050590872622954, 0.019002178841055435, 0.011861618966163773]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 4\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.998670212766\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.961165048544\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.967032967033\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0388349514563\n",
      "Index: 0\n",
      "Coordinates sample: [0.014694596354347216, 0.04700762291410899, 0.024756628910398697, 0.026451644460645893]\n",
      "Index: 1\n",
      "Coordinates sample: [0.12858927006433232, 0.005050590872622954, 0.019002178841055435, 0.011861618966163773]\n"
     ]
    }
   ],
   "source": [
    "# import the MRJob that we created\n",
    "from mr_kmeans import MRKmeans \n",
    "\n",
    "# import pandas to help us save and load \n",
    "# the centroids\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRKmeans(args=['twitter_users_norm.txt','--file=centroids.txt']) \n",
    "\n",
    "# read in the centroids data to get the original\n",
    "# centroids and convert it to a list\n",
    "centroids = pd.read_csv('centroids.txt',\\\n",
    "                        header=None)\n",
    "centroids = map(list,centroids.values)\n",
    "\n",
    "# create a counter to count our iterations\n",
    "# and an initial stopping indicator\n",
    "iteration = 0\n",
    "stopping = False\n",
    "\n",
    "# set up a loop that runs until we tell\n",
    "# it to stop\n",
    "while stopping == False:\n",
    "    \n",
    "    # set the old centroids\n",
    "    old_centroids = centroids[:]\n",
    "    \n",
    "    # create a new array to hold the \n",
    "    # new centroid points\n",
    "    new_centroids = []\n",
    "    \n",
    "    # print the iteration we are on\n",
    "    print \"\\n*~*~*~*~*~*~*~*\\n\"\n",
    "    print \"Iteration:\", iteration\n",
    "\n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run() \n",
    "\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output(): \n",
    "\n",
    "            # set the centroid\n",
    "            index,coordinates = mr_job.parse_output_line(line)\n",
    "\n",
    "            # update the current centroid\n",
    "            new_centroids.append(coordinates)\n",
    "\n",
    "            # print out the centroid values\n",
    "            print \"Index:\", index\n",
    "            print \"Coordinates sample:\", coordinates[0:4]\n",
    "\n",
    "        # set the new centroids as a regular list\n",
    "        new_centroids = new_centroids[:]\n",
    "        centroids = new_centroids[:]\n",
    "        \n",
    "        # convert our array to a pandas data frame\n",
    "        # and write that data frame to an output file\n",
    "        # and update the centroids file\n",
    "        centroids_pd = pd.DataFrame(centroids)\n",
    "        centroids_pd.to_csv('HW4.5_B_Output',\\\n",
    "                         header=False,index=False)\n",
    "        centroids_pd.to_csv('centroids.txt',\\\n",
    "                         header=False,index=False)\n",
    "        \n",
    "        # check the stopping condition with our\n",
    "        # new and old centroids\n",
    "        stopping = stop_reached(old_centroids,\\\n",
    "                                new_centroids,thresh=0.001)\n",
    "        \n",
    "        # iterate the iteration count by 1\n",
    "        iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C. K=4 Petrubation centroids from the aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the random centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. We got 4 centroids\n"
     ]
    }
   ],
   "source": [
    "# import pandas to help us write to csvs\n",
    "import pandas as pd\n",
    "\n",
    "# generate 4 centroids from the aggregate\n",
    "# data \n",
    "centroids = aggregateCentroids(4)\n",
    "\n",
    "# write the data to a centroids file\n",
    "centroids_pd = pd.DataFrame(centroids)\n",
    "centroids_pd.to_csv('centroids.txt',\\\n",
    "                    header=False,index=False)\n",
    "\n",
    "# read the first couple lines\n",
    "print \"Done. We got\", len(centroids), \"centroids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the MRJob class to find the best centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_kmeans.py\n",
    "# import MRJob and some other libraries\n",
    "# to help us get started\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# define a function that will find which centroid\n",
    "# is closest to a given point\n",
    "def ClosestCentroid(point,centroid_points):\n",
    "    \"\"\"takes a point, a list of coordinates, and \n",
    "    compares that point to each of a number of \n",
    "    centroids stored in a list of lists. returns\n",
    "    the index of the centroid closest to the data\n",
    "    point\"\"\"\n",
    "    \n",
    "    # convert our inputs into numpy arrays\n",
    "    point = np.array(point)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    \n",
    "    # calculate the difference between the point\n",
    "    # and each of the centroid points\n",
    "    difference = point - centroid_points\n",
    "    \n",
    "    # square the difference, this will help us\n",
    "    # calculate distance regardless of direction\n",
    "    diff_sq = difference * difference\n",
    "    \n",
    "    # get the index of the centroid that is \n",
    "    # closest to the data point\n",
    "    closest_index = \\\n",
    "    np.argmin(list(diff_sq.sum(axis=1)))\n",
    "    \n",
    "    # return the closest index\n",
    "    return int(closest_index)\n",
    "    \n",
    "\n",
    "# create the MRJob class\n",
    "class MRKmeans(MRJob):\n",
    "    \"\"\"class responsible for find the nearest centroid\n",
    "    to a number of data points\"\"\"\n",
    "    \n",
    "    # create an array to hold our centroid\n",
    "    # points and set a value for K, number\n",
    "    # of centroids\n",
    "    centroid_points = []\n",
    "    K=4\n",
    "    \n",
    "    # set the number of true classifications\n",
    "    # and set the number of dimensions in our \n",
    "    # data\n",
    "    TRUTHS = 4\n",
    "    DIMS = 1000\n",
    "    \n",
    "    # read in the class count file\n",
    "    classes_pd = pd.read_csv('class_counts.txt',\\\n",
    "                         header=None)\n",
    "\n",
    "    # set the class counts\n",
    "    class_counts = map(float,classes_pd.values)\n",
    "    \n",
    "    # create an empty array to hold the counts\n",
    "    # for each class\n",
    "    classes = [0] * TRUTHS\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                      mapper=self.mapper,\\\n",
    "                      combiner=self.combiner,\\\n",
    "                      reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # load the initial centroids from a \n",
    "    # data file passed in\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # read in the centroids data\n",
    "        centroids = pd.read_csv('Centroids.txt',\\\n",
    "                                header=None)\n",
    "        \n",
    "        # set the centroid points based on the \n",
    "        # inputted file\n",
    "        self.centroid_points = map(list,centroids.values)\n",
    "\n",
    "    \n",
    "    # takes a line of the twitter data and \n",
    "    # returns the index of the closest centroid\n",
    "    # and the coordinates of this point, \n",
    "    # along with this point's true class\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # get all the information for the point\n",
    "        point = map(float,line.split(','))\n",
    "        \n",
    "        # get the point's true classification\n",
    "        # and simplify the point to just it's\n",
    "        # coordinates\n",
    "        truth = int(point[1])\n",
    "        point = point[3:]\n",
    "        \n",
    "        # grab the closest centroid\n",
    "        closest = \\\n",
    "        ClosestCentroid(point,self.centroid_points)\n",
    "        \n",
    "        # create an array of zeros of the\n",
    "        # length of the true classifications\n",
    "        classify = [0] * self.TRUTHS\n",
    "        \n",
    "        # set the index of the truth to be 1\n",
    "        classify[truth] = 1\n",
    "        \n",
    "        # yield:\n",
    "        # key: the index of the closest cluster\n",
    "        # value: the coordinates of the point &\n",
    "        # the classification\n",
    "        yield closest, (point, classify)\n",
    "        \n",
    "        \n",
    "    # takes the output of the mapper and combines\n",
    "    # the coordinate positions and updates the \n",
    "    # count of points for this centroid\n",
    "    def combiner(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # convert the numpy arrays back to\n",
    "        # regular arrays for the combiner's\n",
    "        # output\n",
    "        coordinates = list(coordinates)\n",
    "        truths = list(truths)\n",
    "        \n",
    "        # yield the key as the centroid and the \n",
    "        # sum of the coordinates and the sum of\n",
    "        # the classifications\n",
    "        yield centroid, (coordinates,truths)\n",
    "        \n",
    "    # takes the outputs of the mappers and \n",
    "    # combiners and computes the aggregate\n",
    "    # sums for each centroid and uses these\n",
    "    # sums to calculate new centroids at\n",
    "    # the centers of the clusters\n",
    "    def reducer(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # gather the complete count for the\n",
    "        # centroid\n",
    "        num_points = float(sum(truths))\n",
    "        \n",
    "        # calculate the new centroid and \n",
    "        # convert it back to a regular list\n",
    "        new_centroid = coordinates / num_points\n",
    "        new_centroid = list(new_centroid)\n",
    "        \n",
    "        # print out the class breakdown\n",
    "        print \"Cluster #\",centroid\n",
    "        for index,item in enumerate(truths):\n",
    "            proportion = float(item) /\\\n",
    "            self.class_counts[index]\n",
    "            \n",
    "            print \"\\tClass\",index,\"\\t\",proportion\n",
    "        \n",
    "        # yield the centroid index and the \n",
    "        # coordinates of the new centroid\n",
    "        yield centroid, new_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a copy of the stop function to tell us when we have achieved sufficient convergence\n",
    "We put a copy down here because we don't want to have to scroll each time to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the chain tool to combine lists\n",
    "from itertools import chain\n",
    "\n",
    "def stop_reached(centroids_old,\\\n",
    "                 centroids_new,thresh=0.5):\n",
    "    \"\"\"a function that compares two lists of \n",
    "    centroids to determine if coordinate has\n",
    "    moved a greater distance than the\n",
    "    threshold, by default set to 0.5\"\"\"\n",
    "    \n",
    "    # convert the lists of centroids into a \n",
    "    # single list because we don't care about \n",
    "    # the context of the coordinates\n",
    "    centroids_old = list(chain(*centroids_old))\n",
    "    centroids_new = list(chain(*centroids_new))\n",
    "    \n",
    "    # calculate the difference between each\n",
    "    # of the coordinates\n",
    "    difference = [abs(old-new) for old,new in\\\n",
    "                 zip(centroids_old,centroids_new)]\n",
    "    \n",
    "    # set the flag for stopping to true\n",
    "    # by default\n",
    "    stopping = True\n",
    "    \n",
    "    # loop through each difference\n",
    "    for diff in difference:\n",
    "        \n",
    "        # if the difference is greater\n",
    "        # than the threshold, then break\n",
    "        # out of the loop and set the\n",
    "        # indicator for stopping to \n",
    "        # false\n",
    "        if diff > thresh:\n",
    "            stopping = False\n",
    "            break\n",
    "    \n",
    "    # return whether or not we reached the\n",
    "    # threshold or we need to keep going\n",
    "    return stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a runner to run the MRJob class in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 0\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.0252659574468\n",
      "\tClass 1 \t0.10989010989\n",
      "\tClass 2 \t0.314814814815\n",
      "\tClass 3 \t0.0388349514563\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.31914893617\n",
      "\tClass 1 \t0.461538461538\n",
      "\tClass 2 \t0.37037037037\n",
      "\tClass 3 \t0.0970873786408\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.610372340426\n",
      "\tClass 1 \t0.010989010989\n",
      "\tClass 2 \t0.148148148148\n",
      "\tClass 3 \t0.631067961165\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.0452127659574\n",
      "\tClass 1 \t0.417582417582\n",
      "\tClass 2 \t0.166666666667\n",
      "\tClass 3 \t0.233009708738\n",
      "Index: 0\n",
      "Coordinates sample: [0.059113509022869604, 0.013895849784602338, 0.027713639159403936, 0.012885151120974783]\n",
      "Index: 1\n",
      "Coordinates sample: [0.031376541404044056, 0.033644801373359755, 0.026363983237804867, 0.02227330736146639]\n",
      "Index: 2\n",
      "Coordinates sample: [0.014840876675482475, 0.05195407622060502, 0.02251242086039605, 0.027094200815265014]\n",
      "Index: 3\n",
      "Coordinates sample: [0.08749756495750337, 0.024227090564671958, 0.022675447661139495, 0.023585072214450022]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 1\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.538461538462\n",
      "\tClass 2 \t0.5\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0984042553191\n",
      "\tClass 1 \t0.021978021978\n",
      "\tClass 2 \t0.111111111111\n",
      "\tClass 3 \t0.0679611650485\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.897606382979\n",
      "\tClass 1 \t0.010989010989\n",
      "\tClass 2 \t0.0185185185185\n",
      "\tClass 3 \t0.78640776699\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00398936170213\n",
      "\tClass 1 \t0.428571428571\n",
      "\tClass 2 \t0.37037037037\n",
      "\tClass 3 \t0.145631067961\n",
      "Index: 0\n",
      "Coordinates sample: [0.09345373621417893, 0.004040559200611162, 0.011447188819297294, 0.0021880530640920382]\n",
      "Index: 1\n",
      "Coordinates sample: [0.02308316910131965, 0.02476676950214827, 0.03482330086368247, 0.02476991001744669]\n",
      "Index: 2\n",
      "Coordinates sample: [0.012795635043544444, 0.05020126555698912, 0.023791936377802397, 0.026974152054583797]\n",
      "Index: 3\n",
      "Coordinates sample: [0.14268325444770782, 0.01121368020072684, 0.02581479415152595, 0.02199934789716243]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 2\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.18085106383\n",
      "\tClass 1 \t0.021978021978\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.184466019417\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.817819148936\n",
      "\tClass 1 \t0.010989010989\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.766990291262\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.703703703704\n",
      "\tClass 3 \t0.0485436893204\n",
      "Index: 0\n",
      "Coordinates sample: [0.10519387900156982, 0.0021734038420666737, 0.00021414346101234543, 0.0004958214585589612]\n",
      "Index: 1\n",
      "Coordinates sample: [0.018340617328336556, 0.02298504918701455, 0.031964945169413735, 0.02575739184247389]\n",
      "Index: 2\n",
      "Coordinates sample: [0.01368041163398271, 0.05292044918632553, 0.02300247037610911, 0.02664973941357403]\n",
      "Index: 3\n",
      "Coordinates sample: [0.1434960582954136, 0.007431995722050855, 0.03120018410341751, 0.019244563396038975]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 3\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.191489361702\n",
      "\tClass 1 \t0.021978021978\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.223300970874\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.807180851064\n",
      "\tClass 1 \t0.010989010989\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.73786407767\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.703703703704\n",
      "\tClass 3 \t0.0388349514563\n",
      "Index: 0\n",
      "Coordinates sample: [0.10519387900156982, 0.0021734038420666737, 0.00021414346101234543, 0.0004958214585589612]\n",
      "Index: 1\n",
      "Coordinates sample: [0.016697865673743068, 0.022807412658713604, 0.031003003582612893, 0.026057495946701828]\n",
      "Index: 2\n",
      "Coordinates sample: [0.014158633948719376, 0.053482240570157824, 0.023085449721780008, 0.026557096475341452]\n",
      "Index: 3\n",
      "Coordinates sample: [0.14408871664341252, 0.00695672728036649, 0.03144925228033398, 0.01939145981495196]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 4\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.186170212766\n",
      "\tClass 1 \t0.021978021978\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.233009708738\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.8125\n",
      "\tClass 1 \t0.010989010989\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.728155339806\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.703703703704\n",
      "\tClass 3 \t0.0388349514563\n",
      "Index: 0\n",
      "Coordinates sample: [0.10519387900156982, 0.0021734038420666737, 0.00021414346101234543, 0.0004958214585589612]\n",
      "Index: 1\n",
      "Coordinates sample: [0.015853401419623457, 0.022236491341875716, 0.030831595049569054, 0.025806544490375744]\n",
      "Index: 2\n",
      "Coordinates sample: [0.014390979306676582, 0.05349787572779463, 0.023164934725463254, 0.026620666286917543]\n",
      "Index: 3\n",
      "Coordinates sample: [0.14408871664341252, 0.00695672728036649, 0.03144925228033398, 0.01939145981495196]\n"
     ]
    }
   ],
   "source": [
    "# import the MRJob that we created\n",
    "from mr_kmeans import MRKmeans \n",
    "\n",
    "# import pandas to help us save and load \n",
    "# the centroids\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRKmeans(args=['twitter_users_norm.txt','--file=centroids.txt']) \n",
    "\n",
    "# read in the centroids data to get the original\n",
    "# centroids and convert it to a list\n",
    "centroids = pd.read_csv('centroids.txt',\\\n",
    "                        header=None)\n",
    "centroids = map(list,centroids.values)\n",
    "\n",
    "# create a counter to count our iterations\n",
    "# and an initial stopping indicator\n",
    "iteration = 0\n",
    "stopping = False\n",
    "\n",
    "# set up a loop that runs until we tell\n",
    "# it to stop\n",
    "while stopping == False:\n",
    "    \n",
    "    # set the old centroids\n",
    "    old_centroids = centroids[:]\n",
    "    \n",
    "    # create a new array to hold the \n",
    "    # new centroid points\n",
    "    new_centroids = []\n",
    "    \n",
    "    # print the iteration we are on\n",
    "    print \"\\n*~*~*~*~*~*~*~*\\n\"\n",
    "    print \"Iteration:\", iteration\n",
    "\n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run() \n",
    "\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output(): \n",
    "\n",
    "            # set the centroid\n",
    "            index,coordinates = mr_job.parse_output_line(line)\n",
    "\n",
    "            # update the current centroid\n",
    "            new_centroids.append(coordinates)\n",
    "\n",
    "            # print out the centroid values\n",
    "            print \"Index:\", index\n",
    "            print \"Coordinates sample:\", coordinates[0:4]\n",
    "\n",
    "        # set the new centroids as a regular list\n",
    "        new_centroids = new_centroids[:]\n",
    "        centroids = new_centroids[:]\n",
    "        \n",
    "        # convert our array to a pandas data frame\n",
    "        # and write that data frame to an output file\n",
    "        # and update the centroids file\n",
    "        centroids_pd = pd.DataFrame(centroids)\n",
    "        centroids_pd.to_csv('HW4.5_C_Output',\\\n",
    "                         header=False,index=False)\n",
    "        centroids_pd.to_csv('centroids.txt',\\\n",
    "                         header=False,index=False)\n",
    "        \n",
    "        # check the stopping condition with our\n",
    "        # new and old centroids\n",
    "        stopping = stop_reached(old_centroids,\\\n",
    "                                new_centroids,thresh=0.001)\n",
    "        \n",
    "        # iterate the iteration count by 1\n",
    "        iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D. K=4 trained centroids\n",
    "Use the row-level normalized aggregates as 'trained' centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a function calculate the row-level normalized aggregates trained centroids by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the numpy library to help us \n",
    "# with randomization\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def trainedCentroids(k=4):\n",
    "    \"\"\"generates k centroid points from the\n",
    "    aggregate data that is trained by \n",
    "    normalizing the aggregate data for each\n",
    "    class\"\"\"\n",
    "    \n",
    "    # initalize a counter at zero\n",
    "    counter = 0\n",
    "    \n",
    "    # create an array to hold the coordinates for\n",
    "    # each centroid\n",
    "    centroids = []\n",
    "    \n",
    "    # loop through each line in the summary data\n",
    "    for line in \\\n",
    "    open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\")\\\n",
    "    .readlines():\n",
    "        \n",
    "        # if it's between the 3rd and 6th lines\n",
    "        if counter >= 2 and counter <= 5:        \n",
    "            \n",
    "            # split the line by commas\n",
    "            data = re.split(\",\",line)\n",
    "            \n",
    "            # calculate the class aggregate as\n",
    "            # the normalized count for each word\n",
    "            classAggregate = \\\n",
    "            [float(data[i+3])/float(data[2]) \\\n",
    "             for i in range(1000)]\n",
    "            \n",
    "            # add our class aggregate to the\n",
    "            # to the centroids list\n",
    "            centroids.append(classAggregate)\n",
    "        \n",
    "        # increment our line counter by 1\n",
    "        counter += 1\n",
    "            \n",
    "    # return the new centroids\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the 'trained' centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. We got 4 trained centroids\n"
     ]
    }
   ],
   "source": [
    "# import pandas to help us write to csvs\n",
    "import pandas as pd\n",
    "\n",
    "# generate 4 trained centroids from the \n",
    "# aggregate data \n",
    "centroids = trainedCentroids(4)\n",
    "\n",
    "# write the data to a centroids file\n",
    "centroids_pd = pd.DataFrame(centroids)\n",
    "centroids_pd.to_csv('centroids.txt',\\\n",
    "                    header=False,index=False)\n",
    "\n",
    "# read the first couple lines\n",
    "print \"Done. We got\", len(centroids), \"trained centroids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the MRJob class to find the best centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_kmeans.py\n",
    "# import MRJob and some other libraries\n",
    "# to help us get started\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# define a function that will find which centroid\n",
    "# is closest to a given point\n",
    "def ClosestCentroid(point,centroid_points):\n",
    "    \"\"\"takes a point, a list of coordinates, and \n",
    "    compares that point to each of a number of \n",
    "    centroids stored in a list of lists. returns\n",
    "    the index of the centroid closest to the data\n",
    "    point\"\"\"\n",
    "    \n",
    "    # convert our inputs into numpy arrays\n",
    "    point = np.array(point)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    \n",
    "    # calculate the difference between the point\n",
    "    # and each of the centroid points\n",
    "    difference = point - centroid_points\n",
    "    \n",
    "    # square the difference, this will help us\n",
    "    # calculate distance regardless of direction\n",
    "    diff_sq = difference * difference\n",
    "    \n",
    "    # get the index of the centroid that is \n",
    "    # closest to the data point\n",
    "    closest_index = \\\n",
    "    np.argmin(list(diff_sq.sum(axis=1)))\n",
    "    \n",
    "    # return the closest index\n",
    "    return int(closest_index)\n",
    "    \n",
    "\n",
    "# create the MRJob class\n",
    "class MRKmeans(MRJob):\n",
    "    \"\"\"class responsible for find the nearest centroid\n",
    "    to a number of data points\"\"\"\n",
    "    \n",
    "    # create an array to hold our centroid\n",
    "    # points and set a value for K, number\n",
    "    # of centroids\n",
    "    centroid_points = []\n",
    "    K=4\n",
    "    \n",
    "    # set the number of true classifications\n",
    "    # and set the number of dimensions in our \n",
    "    # data\n",
    "    TRUTHS = 4\n",
    "    DIMS = 1000\n",
    "    \n",
    "    # read in the class count file\n",
    "    classes_pd = pd.read_csv('class_counts.txt',\\\n",
    "                         header=None)\n",
    "\n",
    "    # set the class counts\n",
    "    class_counts = map(float,classes_pd.values)\n",
    "    \n",
    "    # create an empty array to hold the counts\n",
    "    # for each class\n",
    "    classes = [0] * TRUTHS\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                      mapper=self.mapper,\\\n",
    "                      combiner=self.combiner,\\\n",
    "                      reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # load the initial centroids from a \n",
    "    # data file passed in\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # read in the centroids data\n",
    "        centroids = pd.read_csv('Centroids.txt',\\\n",
    "                                header=None)\n",
    "        \n",
    "        # set the centroid points based on the \n",
    "        # inputted file\n",
    "        self.centroid_points = map(list,centroids.values)\n",
    "\n",
    "    \n",
    "    # takes a line of the twitter data and \n",
    "    # returns the index of the closest centroid\n",
    "    # and the coordinates of this point, \n",
    "    # along with this point's true class\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # get all the information for the point\n",
    "        point = map(float,line.split(','))\n",
    "        \n",
    "        # get the point's true classification\n",
    "        # and simplify the point to just it's\n",
    "        # coordinates\n",
    "        truth = int(point[1])\n",
    "        point = point[3:]\n",
    "        \n",
    "        # grab the closest centroid\n",
    "        closest = \\\n",
    "        ClosestCentroid(point,self.centroid_points)\n",
    "        \n",
    "        # create an array of zeros of the\n",
    "        # length of the true classifications\n",
    "        classify = [0] * self.TRUTHS\n",
    "        \n",
    "        # set the index of the truth to be 1\n",
    "        classify[truth] = 1\n",
    "        \n",
    "        # yield:\n",
    "        # key: the index of the closest cluster\n",
    "        # value: the coordinates of the point &\n",
    "        # the classification\n",
    "        yield closest, (point, classify)\n",
    "        \n",
    "        \n",
    "    # takes the output of the mapper and combines\n",
    "    # the coordinate positions and updates the \n",
    "    # count of points for this centroid\n",
    "    def combiner(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # convert the numpy arrays back to\n",
    "        # regular arrays for the combiner's\n",
    "        # output\n",
    "        coordinates = list(coordinates)\n",
    "        truths = list(truths)\n",
    "        \n",
    "        # yield the key as the centroid and the \n",
    "        # sum of the coordinates and the sum of\n",
    "        # the classifications\n",
    "        yield centroid, (coordinates,truths)\n",
    "        \n",
    "    # takes the outputs of the mappers and \n",
    "    # combiners and computes the aggregate\n",
    "    # sums for each centroid and uses these\n",
    "    # sums to calculate new centroids at\n",
    "    # the centers of the clusters\n",
    "    def reducer(self, centroid, point_classify):\n",
    "        \n",
    "        # get the centroid value\n",
    "        centroid = int(centroid)\n",
    "        \n",
    "        # set two blank arrays to hold the sums of \n",
    "        # the coordinates and the sums of the true\n",
    "        # classifications\n",
    "        coordinates = [0] * self.DIMS\n",
    "        truths = [0] * self.TRUTHS\n",
    "        \n",
    "        # convert our arrays to numpy arrays\n",
    "        coordinates = np.array(coordinates)\n",
    "        truths = np.array(truths)\n",
    "        \n",
    "        # loop through each point and its \n",
    "        # associated classification \n",
    "        for point,classify in point_classify:\n",
    "            \n",
    "            # set each element as a numpy array\n",
    "            point = np.array(point)\n",
    "            classify = np.array(classify)\n",
    "            \n",
    "            # sum the coordinates and \n",
    "            # classification values\n",
    "            coordinates = coordinates + point\n",
    "            truths = truths + classify\n",
    "        \n",
    "        # gather the complete count for the\n",
    "        # centroid\n",
    "        num_points = float(sum(truths))\n",
    "        \n",
    "        # calculate the new centroid and \n",
    "        # convert it back to a regular list\n",
    "        new_centroid = coordinates / num_points\n",
    "        new_centroid = list(new_centroid)\n",
    "        \n",
    "        # print out the class breakdown\n",
    "        print \"Cluster #\",centroid\n",
    "        for index,item in enumerate(truths):\n",
    "            proportion = float(item) /\\\n",
    "            self.class_counts[index]\n",
    "            \n",
    "            print \"\\tClass\",index,\"\\t\",proportion\n",
    "        \n",
    "        # yield the centroid index and the \n",
    "        # coordinates of the new centroid\n",
    "        yield centroid, new_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a copy of the stop function to tell us when we have achieved sufficient convergence\n",
    "We put a copy down here because we don't want to have to scroll each time to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the chain tool to combine lists\n",
    "from itertools import chain\n",
    "\n",
    "def stop_reached(centroids_old,\\\n",
    "                 centroids_new,thresh=0.5):\n",
    "    \"\"\"a function that compares two lists of \n",
    "    centroids to determine if coordinate has\n",
    "    moved a greater distance than the\n",
    "    threshold, by default set to 0.5\"\"\"\n",
    "    \n",
    "    # convert the lists of centroids into a \n",
    "    # single list because we don't care about \n",
    "    # the context of the coordinates\n",
    "    centroids_old = list(chain(*centroids_old))\n",
    "    centroids_new = list(chain(*centroids_new))\n",
    "    \n",
    "    # calculate the difference between each\n",
    "    # of the coordinates\n",
    "    difference = [abs(old-new) for old,new in\\\n",
    "                 zip(centroids_old,centroids_new)]\n",
    "    \n",
    "    # set the flag for stopping to true\n",
    "    # by default\n",
    "    stopping = True\n",
    "    \n",
    "    # loop through each difference\n",
    "    for diff in difference:\n",
    "        \n",
    "        # if the difference is greater\n",
    "        # than the threshold, then break\n",
    "        # out of the loop and set the\n",
    "        # indicator for stopping to \n",
    "        # false\n",
    "        if diff > thresh:\n",
    "            stopping = False\n",
    "            break\n",
    "    \n",
    "    # return whether or not we reached the\n",
    "    # threshold or we need to keep going\n",
    "    return stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a runner to run the MRJob class in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 0\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.996010638298\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.0555555555556\n",
      "\tClass 3 \t0.31067961165\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.637362637363\n",
      "\tClass 2 \t0.0555555555556\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.32967032967\n",
      "\tClass 2 \t0.888888888889\n",
      "\tClass 3 \t0.0291262135922\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00265957446809\n",
      "\tClass 1 \t0.0\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.660194174757\n",
      "Index: 0\n",
      "Coordinates sample: [0.013056587935590983, 0.047863000573448035, 0.0258128158139248, 0.027212560757059225]\n",
      "Index: 1\n",
      "Coordinates sample: [0.13757453449382295, 0.0025576324607467582, 0.0020380910562584967, 0.003116342125949376]\n",
      "Index: 2\n",
      "Coordinates sample: [0.1054368093820366, 0.005971592697524741, 0.029885185038034556, 0.01689746846828232]\n",
      "Index: 3\n",
      "Coordinates sample: [0.03613126112878477, 0.04447814314264557, 0.015738461115237426, 0.021702806714172827]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 1\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.996010638298\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.148148148148\n",
      "\tClass 3 \t0.349514563107\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.571428571429\n",
      "\tClass 2 \t0.037037037037\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.395604395604\n",
      "\tClass 2 \t0.814814814815\n",
      "\tClass 3 \t0.0388349514563\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00265957446809\n",
      "\tClass 1 \t0.0\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.611650485437\n",
      "Index: 0\n",
      "Coordinates sample: [0.01302441329089856, 0.04747984207751542, 0.02566662757573825, 0.027057683816590336]\n",
      "Index: 1\n",
      "Coordinates sample: [0.11975834018130002, 0.002291815024006652, 0.0002166901341305983, 0.0005012922384216841]\n",
      "Index: 2\n",
      "Coordinates sample: [0.12534419729754473, 0.006273960793754537, 0.02997955000006059, 0.018509928285996367]\n",
      "Index: 3\n",
      "Coordinates sample: [0.03621458212520745, 0.045789825848055375, 0.015395207440618307, 0.021120631795221504]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 2\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.996010638298\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.222222222222\n",
      "\tClass 3 \t0.368932038835\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.777777777778\n",
      "\tClass 3 \t0.0388349514563\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00265957446809\n",
      "\tClass 1 \t0.0\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.592233009709\n",
      "Index: 0\n",
      "Coordinates sample: [0.0130970327448467, 0.047262693074177284, 0.02553556020957466, 0.026916487721991778]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.13774887556667859, 0.006631163234410254, 0.03034929542949903, 0.018517260694573653]\n",
      "Index: 3\n",
      "Coordinates sample: [0.0348029612972061, 0.045245233202876184, 0.01509646577119984, 0.02130824573324679]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 3\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.996010638298\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.368932038835\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0388349514563\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00265957446809\n",
      "\tClass 1 \t0.0\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.592233009709\n",
      "Index: 0\n",
      "Coordinates sample: [0.013118940892406785, 0.04714572061536234, 0.02551358199220159, 0.026854671972867464]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.14057435770089027, 0.0067870510052356, 0.0306821973466673, 0.01891849738044094]\n",
      "Index: 3\n",
      "Coordinates sample: [0.0348029612972061, 0.045245233202876184, 0.01509646577119984, 0.02130824573324679]\n",
      "\n",
      "*~*~*~*~*~*~*~*\n",
      "\n",
      "Iteration: 4\n",
      "Cluster # 0\n",
      "\tClass 0 \t0.996010638298\n",
      "\tClass 1 \t0.032967032967\n",
      "\tClass 2 \t0.259259259259\n",
      "\tClass 3 \t0.368932038835\n",
      "Cluster # 1\n",
      "\tClass 0 \t0.0\n",
      "\tClass 1 \t0.56043956044\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.0\n",
      "Cluster # 2\n",
      "\tClass 0 \t0.00132978723404\n",
      "\tClass 1 \t0.406593406593\n",
      "\tClass 2 \t0.740740740741\n",
      "\tClass 3 \t0.0388349514563\n",
      "Cluster # 3\n",
      "\tClass 0 \t0.00265957446809\n",
      "\tClass 1 \t0.0\n",
      "\tClass 2 \t0.0\n",
      "\tClass 3 \t0.592233009709\n",
      "Index: 0\n",
      "Coordinates sample: [0.013118940892406785, 0.04714572061536234, 0.02551358199220159, 0.026854671972867464]\n",
      "Index: 1\n",
      "Coordinates sample: [0.10931912915849412, 0.0022586353652849747, 0.0002225412437971433, 0.0005152654373259792]\n",
      "Index: 2\n",
      "Coordinates sample: [0.14057435770089027, 0.0067870510052356, 0.0306821973466673, 0.01891849738044094]\n",
      "Index: 3\n",
      "Coordinates sample: [0.0348029612972061, 0.045245233202876184, 0.01509646577119984, 0.02130824573324679]\n"
     ]
    }
   ],
   "source": [
    "# import the MRJob that we created\n",
    "from mr_kmeans import MRKmeans \n",
    "\n",
    "# import pandas to help us save and load \n",
    "# the centroids\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRKmeans(args=['twitter_users_norm.txt','--file=centroids.txt']) \n",
    "\n",
    "# read in the centroids data to get the original\n",
    "# centroids and convert it to a list\n",
    "centroids = pd.read_csv('centroids.txt',\\\n",
    "                        header=None)\n",
    "centroids = map(list,centroids.values)\n",
    "\n",
    "# create a counter to count our iterations\n",
    "# and an initial stopping indicator\n",
    "iteration = 0\n",
    "stopping = False\n",
    "\n",
    "# set up a loop that runs until we tell\n",
    "# it to stop\n",
    "while stopping == False:\n",
    "    \n",
    "    # set the old centroids\n",
    "    old_centroids = centroids[:]\n",
    "    \n",
    "    # create a new array to hold the \n",
    "    # new centroid points\n",
    "    new_centroids = []\n",
    "    \n",
    "    # print the iteration we are on\n",
    "    print \"\\n*~*~*~*~*~*~*~*\\n\"\n",
    "    print \"Iteration:\", iteration\n",
    "\n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run() \n",
    "\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output(): \n",
    "\n",
    "            # set the centroid\n",
    "            index,coordinates = mr_job.parse_output_line(line)\n",
    "\n",
    "            # update the current centroid\n",
    "            new_centroids.append(coordinates)\n",
    "\n",
    "            # print out the centroid values\n",
    "            print \"Index:\", index\n",
    "            print \"Coordinates sample:\", coordinates[0:4]\n",
    "\n",
    "        # set the new centroids as a regular list\n",
    "        new_centroids = new_centroids[:]\n",
    "        centroids = new_centroids[:]\n",
    "        \n",
    "        # convert our array to a pandas data frame\n",
    "        # and write that data frame to an output file\n",
    "        # and update the centroids file\n",
    "        centroids_pd = pd.DataFrame(centroids)\n",
    "        centroids_pd.to_csv('HW4.5_D_Output',\\\n",
    "                         header=False,index=False)\n",
    "        centroids_pd.to_csv('centroids.txt',\\\n",
    "                         header=False,index=False)\n",
    "        \n",
    "        # check the stopping condition with our\n",
    "        # new and old centroids\n",
    "        stopping = stop_reached(old_centroids,\\\n",
    "                                new_centroids,thresh=0.001)\n",
    "        \n",
    "        # iterate the iteration count by 1\n",
    "        iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look through parts A-D, we notice a marked improvement as we move towards less random, and more trained, clusters. For example, part A which is completely took 9 iterations one time I ran it. On the other hand, part D only took 4 iterations. This is because in part D, we are moving towards the true centers for each class. As we move towards the centers, we have to make less iterations to find the true centers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
