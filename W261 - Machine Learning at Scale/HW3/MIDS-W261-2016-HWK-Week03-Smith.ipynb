{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \\#2\n",
    "**Student: Alex Smith** <br>\n",
    "Course: W261 - Machine Learning at Scale <br>\n",
    "Professor: Jimi Shanahan <br>\n",
    "Due Date: May 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Useful resources\n",
    "When completing this notebook, I found the following resources particularly useful:\n",
    "- [Week 3 Slides](https://www.dropbox.com/s/jwycz91sdi549ih/MIDS-LSML-2016-Lecture03-Map%20Reduce%20Algorithm%20Design-LiveSession3-2016-05-24.pdf?dl=0)\n",
    "- [Python Sorting Dictionaries](http://stackoverflow.com/questions/7742752/sorting-a-dictionary-by-value-then-by-key)\n",
    "- Async Lecture on Pairs & Stripes (3.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "The following libraries must be installed before running the below code. They can all be installed through [Pip](https://github.com/pypa/pip).\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "- [Regular Expression](https://docs.python.org/2/library/re.html)\n",
    "- [Pretty Table](https://pypi.python.org/pypi/PrettyTable)\n",
    "- [Random](https://docs.python.org/2/library/random.html)\n",
    "- [Datetime](https://docs.python.org/2/library/datetime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Environment: Cloudera\n",
    "This notebook is designed to run in a Cloudera virtual box. To set up a virtual box like this one, follow the instructions [here](https://docs.google.com/presentation/d/1qCQM-2U2C6e584uM9kqTGr675K3_a8M1mEZaiT4Wmi8/edit#slide=id.p). Before beginning, make sure that you have started (in the following order) from the Cloudera manager:\n",
    "1. Zookeeper\n",
    "1. Yarn\n",
    "1. HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our Hadoop file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/cloudera/w261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up some testing files\n",
    "After you have gotten your data files into data/, let's make some testing files. This is helpful for running quick tests without churning through the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -50 data/Consumer_Complaints.csv > data/Consumer_test.csv\n",
    "!head -50 data/ProductPurchaseData.txt > data/ProductPurchaseData_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW3.0.\n",
    "*How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle] <br>\n",
    "What is  a combiner function in the context of Hadoop? <br>\n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "What is the Hadoop shuffle?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our mappers finish with the data, we feed the data into our reducers. We want to make sure that all of the values for each key are given to a single reducer. For example, in a word count program, we don't want two reducers summing values for a single word. During this phase, we can merge the two sorted lists/arrays of the form [key,value] before feeding them into the reducers. This means that we have multiple lines for a certain [key,value] pair, we can merge them by key before passing them into the reducer. This is useful because it reduces the volume of data we are moving around. We always want to reduce the volume of this data because it is risky to move data around. <br>\n",
    "To help us reduce the amount of data we move around, we can employ a combiner. The combiner is what helps merge the [key,value] pairs by the key. It is done on the mapper node before transferring the data to the reducer. We can also use a combiner on the reducer note to help simplify the data before it enters the reducer program. We can have multiple combiners at multiple points that help consolidate our data. <br>\n",
    "For example, we might want to use a combiner in the classic word count problem. We might want to combine the mapper results from the multiple mappers on a single mapper node. If we have 2 mappers running on a single, they will both produce a list of words (the key) and a vale of 1. We can combine the outputs of the two mappers by summing the values between the outputs. For instance, if mapper 1 outputs \"pig 1\" and mapper 2 outputs \"pig 1\", we can send to the reducer \"pig 2\". This turned two lines of data into 1. <br>\n",
    "The Hadoop **shuffle** is all the steps between mapper output and reducer input. It is helpful to break it down into five simple steps:\n",
    "1. Partition, sort, combine: for a given mapper, let's sort the output and merge the [key,value] pairs by key\n",
    "1. Mergesort: for all the mappers on a given node, let's sort the outputs and merge the [key,value] pairs by key\n",
    "1. Send to reducer: send the data from each mapper node to the reducer\n",
    "1. Mergesort again: for all the outputs from each of the mapper nodes, do a another mergesort to merge the [key,value] pairs by key\n",
    "1. Stream to reducer: finally send the data to the reducer program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "*Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.<br>\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.<br>\n",
    "Use the Consumer Complaints [Dataset](https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0) provide here to complete this question.*<br>\n",
    "*The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:<br>\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?*<br>\n",
    "<br>\n",
    "*Here’s is the first few lines of the  of the Consumer Complaints  Dataset:<br>\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,<br>\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,<br>\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,<br>\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,*<br>\n",
    "<br>\n",
    "*User-defined Counters<br>\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "We write a mapper function that takes an input of consumer complaints and outputs the category (Debt collection, Mortgage, or Other) with the number 1. This is the [key,value] pair. The mapper also writes to Hadoop counters for each of these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.1\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to stderror\n",
    "import sys\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by commas since\n",
    "    # this file is a CSV\n",
    "    line = line.strip().split(\",\")\n",
    "\n",
    "    # set the category as the second\n",
    "    # item in the line\n",
    "    category = line[1]\n",
    "    \n",
    "    # check which of the 3 categoreies\n",
    "    # the line falls into and print that\n",
    "    # with a 1 to the standard output\n",
    "    # also print the counter to the\n",
    "    # standard error so that we can double\n",
    "    # check our results with the counter\n",
    "    if category == \"Debt collection\":\n",
    "        print \"Debt collection\\t1\"\n",
    "        sys.stderr.write(\"reporter:counter:Complaints,Debt collection,1\\n\")\n",
    "    elif category == \"Mortgage\":\n",
    "        print \"Mortgage\\t1\"\n",
    "        sys.stderr.write(\"reporter:counter:Complaints,Mortgage,1\\n\")\n",
    "    else:\n",
    "        print \"Other\\t1\"\n",
    "        sys.stderr.write(\"reporter:counter:Complaints,Other,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "We write a reducer function to sum across the categories, and output the number of records in each of the three categores: debt collection, mortage, and other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.1\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to the stderror\n",
    "import sys\n",
    "\n",
    "# create a dictionary to store the categories\n",
    "# and their respective counts\n",
    "categories = {}\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by tabs\n",
    "    line = line.strip().split(\"\\t\")\n",
    "\n",
    "    # set the category as the first\n",
    "    # item in the line\n",
    "    category = line[0]\n",
    "    \n",
    "    # set the count as the second \n",
    "    # item in the line\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # check to see if the category already\n",
    "    # exists in the dictionary\n",
    "    if category not in categories:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        categories[category] = 0\n",
    "    \n",
    "    # increment the count\n",
    "    categories[category] = categories[category] +\\\n",
    "    count\n",
    "\n",
    "# print the outputs\n",
    "for category in categories.keys():\n",
    "    print category,\"\\t\",categories[category]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the MapReduce job in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 17:23:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_Complaints.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_Complaints.csv1464567820379\n",
      "16/05/29 17:23:48 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-1' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-11464567828574\n",
      "rm: cannot remove `/home/cloudera/w261/Outputs/Out_3_1': No such file or directory\n",
      "16/05/29 17:23:50 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5786349409546119012.jar tmpDir=null\n",
      "16/05/29 17:23:51 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/29 17:23:52 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/29 17:23:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 17:23:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 17:23:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464566418241_0003\n",
      "16/05/29 17:23:54 INFO impl.YarnClientImpl: Submitted application application_1464566418241_0003\n",
      "16/05/29 17:23:54 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464566418241_0003/\n",
      "16/05/29 17:23:54 INFO mapreduce.Job: Running job: job_1464566418241_0003\n",
      "16/05/29 17:24:02 INFO mapreduce.Job: Job job_1464566418241_0003 running in uber mode : false\n",
      "16/05/29 17:24:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 17:24:20 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/05/29 17:24:21 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 17:24:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 17:24:35 INFO mapreduce.Job: Job job_1464566418241_0003 completed successfully\n",
      "16/05/29 17:24:35 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=187260\n",
      "\t\tFILE: Number of bytes written=736181\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50946999\n",
      "\t\tHDFS: Number of bytes written=54\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4070528\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1299712\n",
      "\t\tTotal time spent by all map tasks (ms)=31801\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10154\n",
      "\t\tTotal vcore-seconds taken by all map tasks=31801\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10154\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4070528\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1299712\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=3324280\n",
      "\t\tMap output materialized bytes=187302\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=187302\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=390\n",
      "\t\tCPU time spent (ms)=12030\n",
      "\t\tPhysical memory (bytes) snapshot=390012928\n",
      "\t\tVirtual memory (bytes) snapshot=2212536320\n",
      "\t\tTotal committed heap usage (bytes)=147324928\n",
      "\tComplaints\n",
      "\t\tDebt collection=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50946747\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=54\n",
      "16/05/29 17:24:35 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-1\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_Complaints.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-1\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_1\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-1\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-1/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_3_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the output to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debt collection \t44372\r\n",
      "Mortgage \t125752\r\n",
      "Other \t142789\r\n"
     ]
    }
   ],
   "source": [
    "!cat Outputs/Out_3_1 | sort -k1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the output from the MapReduce function with what the counters calculated. We see that we get the same result! <br>\n",
    "![HW3.1_Counters](https://dl.dropboxusercontent.com/s/rgx4t2wsd91yivj/HW3.1_Counters.png?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 3.2 (a) Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "*For this brief study the Input file will be one record (the next line only):<br>\n",
    "foo foo quux labs foo bar quux<br>*\n",
    "<br>\n",
    "*Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.<br>\n",
    "**Note: we split the instructions between different cells because each instruction is relatively self-contained.** *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the line to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" > data/Input_3_2_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper function\n",
    "This function takes an input, splits the line into words by white space, and outputs a key,value pair where the key is the word and the value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.2(a)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to stderr\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Mapper,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by spaces\n",
    "    line = line.strip().split()\n",
    "\n",
    "    # print out the key-value pair as a tab\n",
    "    # delimited list of word and 1\n",
    "    for word in line:\n",
    "        print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function merges the counts for each word and outputs a list of words with their associated counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.2(a)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to the stderror\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Reducer,1\\n\")\n",
    "\n",
    "# create a dictionary to store the word\n",
    "# counts\n",
    "words = {}\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by tabs\n",
    "    line = line.strip().split(\"\\t\")\n",
    "\n",
    "    # set the category as the first\n",
    "    # item in the line\n",
    "    word = line[0]\n",
    "    \n",
    "    # set the count as the second \n",
    "    # item in the line\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # check to see if the word already\n",
    "    # exists in the dictionary\n",
    "    if word not in words:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        words[word] = 0\n",
    "    \n",
    "    # increment the count\n",
    "    words[word] = words[word] + count\n",
    "\n",
    "# print the outputs\n",
    "for word in words.keys():\n",
    "    print word,\"\\t\",words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the MapReduce job in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 19:34:20 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Input_3_2_a' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Input_3_2_a1464575660216\n",
      "16/05/29 19:34:26 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-a' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-a1464575666879\n",
      "16/05/29 19:34:28 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4194598760713788390.jar tmpDir=null\n",
      "16/05/29 19:34:30 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/29 19:34:30 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/29 19:34:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 19:34:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/29 19:34:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/29 19:34:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/29 19:34:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464566418241_0007\n",
      "16/05/29 19:34:32 INFO impl.YarnClientImpl: Submitted application application_1464566418241_0007\n",
      "16/05/29 19:34:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464566418241_0007/\n",
      "16/05/29 19:34:32 INFO mapreduce.Job: Running job: job_1464566418241_0007\n",
      "16/05/29 19:34:42 INFO mapreduce.Job: Job job_1464566418241_0007 running in uber mode : false\n",
      "16/05/29 19:34:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 19:34:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 19:35:18 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/05/29 19:35:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 19:35:33 INFO mapreduce.Job: Job job_1464566418241_0007 completed successfully\n",
      "16/05/29 19:35:33 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=132\n",
      "\t\tFILE: Number of bytes written=602905\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145\n",
      "\t\tHDFS: Number of bytes written=34\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1684864\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7617536\n",
      "\t\tTotal time spent by all map tasks (ms)=13163\n",
      "\t\tTotal time spent by all reduce tasks (ms)=59512\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13163\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=59512\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1684864\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7617536\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=52\n",
      "\t\tMap output materialized bytes=116\n",
      "\t\tInput split bytes=114\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=116\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=639\n",
      "\t\tCPU time spent (ms)=7150\n",
      "\t\tPhysical memory (bytes) snapshot=582488064\n",
      "\t\tVirtual memory (bytes) snapshot=3701907456\n",
      "\t\tTotal committed heap usage (bytes)=232259584\n",
      "\tMyJob\n",
      "\t\tMapper=1\n",
      "\t\tReducer=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "16/05/29 19:35:33 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-a\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Input_3_2_a /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-a\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-a\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-a\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=1 -D mapred.reduce.tasks=4 \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-a\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-a/* /home/cloudera/w261/Outputs/Out_3_2-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo  \t3\r\n",
      "quux  \t2\r\n",
      "labs  \t1\r\n",
      "bar  \t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat Outputs/Out_3_2-a/* | sort -n -r -k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of counters\n",
    "We use a single mapper and 4 reducers. In this simple problem, we use a single mapper because our input file is small and does not need to be chunked. We didn't need to use 4 reducers, but we go ahead with 4 so that each reducer handles 1 word and only 1 word. We might want to segregate each reducer to a single word if we were analyzing a very large corpus. This would be an intuitive way to break up the output files.<br>\n",
    "![Counters_3_2-a](https://dl.dropboxusercontent.com/s/lwl0a2pczcojgb5/HW3.2a_Counters.png?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 3.2 (b)\n",
    "*Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "This function takes the string from the issue column and outputs a line for each word following by the integer 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.2(b)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to stderr\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Mapper,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by spaces\n",
    "    line = line.strip().split(\",\")\n",
    "    \n",
    "    # grab the words we're interested in\n",
    "    words = line[3].split()\n",
    "\n",
    "    # print out the key-value pair as a tab\n",
    "    # delimited list of word and 1\n",
    "    for word in words:\n",
    "        print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function takes the inputs of key,value pairs of words and counts and merges the counts by word to generate a list of total counts for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.2(b)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to the stderror\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Reducer,1\\n\")\n",
    "\n",
    "# create a dictionary to store the word\n",
    "# counts\n",
    "words = {}\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by tabs\n",
    "    line = line.strip().split(\"\\t\")\n",
    "\n",
    "    # set the category as the first\n",
    "    # item in the line\n",
    "    word = line[0]\n",
    "    \n",
    "    # set the count as the second \n",
    "    # item in the line\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # check to see if the word already\n",
    "    # exists in the dictionary\n",
    "    if word not in words:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        words[word] = 0\n",
    "    \n",
    "    # increment the count\n",
    "    words[word] = words[word] + count\n",
    "\n",
    "# print the outputs\n",
    "for word in words.keys():\n",
    "    print word,\"\\t\",words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Make the files executable and run the test data in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/30 12:22:26 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_test.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_test.csv\n",
      "16/05/30 12:22:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-b' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-b\n",
      "16/05/30 12:22:42 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3598338797964190338.jar tmpDir=null\n",
      "16/05/30 12:22:45 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 12:22:45 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 12:22:47 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/30 12:22:48 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/30 12:22:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/30 12:22:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/30 12:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0002\n",
      "16/05/30 12:22:49 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0002\n",
      "16/05/30 12:22:50 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0002/\n",
      "16/05/30 12:22:50 INFO mapreduce.Job: Running job: job_1464634906532_0002\n",
      "16/05/30 12:23:17 INFO mapreduce.Job: Job job_1464634906532_0002 running in uber mode : false\n",
      "16/05/30 12:23:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/30 12:24:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/30 12:24:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/30 12:24:36 INFO mapreduce.Job: Job job_1464634906532_0002 completed successfully\n",
      "16/05/30 12:24:36 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1768\n",
      "\t\tFILE: Number of bytes written=486360\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=61947\n",
      "\t\tHDFS: Number of bytes written=1195\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13632256\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4535424\n",
      "\t\tTotal time spent by all map tasks (ms)=106502\n",
      "\t\tTotal time spent by all reduce tasks (ms)=35433\n",
      "\t\tTotal vcore-seconds taken by all map tasks=106502\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=35433\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13632256\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4535424\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=249\n",
      "\t\tMap output records=1083\n",
      "\t\tMap output bytes=11212\n",
      "\t\tMap output materialized bytes=2410\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=102\n",
      "\t\tReduce shuffle bytes=2410\n",
      "\t\tReduce input records=1083\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=2166\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=2022\n",
      "\t\tCPU time spent (ms)=5850\n",
      "\t\tPhysical memory (bytes) snapshot=491601920\n",
      "\t\tVirtual memory (bytes) snapshot=2949853184\n",
      "\t\tTotal committed heap usage (bytes)=190316544\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=61707\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1195\n",
      "16/05/30 12:24:36 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-b\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_test.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-b\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-b\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-b\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=2 \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-b\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-b/* /home/cloudera/w261/Outputs/Out_3_2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the full data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 09:56:24 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_Complaints.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_Complaints.csv\n",
      "16/06/04 09:56:35 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-b' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-b1465059395441\n",
      "16/06/04 09:56:37 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1815764423982164672.jar tmpDir=null\n",
      "16/06/04 09:56:39 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 09:56:40 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 09:56:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 09:56:42 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 09:56:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/06/04 09:56:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/04 09:56:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0019\n",
      "16/06/04 09:56:44 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0019\n",
      "16/06/04 09:56:44 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0019/\n",
      "16/06/04 09:56:44 INFO mapreduce.Job: Running job: job_1464634906532_0019\n",
      "16/06/04 09:57:04 INFO mapreduce.Job: Job job_1464634906532_0019 running in uber mode : false\n",
      "16/06/04 09:57:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 09:57:58 INFO mapreduce.Job:  map 64% reduce 0%\n",
      "16/06/04 09:58:02 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/04 09:58:04 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/06/04 09:58:05 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "16/06/04 09:58:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 09:58:31 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/06/04 09:58:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 09:58:41 INFO mapreduce.Job: Job job_1464634906532_0019 completed successfully\n",
      "16/06/04 09:58:41 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=916715\n",
      "\t\tFILE: Number of bytes written=1976599\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50946999\n",
      "\t\tHDFS: Number of bytes written=2659\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13936128\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7184640\n",
      "\t\tTotal time spent by all map tasks (ms)=108876\n",
      "\t\tTotal time spent by all reduce tasks (ms)=56130\n",
      "\t\tTotal vcore-seconds taken by all map tasks=108876\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=56130\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13936128\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7184640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=10407527\n",
      "\t\tMap output materialized bytes=590545\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=590545\n",
      "\t\tReduce input records=978634\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=2483605\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=2924\n",
      "\t\tCPU time spent (ms)=20260\n",
      "\t\tPhysical memory (bytes) snapshot=497647616\n",
      "\t\tVirtual memory (bytes) snapshot=2973028352\n",
      "\t\tTotal committed heap usage (bytes)=188743680\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50946747\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2659\n",
      "16/06/04 09:58:41 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-b\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_Complaints.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-b\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-b\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-b\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=2 \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-b\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-b/* /home/cloudera/w261/Outputs/Out_3_2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the output file\n",
    "Let's print a sample from one of the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Account  \t16205\r\n",
      "the  \t6248\r\n",
      "increase/decrease  \t1149\r\n",
      "APR  \t3431\r\n",
      "Account  \t350\r\n",
      "promised  \t274\r\n",
      "Overlimit  \t127\r\n",
      "process  \t5505\r\n",
      "issue  \t1098\r\n",
      "Debt  \t1343\r\n"
     ]
    }
   ],
   "source": [
    "!head Outputs/Out_3_2-b/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper and reducer counters\n",
    "We used a total of 2 reducers and 2 mappers. We can use 2 reducers easily because we are not looking at connections between words, only the count of each word. Because Hadoop sorts by keys, each reducer will get all the records for a single word. <br>\n",
    "![3.2b_counters](https://dl.dropboxusercontent.com/u/37624818/W261_Week3/HW3.2b_Counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.2 (c)\n",
    "*Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "This function takes the words in the issue column and outputs each word with an integer 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.2(c)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to stderr\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Mapper,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by spaces\n",
    "    line = line.strip().split(\",\")\n",
    "    \n",
    "    # grab the words we're interested in\n",
    "    words = line[3].split()\n",
    "\n",
    "    # print out the key-value pair as a tab\n",
    "    # delimited list of word and 1\n",
    "    for word in words:\n",
    "        print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function takes the words outputted from the mapper and merges based on the word to get a sum of counts for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.2(c)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to the stderror\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Reducer,1\\n\")\n",
    "\n",
    "# create a dictionary to store the word\n",
    "# counts\n",
    "words = {}\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by tabs\n",
    "    line = line.strip().split(\"\\t\")\n",
    "\n",
    "    # set the category as the first\n",
    "    # item in the line\n",
    "    word = line[0]\n",
    "    \n",
    "    # set the count as the second \n",
    "    # item in the line\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # check to see if the word already\n",
    "    # exists in the dictionary\n",
    "    if word not in words:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        words[word] = 0\n",
    "    \n",
    "    # increment the count\n",
    "    words[word] = words[word] + count\n",
    "\n",
    "# print the outputs\n",
    "for word in words.keys():\n",
    "    print word,\"\\t\",words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make executable and run the test data in hadoop\n",
    "We add our reducer as a stand-alone combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/30 12:54:01 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_test.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_test.csv1464638041156\n",
      "16/05/30 12:54:09 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-c' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-c\n",
      "16/05/30 12:54:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/30 12:54:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob247530583643913124.jar tmpDir=null\n",
      "16/05/30 12:54:14 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 12:54:14 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 12:54:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/30 12:54:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/30 12:54:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0004\n",
      "16/05/30 12:54:17 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0004\n",
      "16/05/30 12:54:17 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0004/\n",
      "16/05/30 12:54:17 INFO mapreduce.Job: Running job: job_1464634906532_0004\n",
      "16/05/30 12:54:26 INFO mapreduce.Job: Job job_1464634906532_0004 running in uber mode : false\n",
      "16/05/30 12:54:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/30 12:54:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/30 12:55:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/30 12:55:16 INFO mapreduce.Job: Job job_1464634906532_0004 completed successfully\n",
      "16/05/30 12:55:16 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1476\n",
      "\t\tFILE: Number of bytes written=486790\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=61947\n",
      "\t\tHDFS: Number of bytes written=1297\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5956480\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4658816\n",
      "\t\tTotal time spent by all map tasks (ms)=46535\n",
      "\t\tTotal time spent by all reduce tasks (ms)=36397\n",
      "\t\tTotal vcore-seconds taken by all map tasks=46535\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=36397\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5956480\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4658816\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=249\n",
      "\t\tMap output records=1083\n",
      "\t\tMap output bytes=11212\n",
      "\t\tMap output materialized bytes=1940\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=1083\n",
      "\t\tCombine output records=159\n",
      "\t\tReduce input groups=156\n",
      "\t\tReduce shuffle bytes=1940\n",
      "\t\tReduce input records=159\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=318\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=667\n",
      "\t\tCPU time spent (ms)=4750\n",
      "\t\tPhysical memory (bytes) snapshot=488947712\n",
      "\t\tVirtual memory (bytes) snapshot=2959560704\n",
      "\t\tTotal committed heap usage (bytes)=191889408\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=6\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=61707\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1297\n",
      "16/05/30 12:55:16 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-c\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_test.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-c\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-c\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-c\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=2 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-c\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-c/* /home/cloudera/w261/Outputs/Out_3_2-c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the full data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 10:10:41 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_Complaints.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_Complaints.csv1465060241812\n",
      "16/06/04 10:10:57 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-c' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-c\n",
      "16/06/04 10:11:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/04 10:11:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob438746980786419372.jar tmpDir=null\n",
      "16/06/04 10:11:02 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:11:02 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:11:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 10:11:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 10:11:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0020\n",
      "16/06/04 10:11:05 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0020\n",
      "16/06/04 10:11:05 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0020/\n",
      "16/06/04 10:11:05 INFO mapreduce.Job: Running job: job_1464634906532_0020\n",
      "16/06/04 10:11:16 INFO mapreduce.Job: Job job_1464634906532_0020 running in uber mode : false\n",
      "16/06/04 10:11:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 10:11:54 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/04 10:11:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 10:12:18 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/04 10:12:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 10:12:21 INFO mapreduce.Job: Job job_1464634906532_0020 completed successfully\n",
      "16/06/04 10:12:21 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12550\n",
      "\t\tFILE: Number of bytes written=499602\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50946999\n",
      "\t\tHDFS: Number of bytes written=2841\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10056576\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4282624\n",
      "\t\tTotal time spent by all map tasks (ms)=78567\n",
      "\t\tTotal time spent by all reduce tasks (ms)=33458\n",
      "\t\tTotal vcore-seconds taken by all map tasks=78567\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=33458\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10056576\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4282624\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=10407527\n",
      "\t\tMap output materialized bytes=6208\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=978634\n",
      "\t\tCombine output records=508\n",
      "\t\tReduce input groups=506\n",
      "\t\tReduce shuffle bytes=6208\n",
      "\t\tReduce input records=508\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=1369\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1229\n",
      "\t\tCPU time spent (ms)=9250\n",
      "\t\tPhysical memory (bytes) snapshot=485023744\n",
      "\t\tVirtual memory (bytes) snapshot=2953555968\n",
      "\t\tTotal committed heap usage (bytes)=192937984\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=8\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50946747\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2841\n",
      "16/06/04 10:12:21 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-c\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_Complaints.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-c\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-c\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-c\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=2 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-c\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-c/* /home/cloudera/w261/Outputs/Out_3_2-c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper and reducer counters\n",
    "We use a total of 8 counters, 6 reducer counters and 2 mapper counters. On the mapper side, we use 2 mapper counters and 4 reducer counters. On the reducer side, we use 2 reducer counters. We only have 2 mappers and 2 reducers but the counters show more because we are using the reducers as combiners. We are running the reducers as combiners twice for each mapper. In the mapper, this could be happening once in the circular buffer and once after all the output has been spilled to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW3.2 (d)\n",
    "*Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Mapper function\n",
    "This function thakes the words from the complaints data and outputs each word with an integer 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.2(d)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to stderr\n",
    "import sys\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Mapper,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by spaces\n",
    "    line = line.strip().split(\",\")\n",
    "    \n",
    "    # grab the words we're interested in\n",
    "    words = line[3].split()\n",
    "\n",
    "    # print out the key-value pair as a tab\n",
    "    # delimited list of word and 1\n",
    "    for word in words:\n",
    "        print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function merges the counts for each word, by word to produce a sum of word counts. It returns a sorted list by the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.2(d)\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to the stderror\n",
    "# import the operator library to help sort\n",
    "# the dictionary\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "# add a counter line for each time the \n",
    "# mapper is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Reducer,1\\n\")\n",
    "\n",
    "# create a dictionary to store the word\n",
    "# counts\n",
    "words = {}\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by tabs\n",
    "    line = line.strip().split(\"\\t\")\n",
    "\n",
    "    # set the word as the first\n",
    "    # item in the line\n",
    "    word = line[0].strip()\n",
    "    \n",
    "    # set the count as the second \n",
    "    # item in the line\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # check to see if the word already\n",
    "    # exists in the dictionary\n",
    "    if word not in words:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        words[word] = 0\n",
    "    \n",
    "    # increment the count\n",
    "    words[word] = words[word] + count\n",
    "    \n",
    "# print the outputs\n",
    "for word in sorted(words.items(),\\\n",
    "                   key=operator.itemgetter(1)):\n",
    "    print word[0],\"\\t\",word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/30 13:28:01 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_test.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_test.csv1464640081215\n",
      "rm: `/user/cloudera/w261-output-3-2-d': No such file or directory\n",
      "rm: cannot remove `/home/cloudera/w261/Outputs/Out_3_2-d': No such file or directory\n",
      "16/05/30 13:28:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/30 13:28:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6504188710895385985.jar tmpDir=null\n",
      "16/05/30 13:28:20 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 13:28:21 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 13:28:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/30 13:28:24 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/30 13:28:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0005\n",
      "16/05/30 13:28:25 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0005\n",
      "16/05/30 13:28:25 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0005/\n",
      "16/05/30 13:28:25 INFO mapreduce.Job: Running job: job_1464634906532_0005\n",
      "16/05/30 13:28:47 INFO mapreduce.Job: Job job_1464634906532_0005 running in uber mode : false\n",
      "16/05/30 13:28:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/30 13:29:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/30 13:29:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/30 13:29:56 INFO mapreduce.Job: Job job_1464634906532_0005 completed successfully\n",
      "16/05/30 13:29:56 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1357\n",
      "\t\tFILE: Number of bytes written=365632\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=61947\n",
      "\t\tHDFS: Number of bytes written=1093\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13684992\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1063296\n",
      "\t\tTotal time spent by all map tasks (ms)=106914\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8307\n",
      "\t\tTotal vcore-seconds taken by all map tasks=106914\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8307\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13684992\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1063296\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=249\n",
      "\t\tMap output records=1083\n",
      "\t\tMap output bytes=11212\n",
      "\t\tMap output materialized bytes=1750\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=1083\n",
      "\t\tCombine output records=159\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=1750\n",
      "\t\tReduce input records=159\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=318\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=537\n",
      "\t\tCPU time spent (ms)=3430\n",
      "\t\tPhysical memory (bytes) snapshot=389562368\n",
      "\t\tVirtual memory (bytes) snapshot=2207375360\n",
      "\t\tTotal committed heap usage (bytes)=143130624\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=3\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=61707\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1093\n",
      "16/05/30 13:29:56 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-d\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_test.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-d\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-d\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-d\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-d\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-d/part-00000 /home/cloudera/w261/Outputs/Out_3_2-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the full data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 10:22:53 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_Complaints.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_Complaints.csv1465060973395\n",
      "16/06/04 10:23:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-d' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-d\n",
      "16/06/04 10:23:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/04 10:23:05 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7367082926943165624.jar tmpDir=null\n",
      "16/06/04 10:23:06 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:23:06 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:23:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 10:23:08 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 10:23:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0021\n",
      "16/06/04 10:23:09 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0021\n",
      "16/06/04 10:23:09 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0021/\n",
      "16/06/04 10:23:09 INFO mapreduce.Job: Running job: job_1464634906532_0021\n",
      "16/06/04 10:23:19 INFO mapreduce.Job: Job job_1464634906532_0021 running in uber mode : false\n",
      "16/06/04 10:23:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 10:23:56 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/04 10:23:59 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 10:24:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 10:24:10 INFO mapreduce.Job: Job job_1464634906532_0021 completed successfully\n",
      "16/06/04 10:24:10 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9242\n",
      "\t\tFILE: Number of bytes written=377430\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50946999\n",
      "\t\tHDFS: Number of bytes written=2477\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9445888\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1024768\n",
      "\t\tTotal time spent by all map tasks (ms)=73796\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8006\n",
      "\t\tTotal vcore-seconds taken by all map tasks=73796\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8006\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9445888\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1024768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=10407527\n",
      "\t\tMap output materialized bytes=5663\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=978634\n",
      "\t\tCombine output records=508\n",
      "\t\tReduce input groups=508\n",
      "\t\tReduce shuffle bytes=5663\n",
      "\t\tReduce input records=508\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=1369\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=899\n",
      "\t\tCPU time spent (ms)=7400\n",
      "\t\tPhysical memory (bytes) snapshot=376905728\n",
      "\t\tVirtual memory (bytes) snapshot=2190626816\n",
      "\t\tTotal committed heap usage (bytes)=145227776\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50946747\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2477\n",
      "16/06/04 10:24:10 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-d\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_Complaints.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-d\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-d\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-d\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-d\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-d/part-00000 /home/cloudera/w261/Outputs/Out_3_2-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 50 tokens\n",
    "These 50 words appeared most frequently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Loan \t107254\r\n",
      "modification \t70487\r\n",
      "servicing \t36767\r\n",
      "credit \t36126\r\n",
      "report \t30546\r\n",
      "Incorrect \t29069\r\n",
      "information \t29069\r\n",
      "on \t29069\r\n",
      "or \t22533\r\n",
      "debt \t17966\r\n",
      "and \t16448\r\n",
      "\"Account \t16205\r\n",
      "opening \t16205\r\n",
      "Credit \t14768\r\n",
      "club \t12545\r\n",
      "health \t12545\r\n",
      "/ \t12386\r\n",
      "not \t12353\r\n",
      "loan \t12237\r\n",
      "attempts \t11848\r\n",
      "collect \t11848\r\n",
      "Cont'd \t11848\r\n",
      "owed \t11848\r\n",
      "of \t10885\r\n",
      "my \t10731\r\n",
      "Deposits \t10555\r\n",
      "withdrawals \t10555\r\n",
      "Problems \t9484\r\n",
      "\"Application \t8625\r\n",
      "to \t8401\r\n",
      "Billing \t8158\r\n",
      "Other \t7886\r\n",
      "disputes \t6938\r\n",
      "Communication \t6920\r\n",
      "tactics \t6920\r\n",
      "reporting \t6559\r\n",
      "lease \t6337\r\n",
      "the \t6248\r\n",
      "being \t5663\r\n",
      "by \t5663\r\n",
      "caused \t5663\r\n",
      "funds \t5663\r\n",
      "low \t5663\r\n",
      "process \t5505\r\n",
      "Disclosure \t5214\r\n",
      "verification \t5214\r\n",
      "Managing \t5006\r\n",
      "company's \t4858\r\n",
      "investigation \t4858\r\n",
      "card \t4405\r\n"
     ]
    }
   ],
   "source": [
    "!tail -50 Outputs/Out_3_2-d/part-00000 | sort -k2nr,2 -k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bottom 10 tokens\n",
    "These 10 words appear least frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credited \t92\r\n",
      "Payment \t92\r\n",
      "checks \t75\r\n",
      "Convenience \t75\r\n",
      "amt \t71\r\n",
      "day \t71\r\n",
      "wrong \t71\r\n",
      "disclosures \t64\r\n",
      "Incorrect/missing \t64\r\n",
      "Issue \t1\r\n"
     ]
    }
   ],
   "source": [
    "!head Outputs/Out_3_2-d/part-00000 | sort -k2nr,2 -k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 3.2.1\n",
    "*Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "This function takes an input of words and ouputs each word with it's associated count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.2.1\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to stderr\n",
    "import sys\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by spaces\n",
    "    line = line.strip().split(\",\")\n",
    "    \n",
    "    # grab the words we're interested in\n",
    "    words = line[3].split()\n",
    "\n",
    "    # print out the key-value pair as a tab\n",
    "    # delimited list of word and 1\n",
    "    for word in words:\n",
    "        print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function sums across the counts for a single word and outputs a list of word with the final counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.2.1\n",
    "\n",
    "# import the system library to read from\n",
    "# the input and also write to the stderror\n",
    "# import the operator library to help sort\n",
    "# the dictionary\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "# create a dictionary to store the word\n",
    "# counts\n",
    "words = {}\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # stirp off any extra spaces and \n",
    "    # split the line by tabs\n",
    "    line = line.strip().split(\"\\t\")\n",
    "\n",
    "    # set the word as the first\n",
    "    # item in the line\n",
    "    word = line[0].strip()\n",
    "    \n",
    "    # set the count as the second \n",
    "    # item in the line\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # check to see if the word already\n",
    "    # exists in the dictionary\n",
    "    if word not in words:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        words[word] = 0\n",
    "    \n",
    "    # increment the count\n",
    "    words[word] = words[word] + count\n",
    "    \n",
    "# print the outputs\n",
    "for word in sorted(words.items(),\\\n",
    "                   key=operator.itemgetter(1)):\n",
    "    print word[0],\"\\t\",word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the code on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disclosure \t1\r\n",
      "verification \t1\r\n",
      "of \t1\r\n",
      "debt \t1\r\n",
      "Disclosure \t1\r\n",
      "verification \t1\r\n",
      "of \t1\r\n",
      "debt \t1\r\n",
      "Deposits \t1\r\n",
      "and \t1\r\n"
     ]
    }
   ],
   "source": [
    "# make the files executable\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# first try with just the reducer\n",
    "!cat data/Consumer_test.csv | ~/w261/mapper.py > Outputs/testing.txt\n",
    "\n",
    "# read the first couple lines of the testing file\n",
    "!head Outputs/testing.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health \t26\r\n",
      "Cont'd \t26\r\n",
      "club \t26\r\n",
      "owed \t26\r\n",
      "debt \t34\r\n",
      "on \t74\r\n",
      "Incorrect \t74\r\n",
      "information \t74\r\n",
      "report \t77\r\n",
      "credit \t97\r\n"
     ]
    }
   ],
   "source": [
    "# now let's test the output of the mapper into \n",
    "# the reducer\n",
    "!cat data/Consumer_test.csv | ~/w261/mapper.py | ~/w261/reducer.py > Outputs/testing.txt\n",
    "\n",
    "# read the first couple lines of the testing file\n",
    "!tail Outputs/testing.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the hadoop with 2 reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 08:32:41 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchaseData_test.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchaseData_test.txt1465054361377\n",
      "rm: `/user/cloudera/w261-output-3-2-1': No such file or directory\n",
      "rm: cannot remove `/home/cloudera/w261/Outputs/Out_3_2-1': No such file or directory\n",
      "16/06/04 08:32:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/04 08:32:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7077778030319964414.jar tmpDir=null\n",
      "16/06/04 08:32:59 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 08:32:59 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 08:33:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 08:33:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 08:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0017\n",
      "16/06/04 08:33:05 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0017\n",
      "16/06/04 08:33:05 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0017/\n",
      "16/06/04 08:33:06 INFO mapreduce.Job: Running job: job_1464634906532_0017\n",
      "16/06/04 08:33:24 INFO mapreduce.Job: Job job_1464634906532_0017 running in uber mode : false\n",
      "16/06/04 08:33:24 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 08:34:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 08:34:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 08:34:57 INFO mapreduce.Job: Job job_1464634906532_0017 completed successfully\n",
      "16/06/04 08:34:57 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1448\n",
      "\t\tFILE: Number of bytes written=486673\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=61947\n",
      "\t\tHDFS: Number of bytes written=1093\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15365760\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6151808\n",
      "\t\tTotal time spent by all map tasks (ms)=120045\n",
      "\t\tTotal time spent by all reduce tasks (ms)=48061\n",
      "\t\tTotal vcore-seconds taken by all map tasks=120045\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=48061\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15365760\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6151808\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=249\n",
      "\t\tMap output records=1083\n",
      "\t\tMap output bytes=11212\n",
      "\t\tMap output materialized bytes=1847\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=1083\n",
      "\t\tCombine output records=159\n",
      "\t\tReduce input groups=159\n",
      "\t\tReduce shuffle bytes=1847\n",
      "\t\tReduce input records=159\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=318\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1311\n",
      "\t\tCPU time spent (ms)=5560\n",
      "\t\tPhysical memory (bytes) snapshot=501768192\n",
      "\t\tVirtual memory (bytes) snapshot=2943021056\n",
      "\t\tTotal committed heap usage (bytes)=190840832\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=61707\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1093\n",
      "16/06/04 08:34:57 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-1\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_test.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-1\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-1\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-1\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=2 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-1\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-1/* /home/cloudera/w261/Outputs/Out_3_2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the full data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 10:30:51 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Consumer_Complaints.csv' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Consumer_Complaints.csv1465061451380\n",
      "16/06/04 10:31:00 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-2-1' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-2-1\n",
      "16/06/04 10:31:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/04 10:31:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4751754695751656259.jar tmpDir=null\n",
      "16/06/04 10:31:05 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:31:06 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:31:07 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 10:31:07 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 10:31:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0022\n",
      "16/06/04 10:31:08 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0022\n",
      "16/06/04 10:31:08 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0022/\n",
      "16/06/04 10:31:08 INFO mapreduce.Job: Running job: job_1464634906532_0022\n",
      "16/06/04 10:31:18 INFO mapreduce.Job: Job job_1464634906532_0022 running in uber mode : false\n",
      "16/06/04 10:31:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 10:31:47 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/06/04 10:31:48 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "16/06/04 10:31:49 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/06/04 10:31:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 10:32:05 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 10:32:06 INFO mapreduce.Job: Job job_1464634906532_0022 completed successfully\n",
      "16/06/04 10:32:07 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12127\n",
      "\t\tFILE: Number of bytes written=499029\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50946999\n",
      "\t\tHDFS: Number of bytes written=2477\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7686400\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3129856\n",
      "\t\tTotal time spent by all map tasks (ms)=60050\n",
      "\t\tTotal time spent by all reduce tasks (ms)=24452\n",
      "\t\tTotal vcore-seconds taken by all map tasks=60050\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=24452\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7686400\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3129856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=10407527\n",
      "\t\tMap output materialized bytes=5984\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=978634\n",
      "\t\tCombine output records=508\n",
      "\t\tReduce input groups=508\n",
      "\t\tReduce shuffle bytes=5984\n",
      "\t\tReduce input records=508\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=1369\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1239\n",
      "\t\tCPU time spent (ms)=9540\n",
      "\t\tPhysical memory (bytes) snapshot=473772032\n",
      "\t\tVirtual memory (bytes) snapshot=2924826624\n",
      "\t\tTotal committed heap usage (bytes)=187170816\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50946747\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2477\n",
      "16/06/04 10:32:07 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-2-1\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/Consumer_Complaints.csv /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-2-1\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_2-1\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_2-1\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=2 -D mapred.reduce.tasks=2 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-2-1\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-2-1/* /home/cloudera/w261/Outputs/Out_3_2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pull out the top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Loan\t107254\n",
      "modification\t70487\n",
      "servicing\t36767\n",
      "credit\t36126\n",
      "report\t30546\n",
      "Incorrect\t29069\n",
      "information\t29069\n",
      "on\t29069\n",
      "or\t22533\n",
      "debt\t17966\n",
      "and\t16448\n",
      "\"Account\t16205\n",
      "opening\t16205\n",
      "Credit\t14768\n",
      "club\t12545\n",
      "health\t12545\n",
      "/\t12386\n",
      "not\t12353\n",
      "loan\t12237\n",
      "collect\t11848\n",
      "attempts\t11848\n",
      "owed\t11848\n",
      "Cont'd\t11848\n",
      "of\t10885\n",
      "my\t10731\n",
      "Deposits\t10555\n",
      "withdrawals\t10555\n",
      "Problems\t9484\n",
      "\"Application\t8625\n",
      "to\t8401\n",
      "Billing\t8158\n",
      "Other\t7886\n",
      "disputes\t6938\n",
      "Communication\t6920\n",
      "tactics\t6920\n",
      "reporting\t6559\n",
      "lease\t6337\n",
      "the\t6248\n",
      "by\t5663\n",
      "caused\t5663\n",
      "being\t5663\n",
      "funds\t5663\n",
      "low\t5663\n",
      "process\t5505\n",
      "verification\t5214\n",
      "Disclosure\t5214\n",
      "Managing\t5006\n",
      "company's\t4858\n",
      "investigation\t4858\n",
      "card\t4405\n"
     ]
    }
   ],
   "source": [
    "# pull the two files that we outputted from\n",
    "# hadoop\n",
    "file1 = \"Outputs/Out_3_2-1/part-00000\"\n",
    "file2 = \"Outputs/Out_3_2-1/part-00001\"\n",
    "\n",
    "# set the lists for holding the words and \n",
    "# the counts\n",
    "words1 = []\n",
    "counts1 = []\n",
    "words2 = []\n",
    "counts2 = []\n",
    "\n",
    "# get the words from file 1\n",
    "with open(file1, \"r\") as myfile: \n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # split the line\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the word and the count\n",
    "        word = line[0].strip()\n",
    "        count = int(line[1].strip())\n",
    "        \n",
    "        # append to the lists\n",
    "        words1.append(word)\n",
    "        counts1.append(count)\n",
    "\n",
    "# get the words from file 2\n",
    "with open(file2, \"r\") as myfile: \n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # split the line\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the word and the count\n",
    "        word = line[0].strip()\n",
    "        count = int(line[1].strip())\n",
    "        \n",
    "        # append to the lists\n",
    "        words2.append(word)\n",
    "        counts2.append(count)\n",
    "\n",
    "# create a final list to store the counts \n",
    "# and words\n",
    "words = []\n",
    "counts = []\n",
    "LENGTH = 50\n",
    "        \n",
    "# while we have less than 50 items in \n",
    "# our list\n",
    "while len(words) < LENGTH:\n",
    "    \n",
    "    # compare the counts and words at \n",
    "    # the end of each list\n",
    "    word1 = words1[-1]\n",
    "    count1 = counts1[-1]\n",
    "    word2 = words2[-1]\n",
    "    count2 = counts2[-1]\n",
    "    \n",
    "    # if word1 is larger, append it to the\n",
    "    # final list\n",
    "    if count1 > count2:\n",
    "        words.append(words1.pop())\n",
    "        counts.append(counts1.pop())\n",
    "    \n",
    "    # else if word is actually bigger, append\n",
    "    # it to the final list\n",
    "    elif count2 > count1:\n",
    "        words.append(words2.pop())\n",
    "        counts.append(counts2.pop())\n",
    "        \n",
    "    # else if the two words have equal counts\n",
    "    else:\n",
    "        \n",
    "        # append both words to a new list and\n",
    "        # sort that list\n",
    "        sort = []\n",
    "        sort.append(word1)\n",
    "        sort.append(word2)\n",
    "        sort.sort()\n",
    "        \n",
    "        # check which word is alphabetically\n",
    "        # first and add that word to our\n",
    "        # final list\n",
    "        if word1 == sort[0]:\n",
    "            words.append(words1.pop())\n",
    "            counts.append(counts1.pop())\n",
    "        else:\n",
    "            words.append(words2.pop())\n",
    "            counts.append(counts2.pop())\n",
    "\n",
    "# print out the list of the top 50 words\n",
    "for index,word in enumerate(words):\n",
    "    info = word + \"\\t\" + str(counts[index])\n",
    "    print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 3.3 Shopping Cart Analysis\n",
    "*Product Recommendations: The action or practice of selling additional products or services to existing customers is called cross-selling. Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers. For this homework use the online browsing behavior dataset located at:<br>\n",
    "https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0 *<br>\n",
    "<br>\n",
    "*Each line in this dataset represents a browsing session of a customer. On each line, each string of 8 characters represents the id of an item browsed during that session. The items are separated by spaces.<br>\n",
    "Here are the first few lines of the ProductPurchaseData:<br>\n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 <br>\n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 <br>\n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 <br>\n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 <br> \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 *<br>\n",
    "<br>\n",
    "*Do some exploratory data analysis of this dataset guided by the following questions:<br>\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess the data\n",
    "We want to preprocess the file to add a unique identifier for each basket. This allows the products in each basket to be sorted but still retain their association with the original basket. This is a step similar to the one described when dealing with Microsoft.com's log files in the Async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Addkey(filename):\n",
    "    \"\"\"Adds a unique key to every row in a \n",
    "    data file\"\"\"\n",
    "    \n",
    "    # open the original file\n",
    "    with open(filename,\"r\") as myfile:\n",
    "\n",
    "        # create a new filename\n",
    "        newfilename = filename + \"_mod\"\n",
    "        \n",
    "        # create a new file for the found\n",
    "        with open(newfilename,\"w\") as mynewfile:\n",
    "\n",
    "            # initalize a record counter to assign each\n",
    "            # basket it's own unique identifier\n",
    "            count = 0\n",
    "\n",
    "            # loop through all the lines\n",
    "            for line in myfile.readlines():\n",
    "\n",
    "                # create what we will be writing\n",
    "                info = str(count) + \"..\\t..\" + line\n",
    "\n",
    "                # write to the new file\n",
    "                mynewfile.write(info)\n",
    "\n",
    "                # increment the count\n",
    "                count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys successfully added\n"
     ]
    }
   ],
   "source": [
    "# add a key to both my test and full data\n",
    "Addkey(\"data/ProductPurchaseData_test.txt\")\n",
    "Addkey(\"data/ProductPurchaseData.txt\")\n",
    "print \"Keys successfully added\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "The mapper function takes the browsing session from a website and outputs each product visited and the number of products visited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.3\n",
    "\n",
    "# import the system library to read from\n",
    "# the input\n",
    "import sys\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split by the divider\n",
    "    line = line.split(\"..\\t..\")\n",
    "    \n",
    "    # set the basket number and the items\n",
    "    # in each basket\n",
    "    basket_id = int(line[0])\n",
    "    products = line[1].strip().split()\n",
    "    \n",
    "    # count the number of products in \n",
    "    # the basket\n",
    "    number = len(products)\n",
    "\n",
    "    # print out the key-value pair as a tab\n",
    "    # delimited list of word and \n",
    "    for product in products:\n",
    "        info = str(basket_id) + \"\\t\" + \\\n",
    "        product.strip() + \",\" + str(number)\n",
    "        print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "The reducer function takes the output from the mapper and outputs:\n",
    "- a list of products sorted by frequency of order, along with relative frequency\n",
    "- number of unique products\n",
    "- contents of the largest basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.3\n",
    "\n",
    "# import the system library to read from\n",
    "# the input\n",
    "# import the operator library to help sort\n",
    "# the dictionary\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "# create a dictionary to store the products\n",
    "# and their respective counts\n",
    "products = {}\n",
    "\n",
    "# create an array to hold the products in the\n",
    "# largest basket, also add a marker to help\n",
    "# us keep track of the items we add to the basket\n",
    "basket = []\n",
    "max_basket_id = None\n",
    "\n",
    "# keep a running count of all products\n",
    "all_prods = 0\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by the tab and set the \n",
    "    # basket id\n",
    "    line = line.split(\"\\t\")\n",
    "    _basket_id = line[0]\n",
    "\n",
    "    # set the product the next item\n",
    "    # and the number of items as the one\n",
    "    # after that\n",
    "    _product = line[1].split(\",\")[0]\n",
    "    _basket_total = int(line[1].split(\",\")[1])\n",
    "    \n",
    "    # check to see if the product already\n",
    "    # exists in the dictionary\n",
    "    if _product not in products:\n",
    "        \n",
    "        # if it's not, add it, and initalize\n",
    "        # it with a count of 0\n",
    "        products[_product] = 0\n",
    "    \n",
    "    # increment the counts\n",
    "    products[_product] = products[_product] + 1\n",
    "    all_prods = all_prods + 1\n",
    "    \n",
    "    # compare the length of the current basket \n",
    "    # with the length of the basket currently stored\n",
    "    if _basket_total > len(basket):\n",
    "\n",
    "        # set how many items we want to add to\n",
    "        # this basket\n",
    "        max_basket_id = _basket_id\n",
    "        \n",
    "    # if we need to add items to the basket \n",
    "    if _basket_id == max_basket_id:\n",
    "        \n",
    "        # add the item to the basket\n",
    "        basket.append(_product)\n",
    "    \n",
    "# print the sorted list of products\n",
    "for product in sorted(products.items(),\\\n",
    "                      key=lambda k: (-k[1], k[0])):\n",
    "    freq = float(product[1])/float(all_prods)\n",
    "    print product[0],\"\\t\",product[1],\"\\t\",freq\n",
    "\n",
    "# print a dividing line\n",
    "print \"*~*~*~*~*\"\n",
    "\n",
    "# print the lenght of the largest basket\n",
    "print \"The number of items in the largest basket \\\n",
    "is\", len(basket)\n",
    "\n",
    "# print each item in the basket\n",
    "info = \"\"\n",
    "for _item in basket:\n",
    "    info = info + str(_item) + \" \"\n",
    "print info\n",
    "\n",
    "# print a dividing line\n",
    "print \"*~*~*~*~*\"\n",
    "\n",
    "# print the number of unique products\n",
    "print \"Unique products:\", len(products.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the test data hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/30 15:43:48 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchase_mod.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchase_mod.txt\n",
      "16/05/30 15:43:56 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-3' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-3\n",
      "16/05/30 15:43:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6949182719773429115.jar tmpDir=null\n",
      "16/05/30 15:44:00 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 15:44:01 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/05/30 15:44:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/30 15:44:03 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/30 15:44:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0007\n",
      "16/05/30 15:44:04 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0007\n",
      "16/05/30 15:44:04 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0007/\n",
      "16/05/30 15:44:04 INFO mapreduce.Job: Running job: job_1464634906532_0007\n",
      "16/05/30 15:44:27 INFO mapreduce.Job: Job job_1464634906532_0007 running in uber mode : false\n",
      "16/05/30 15:44:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/30 15:45:21 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/30 15:45:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/30 15:45:31 INFO mapreduce.Job: Job job_1464634906532_0007 completed successfully\n",
      "16/05/30 15:45:31 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9505\n",
      "\t\tFILE: Number of bytes written=380822\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=18582\n",
      "\t\tHDFS: Number of bytes written=13913\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11746944\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=956160\n",
      "\t\tTotal time spent by all map tasks (ms)=91773\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7470\n",
      "\t\tTotal vcore-seconds taken by all map tasks=91773\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7470\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11746944\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=956160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=105\n",
      "\t\tMap output records=1265\n",
      "\t\tMap output bytes=18709\n",
      "\t\tMap output materialized bytes=9749\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=105\n",
      "\t\tReduce shuffle bytes=9749\n",
      "\t\tReduce input records=1265\n",
      "\t\tReduce output records=447\n",
      "\t\tSpilled Records=2530\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=3090\n",
      "\t\tCPU time spent (ms)=3960\n",
      "\t\tPhysical memory (bytes) snapshot=361803776\n",
      "\t\tVirtual memory (bytes) snapshot=2188435456\n",
      "\t\tTotal committed heap usage (bytes)=142606336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=18330\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13913\n",
      "16/05/30 15:45:31 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-3\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/ProductPurchaseData_test.txt_mod /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-3\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_3\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_3\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-3\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-3/part-00000 /home/cloudera/w261/Outputs/Out_3_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the full data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 10:45:35 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchaseData.txt_mod' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchaseData.txt_mod\n",
      "16/06/04 10:45:44 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-3' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-31465062344300\n",
      "16/06/04 10:45:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8907208976098100491.jar tmpDir=null\n",
      "16/06/04 10:45:48 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:45:49 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "16/06/04 10:45:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 10:45:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 10:45:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464634906532_0024\n",
      "16/06/04 10:45:51 INFO impl.YarnClientImpl: Submitted application application_1464634906532_0024\n",
      "16/06/04 10:45:51 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464634906532_0024/\n",
      "16/06/04 10:45:51 INFO mapreduce.Job: Running job: job_1464634906532_0024\n",
      "16/06/04 10:46:03 INFO mapreduce.Job: Job job_1464634906532_0024 running in uber mode : false\n",
      "16/06/04 10:46:03 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 10:46:34 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/04 10:46:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 10:46:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 10:46:51 INFO mapreduce.Job: Job job_1464634906532_0024 completed successfully\n",
      "16/06/04 10:46:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3065209\n",
      "\t\tFILE: Number of bytes written=6467348\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3780013\n",
      "\t\tHDFS: Number of bytes written=394555\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7957248\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1406592\n",
      "\t\tTotal time spent by all map tasks (ms)=62166\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10989\n",
      "\t\tTotal vcore-seconds taken by all map tasks=62166\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10989\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7957248\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1406592\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=6631793\n",
      "\t\tMap output materialized bytes=3040571\n",
      "\t\tInput split bytes=260\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=31101\n",
      "\t\tReduce shuffle bytes=3040571\n",
      "\t\tReduce input records=380824\n",
      "\t\tReduce output records=12597\n",
      "\t\tSpilled Records=761648\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=735\n",
      "\t\tCPU time spent (ms)=8560\n",
      "\t\tPhysical memory (bytes) snapshot=376811520\n",
      "\t\tVirtual memory (bytes) snapshot=2195718144\n",
      "\t\tTotal committed heap usage (bytes)=143654912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3779753\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=394555\n",
      "16/06/04 10:46:51 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-3\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/ProductPurchaseData.txt_mod /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-3\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_3\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_3\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-3\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-3/part-00000 /home/cloudera/w261/Outputs/Out_3_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle the output data\n",
    "The output data has the answer to multiple questions. We split the file into multiple files, one for each part of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the divider information\n",
    "divider = \"*~*~*~*~*\"\n",
    "divider_count = 0\n",
    "\n",
    "# initalize variables to hold the information\n",
    "# for each future file\n",
    "products = []\n",
    "largest = []\n",
    "uniques = []\n",
    "\n",
    "# open the output file\n",
    "with open(\"Outputs/Out_3_3/part-00000\",\"r\") \\\n",
    "as myfile:\n",
    "    \n",
    "    # loop through every line\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # check to see if we've reached a divider\n",
    "        if line.strip() == divider:\n",
    "            divider_count = divider_count + 1\n",
    "        else:\n",
    "        \n",
    "            # add the line to the appropriate file\n",
    "            # based on the divider count\n",
    "            if divider_count == 0:\n",
    "                products.append(line)\n",
    "            elif divider_count == 1:\n",
    "                largest.append(line)\n",
    "            else:\n",
    "                uniques.append(line)\n",
    "\n",
    "# write to each of the new files\n",
    "with open(\"Outputs/Out_3_3/products\",\"w\") as myfile:\n",
    "    for product in products:\n",
    "        myfile.write(product)\n",
    "with open(\"Outputs/Out_3_3/largest\",\"w\") as myfile:\n",
    "    for line in largest:\n",
    "        myfile.write(line)\n",
    "with open(\"Outputs/Out_3_3/unqiues\",\"w\") as myfile:\n",
    "    for line in uniques:\n",
    "        myfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique products: 12592\t\r\n"
     ]
    }
   ],
   "source": [
    "!cat Outputs/Out_3_3/unqiues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Largest basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of items in the largest basket is 71\t\r\n",
      "ELE89019 FRO11987 ELE17451 GRO99222 SNA90258 FRO12685 GRO12298 GRO99222 SNA80192 FRO84225 ELE17451 SNA30755 FRO90334 ELE52966 ELE26917 SNA11465 ELE91550 DAI92253 DAI93692 SNA55952 GRO12935 GRO48282 DAI87514 SNA94781 SNA17715 ELE82555 GRO36567 SNA47306 GRO99222 DAI22896 GRO73461 ELE17451 DAI22177 ELE26917 FRO82427 ELE24180 GRO32086 ELE36890 ELE56095 SNA55762 SNA80324 SNA72462 ELE87243 DAI67621 ELE20847 SNA61380 FRO15030 DAI75645 GRO99863 DAI38969 DAI62779 ELE56788 ELE81346 GRO94758 ELE49801 GRO68067 SNA47306 ELE59028 GRO69543 DAI53152 FRO84460 GRO81087 GRO61133 DAI85309 DAI84511 DAI54320 FRO37721 GRO46627 SNA96364 ELE35632 DAI67347 \t\r\n"
     ]
    }
   ],
   "source": [
    "!cat Outputs/Out_3_3/largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 50 products with their frequencies and relative frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\t\tCount\tRelative Frequency\n",
      "DAI62779 \t6667 \t0.0175067747831\r\n",
      "FRO40251 \t3881 \t0.010191059387\r\n",
      "ELE17451 \t3875 \t0.0101753040775\r\n",
      "GRO73461 \t3602 \t0.00945843749344\r\n",
      "SNA80324 \t3044 \t0.00799319370628\r\n",
      "ELE32164 \t2851 \t0.0074863979161\r\n",
      "DAI75645 \t2736 \t0.00718442114993\r\n",
      "SNA45677 \t2455 \t0.0064465474865\r\n",
      "FRO31317 \t2330 \t0.0061183118711\r\n",
      "DAI85309 \t2293 \t0.00602115412894\r\n",
      "ELE26917 \t2292 \t0.00601852824402\r\n",
      "FRO80039 \t2233 \t0.00586360103355\r\n",
      "GRO21487 \t2115 \t0.00555374661261\r\n",
      "SNA99873 \t2083 \t0.00546971829507\r\n",
      "GRO59710 \t2004 \t0.00526227338613\r\n",
      "GRO71621 \t1920 \t0.00504169905258\r\n",
      "FRO85978 \t1918 \t0.00503644728273\r\n",
      "GRO30386 \t1840 \t0.00483162825872\r\n",
      "ELE74009 \t1816 \t0.00476860702057\r\n",
      "GRO56726 \t1784 \t0.00468457870302\r\n",
      "DAI63921 \t1773 \t0.00465569396887\r\n",
      "GRO46854 \t1756 \t0.00461105392517\r\n",
      "ELE66600 \t1713 \t0.00449814087347\r\n",
      "DAI83733 \t1712 \t0.00449551498855\r\n",
      "FRO32293 \t1702 \t0.00446925613932\r\n",
      "ELE66810 \t1697 \t0.0044561267147\r\n",
      "SNA55762 \t1646 \t0.00432220658362\r\n",
      "DAI22177 \t1627 \t0.00427231477008\r\n",
      "FRO78087 \t1531 \t0.00402022981745\r\n",
      "ELE99737 \t1516 \t0.0039808415436\r\n",
      "ELE34057 \t1489 \t0.00390994265067\r\n",
      "GRO94758 \t1489 \t0.00390994265067\r\n",
      "FRO35904 \t1436 \t0.00377077074974\r\n",
      "FRO53271 \t1420 \t0.00372875659097\r\n",
      "SNA93860 \t1407 \t0.00369462008697\r\n",
      "SNA90094 \t1390 \t0.00364998004327\r\n",
      "GRO38814 \t1352 \t0.00355019641619\r\n",
      "ELE56788 \t1345 \t0.00353181522173\r\n",
      "GRO61133 \t1321 \t0.00346879398357\r\n",
      "DAI88807 \t1316 \t0.00345566455896\r\n",
      "ELE74482 \t1316 \t0.00345566455896\r\n",
      "ELE59935 \t1311 \t0.00344253513434\r\n",
      "SNA96271 \t1295 \t0.00340052097557\r\n",
      "DAI43223 \t1290 \t0.00338739155095\r\n",
      "ELE91337 \t1289 \t0.00338476566603\r\n",
      "GRO15017 \t1275 \t0.0033480032771\r\n",
      "DAI31081 \t1261 \t0.00331124088818\r\n",
      "GRO81087 \t1220 \t0.00320357960633\r\n",
      "DAI22896 \t1219 \t0.0032009537214\r\n",
      "GRO85051 \t1214 \t0.00318782429679\r\n"
     ]
    }
   ],
   "source": [
    "print \"Product\\t\\tCount\\tRelative Frequency\"\n",
    "!head -50 Outputs/Out_3_3/products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "*Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a map-reduce program to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 (i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.<br>\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. <br>\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.<br>\n",
    "<br>\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2):<br>\n",
    "item1, item2, support count, support<br>\n",
    "<br>\n",
    "Fix the ordering of the pairs lexicographically (left to right), and break ties in support (between pairs, if any exist) by taking the first ones in lexicographically increasing order.<br>\n",
    "<br>\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers). <br>\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function \n",
    "This mapper function takes the items in a basket and outputs pairs for each co-occurence for every term. For example, the line a,b,c would output: ab 1, ac 1, ba 1, bc 1, ca 1, cb 1. This is the pairs method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.4\n",
    "\n",
    "# import the system library to read from\n",
    "# the input\n",
    "import sys\n",
    "\n",
    "# initalize counter to keep track of the number\n",
    "# of baskets\n",
    "baskets = 0\n",
    "\n",
    "# write to standard error to keep track of how often\n",
    "# we call this function\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Mapper,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # increment the counter\n",
    "    baskets = baskets + 1\n",
    "    \n",
    "    # split by spaces\n",
    "    products = line.split()\n",
    "    \n",
    "    # loop through each product\n",
    "    for product in products:\n",
    "        \n",
    "        # loop through every other product,\n",
    "        # ignoring the the product itself \n",
    "        # because we wouldn't want \n",
    "        # to make a product recomendation for the\n",
    "        # same product we're already visiting\n",
    "        for pair in products:\n",
    "            \n",
    "            # if this is another product\n",
    "            if product.strip() != pair.strip(): \n",
    "                \n",
    "                # create the string to print\n",
    "                info = product.strip() + \",\" + \\\n",
    "                pair.strip() + \"\\t1\"\n",
    "                print info\n",
    "\n",
    "# print the counter as the last line\n",
    "info = \"Records:\" + \"\\t\" + str(baskets)\n",
    "print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function takes the output from the mappers and outputs in a sorted list for each co-occurring pair:\n",
    "- item1\n",
    "- item2\n",
    "- support count (number of times this pair occurs)\n",
    "- support (portion of baskets this pair occurs in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/cloudera/anaconda2/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.4\n",
    "\n",
    "# import the system library to read from\n",
    "# the input\n",
    "# import the numpy library to help us sort\n",
    "# before output\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# create a dictionary to store the product pairs\n",
    "# and their respective counts\n",
    "products = {}\n",
    "\n",
    "# keep a running count of how many records we are\n",
    "# dealing with\n",
    "records = 0\n",
    "\n",
    "# write a line to the standard error to keep\n",
    "# track of how often this function is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Reducer,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by the tab\n",
    "    line = line.split(\"\\t\")\n",
    "    \n",
    "    # set the pair and count values\n",
    "    pair = line[0].strip()\n",
    "    count = int(line[1])\n",
    "\n",
    "    # if its the count variable let's update\n",
    "    # our counts\n",
    "    if pair == \"Records:\":\n",
    "        records = records + count\n",
    "    \n",
    "    else:\n",
    "        # check to see if the pair already\n",
    "        # exists in the dictionary\n",
    "        if pair not in products:\n",
    "\n",
    "            # if it's not, add it, and initalize\n",
    "            # it with a count of 0\n",
    "            products[pair] = 0\n",
    "\n",
    "        # increment the counts\n",
    "        products[pair] = products[pair] + 1\n",
    "\n",
    "# create a set of array to store those pairs that\n",
    "# frequently occur together\n",
    "FREQUENT = 100\n",
    "item1s = []\n",
    "item2s = []\n",
    "counts = []\n",
    "frequs = []\n",
    "\n",
    "# loop through all the pairs in the dictionary\n",
    "for pair in products.keys():\n",
    "    \n",
    "    # if the pair occurs more than 100 times\n",
    "    if int(products[pair]) > FREQUENT:\n",
    "        \n",
    "        # split the items and store them\n",
    "        item1 = pair.split(',')[0]\n",
    "        item2 = pair.split(',')[1]\n",
    "        \n",
    "        # get the count and relative \n",
    "        # frequency\n",
    "        count = products[pair]\n",
    "        frequ = float(count)/float(records)\n",
    "        \n",
    "        # append all the values to our arrays\n",
    "        item1s.append(item1)\n",
    "        item2s.append(item2)\n",
    "        counts.append(count)\n",
    "        frequs.append(frequ)\n",
    "        \n",
    "# convert our arrays to numpy arrays\n",
    "item1s = np.array(item1s)\n",
    "item2s = np.array(item2s)\n",
    "counts = np.array(counts)\n",
    "frequs = np.array(frequs)\n",
    "\n",
    "# get the indices for the sorted list\n",
    "indexs = np.argsort(frequs)[::-1]\n",
    "\n",
    "# print the elements to the output\n",
    "print \"Item1\\t\\tItem2\\t\\tCount\\tSupport\"\n",
    "for index in indexs:\n",
    "    \n",
    "    # gather all the information\n",
    "    item1 = item1s[index]\n",
    "    item2 = item2s[index]\n",
    "    count = counts[index]\n",
    "    frequ = frequs[index]\n",
    "    \n",
    "    # create the line to print out\n",
    "    line = str(item1) + \"\\t\" + str(item2) + \\\n",
    "    \"\\t\" + str(count) + \"\\t\" + str(frequ)\n",
    "    \n",
    "    # print the line out\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the test data in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 12:46:54 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchaseData_test.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchaseData_test.txt1465069614349\n",
      "16/06/04 12:47:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-4' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-41465069622054\n",
      "16/06/04 12:47:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2656552094061797701.jar tmpDir=null\n",
      "16/06/04 12:47:05 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 12:47:05 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 12:47:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 12:47:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 12:47:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465069396819_0001\n",
      "16/06/04 12:47:07 INFO impl.YarnClientImpl: Submitted application application_1465069396819_0001\n",
      "16/06/04 12:47:08 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465069396819_0001/\n",
      "16/06/04 12:47:08 INFO mapreduce.Job: Running job: job_1465069396819_0001\n",
      "16/06/04 12:47:17 INFO mapreduce.Job: Job job_1465069396819_0001 running in uber mode : false\n",
      "16/06/04 12:47:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 12:47:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 12:47:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 12:47:37 INFO mapreduce.Job: Job job_1465069396819_0001 completed successfully\n",
      "16/06/04 12:47:37 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=36092\n",
      "\t\tFILE: Number of bytes written=434757\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8046\n",
      "\t\tHDFS: Number of bytes written=28\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2490496\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=654848\n",
      "\t\tTotal time spent by all map tasks (ms)=19457\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5116\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19457\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5116\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=2490496\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=654848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=7026\n",
      "\t\tMap output bytes=140504\n",
      "\t\tMap output materialized bytes=37094\n",
      "\t\tInput split bytes=262\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5207\n",
      "\t\tReduce shuffle bytes=37094\n",
      "\t\tReduce input records=7026\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=14052\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=180\n",
      "\t\tCPU time spent (ms)=3690\n",
      "\t\tPhysical memory (bytes) snapshot=428154880\n",
      "\t\tVirtual memory (bytes) snapshot=2210664448\n",
      "\t\tTotal committed heap usage (bytes)=142082048\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7784\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28\n",
      "16/06/04 12:47:37 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-4\n"
     ]
    }
   ],
   "source": [
    "# write the time to a file\n",
    "!echo $(date) > Outputs/Time_3_4\n",
    "\n",
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/ProductPurchaseData_test.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-4\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_4\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_4\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-4\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-4/part-00000 /home/cloudera/w261/Outputs/Out_3_4\n",
    "\n",
    "# write the ending time to a file\n",
    "!echo $(date) >> Outputs/Time_3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the whole data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 12:54:34 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchaseData_test.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchaseData_test.txt1465070074023\n",
      "16/06/04 12:54:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-4' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-41465070080269\n",
      "16/06/04 12:54:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob9078575781275816040.jar tmpDir=null\n",
      "16/06/04 12:54:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 12:54:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 12:54:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 12:54:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 12:54:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465069396819_0002\n",
      "16/06/04 12:54:45 INFO impl.YarnClientImpl: Submitted application application_1465069396819_0002\n",
      "16/06/04 12:54:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465069396819_0002/\n",
      "16/06/04 12:54:45 INFO mapreduce.Job: Running job: job_1465069396819_0002\n",
      "16/06/04 12:54:53 INFO mapreduce.Job: Job job_1465069396819_0002 running in uber mode : false\n",
      "16/06/04 12:54:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 12:55:07 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "16/06/04 12:55:10 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "16/06/04 12:55:11 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "16/06/04 12:55:12 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/06/04 12:55:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 12:55:25 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/06/04 12:55:28 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/06/04 12:55:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 12:55:32 INFO mapreduce.Job: Job job_1465069396819_0002 completed successfully\n",
      "16/06/04 12:55:32 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=43244633\n",
      "\t\tFILE: Number of bytes written=62302003\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3498983\n",
      "\t\tHDFS: Number of bytes written=101764\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4428288\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2150144\n",
      "\t\tTotal time spent by all map tasks (ms)=34596\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16798\n",
      "\t\tTotal vcore-seconds taken by all map tasks=34596\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=16798\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4428288\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2150144\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=5068110\n",
      "\t\tMap output bytes=101362190\n",
      "\t\tMap output materialized bytes=18550851\n",
      "\t\tInput split bytes=252\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1754191\n",
      "\t\tReduce shuffle bytes=18550851\n",
      "\t\tReduce input records=5068110\n",
      "\t\tReduce output records=2623\n",
      "\t\tSpilled Records=15204330\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=200\n",
      "\t\tCPU time spent (ms)=31270\n",
      "\t\tPhysical memory (bytes) snapshot=431661056\n",
      "\t\tVirtual memory (bytes) snapshot=2203160576\n",
      "\t\tTotal committed heap usage (bytes)=144179200\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3498731\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=101764\n",
      "16/06/04 12:55:32 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-4\n"
     ]
    }
   ],
   "source": [
    "# write the time to a file\n",
    "!echo $(date) > Outputs/Time_3_4\n",
    "\n",
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/ProductPurchaseData.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-4\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_4\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_4\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-4\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-4/part-00000 /home/cloudera/w261/Outputs/Out_3_4\n",
    "\n",
    "# write the ending time to a file\n",
    "!echo $(date) >> Outputs/Time_3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item1\t\tItem2\t\tCount\tSupport\r\n",
      "ELE17451\tDAI62779\t1592\t0.0511880646925\r\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\r\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\r\n",
      "SNA80324\tFRO40251\t1412\t0.0454004694383\r\n",
      "FRO40251\tDAI75645\t1254\t0.0403202469374\r\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\r\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\r\n",
      "GRO85051\tFRO40251\t1213\t0.0390019613517\r\n",
      "GRO73461\tDAI62779\t1139\t0.0366226166361\r\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\r\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\r\n",
      "SNA80324\tDAI75645\t1130\t0.0363332368734\r\n",
      "FRO40251\tDAI62779\t1070\t0.0344040384554\r\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\r\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\r\n",
      "SNA80324\tDAI62779\t923\t0.0296775023311\r\n",
      "DAI85309\tDAI62779\t918\t0.0295167357963\r\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\r\n",
      "GRO59710\tELE32164\t911\t0.0292916626475\r\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\r\n",
      "GRO73461\tFRO40251\t882\t0.0283592167454\r\n",
      "DAI75645\tDAI62779\t882\t0.0283592167454\r\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\r\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\r\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\r\n",
      "ELE92920\tDAI62779\t877\t0.0281984502106\r\n",
      "FRO92469\tFRO40251\t835\t0.026848011318\r\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\r\n",
      "ELE32164\tDAI62779\t832\t0.0267515513971\r\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\r\n",
      "GRO73461\tDAI75645\t712\t0.0228931545609\r\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\r\n",
      "ELE32164\tDAI43223\t711\t0.022861001254\r\n",
      "DAI43223\tELE32164\t711\t0.022861001254\r\n",
      "GRO30386\tDAI62779\t709\t0.02279669464\r\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\r\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\r\n",
      "FRO40251\tELE17451\t697\t0.0224108549564\r\n",
      "ELE99737\tDAI85309\t659\t0.0211890292917\r\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\r\n",
      "DAI62779\tELE26917\t650\t0.020899649529\r\n",
      "ELE26917\tDAI62779\t650\t0.020899649529\r\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\r\n",
      "GRO73461\tGRO21487\t631\t0.0202887366966\r\n",
      "SNA45677\tDAI62779\t604\t0.0194205974084\r\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\r\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\r\n",
      "SNA80324\tELE17451\t597\t0.0191955242597\r\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 Outputs/Out_3_4/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the time the program took to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def CalcTime(filepath):\n",
    "    \"\"\"A function that takes a simply file with\n",
    "    only two lines, a starting and an ending time\n",
    "    and returns the time elapsed between the two\n",
    "    times\"\"\"\n",
    "    \n",
    "    # open the file\n",
    "    with open(filepath,\"r\") as myfile:\n",
    "        \n",
    "        # set a starting and ending time\n",
    "        start = None\n",
    "        end = None\n",
    "        \n",
    "        # read in the lines and store them\n",
    "        start = myfile.readline().strip()\n",
    "        end = myfile.readline().strip()\n",
    "        \n",
    "        # set the time format\n",
    "        time_format = \"%a %b %d %H:%M:%S %Z %Y\"\n",
    "        \n",
    "        # convert it to a time\n",
    "        start = datetime.datetime.strptime(\\\n",
    "                                           start,\\\n",
    "                                           time_format)\n",
    "        end = datetime.datetime.strptime(\\\n",
    "                                           end,\\\n",
    "                                           time_format)\n",
    "\n",
    "        # calculate the elapsed time\n",
    "        elapsed = end - start\n",
    "        \n",
    "        # return the elapsed time\n",
    "        return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time it took to run the program \n",
      "is 0:01:05\n"
     ]
    }
   ],
   "source": [
    "# calculate the elapsed time\n",
    "elapsed_3_4 = CalcTime(\"Outputs/Time_3_4\")\n",
    "print \"The time it took to run the program \\\n",
    "\\nis\",elapsed_3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counters for mappers and reducers\n",
    "We used a total of 3 counters, 2 for mappers and 1 for reducers. <br>\n",
    "![3.4_counters](https://dl.dropboxusercontent.com/u/37624818/W261_Week3/HW3.4_Counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 3.5\n",
    "*Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.<br> \n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)<br>\n",
    "<br>\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "The mapper function takes as input all items in a basket, each basket separated by a new line character. It outputs a dictionary key that holds the counts for each product and the different products it is paired with. For example, consider reading in the following list: a,b,c ; a,b ; c,d, ; a,b,e. Our output for product a will be: a (b:3, c:1, e:1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW3.5\n",
    "\n",
    "# import the system library to read from\n",
    "# the input\n",
    "import sys\n",
    "\n",
    "# initalize counter to keep track of the number\n",
    "# of baskets\n",
    "baskets = 0\n",
    "\n",
    "# initalize a dictionary to store each product\n",
    "# and the counts for its paired products\n",
    "prods = {}\n",
    "\n",
    "# write to standard error to keep track of how often\n",
    "# we call this function\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Mapper,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # increment the counter\n",
    "    baskets = baskets + 1\n",
    "    \n",
    "    # split by spaces\n",
    "    products = line.split()\n",
    "    \n",
    "    # loop through each product\n",
    "    for product in products:\n",
    "        \n",
    "        # strip away any excess\n",
    "        product = product.strip()\n",
    "        \n",
    "        # if the product is not in already added\n",
    "        # to the dictionary, add it\n",
    "        if product not in prods.keys():\n",
    "            prods[product]={}\n",
    "        \n",
    "        # loop through every other product,\n",
    "        # ignoring the the product itself \n",
    "        # because we wouldn't want \n",
    "        # to make a product recomendation for the\n",
    "        # same product we're already visiting\n",
    "        for pair in products:\n",
    "            \n",
    "            # strip away any excess\n",
    "            pair = pair.strip()\n",
    "            \n",
    "            # if this is another product\n",
    "            if product != pair: \n",
    "                \n",
    "                # check to see if we already have\n",
    "                # this pair in this product's\n",
    "                # dictionary\n",
    "                if pair not in prods[product].keys():\n",
    "                    prods[product][pair] = 0\n",
    "                \n",
    "                # increment the count for the pairs\n",
    "                # we've seen with this specific product\n",
    "                prods[product][pair] = \\\n",
    "                prods[product][pair] + 1\n",
    "                    \n",
    "# print the dictionary for each product\n",
    "for product in prods.keys():\n",
    "    info = product + \"\\t\" + str(prods[product])\n",
    "    print info\n",
    "                    \n",
    "# print the counter as the last line\n",
    "info = \"Records:\" + \"\\t\" + str(baskets)\n",
    "print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:MyJob,Mapper,1\n",
      "DAI50921\t{'FRO78994': 1, 'GRO56989': 1, 'FRO32293': 1, 'FRO71213': 1, 'FRO18919': 1, 'SNA93730': 1, 'DAI36452': 1, 'ELE59935': 1, 'GRO75578': 1, 'SNA14713': 1, 'GRO73461': 1, 'SNA58915': 1, 'GRO12935': 1, 'DAI35347': 1, 'SNA81153': 1, 'SNA99873': 1, 'SNA45677': 1, 'SNA80192': 1, 'ELE17451': 2, 'SNA89670': 1, 'DAI22896': 1, 'FRO41069': 1, 'DAI62779': 1}\n",
      "FRO70974\t{'GRO30386': 1, 'DAI88808': 1, 'GRO36567': 1, 'SNA91554': 1, 'SNA80192': 1, 'FRO16142': 1, 'ELE17451': 1, 'ELE52446': 1, 'ELE96863': 1, 'DAI22896': 1, 'SNA47306': 1, 'SNA90258': 1, 'FRO18919': 1, 'FRO41069': 1, 'GRO99222': 1, 'GRO49037': 1, 'FRO75418': 1, 'GRO73461': 1}\n",
      "GRO94047\t{'DAI22177': 1, 'FRO60023': 1, 'DAI59119': 1, 'FRO31317': 1, 'ELE89019': 1, 'ELE20196': 1, 'ELE17451': 1}\n",
      "ELE13292\t{'ELE12792': 1, 'DAI33885': 1, 'ELE27376': 1, 'DAI44355': 1, 'SNA56035': 1, 'ELE22574': 1, 'GRO71615': 1, 'ELE20166': 1, 'GRO92942': 1}\n",
      "ELE89019\t{'GRO36567': 1, 'GRO56989': 1, 'GRO94047': 1, 'FRO31317': 1, 'ELE66067': 1, 'ELE14480': 1, 'FRO11987': 1, 'FRO36081': 1, 'DAI93692': 1, 'ELE20196': 1, 'FRO60023': 1, 'SNA83730': 1, 'DAI22177': 1, 'DAI59119': 1, 'SNA85034': 1, 'ELE73246': 1, 'ELE59935': 2, 'DAI22534': 1, 'SNA63157': 1, 'ELE37798': 1, 'FRO92261': 1, 'SNA90258': 1, 'FRO75418': 1, 'ELE96863': 1, 'DAI91290': 1, 'DAI35347': 1, 'GRO39070': 1, 'SNA55952': 1, 'DAI63921': 1, 'GRO82070': 1, 'SNA80192': 1, 'ELE17451': 5, 'FRO62970': 1, 'FRO79301': 1, 'DAI48891': 1, 'DAI62779': 1, 'GRO99222': 2, 'GRO49037': 1, 'FRO78087': 2}\n",
      "SNA11465\t{'GRO56989': 2, 'GRO39369': 1, 'ELE11375': 1, 'DAI54444': 1, 'ELE91550': 1, 'FRO84225': 1, 'SNA69641': 1, 'FRO12685': 1, 'FRO86643': 3, 'ELE23393': 1, 'FRO90334': 1, 'ELE52966': 1, 'ELE37798': 2, 'GRO39357': 2, 'ELE28573': 1, 'SNA30755': 1, 'FRO78087': 2, 'GRO12298': 1, 'GRO75578': 1, 'ELE26917': 1, 'SNA80192': 1, 'ELE17451': 4, 'GRO99222': 1, 'DAI95741': 1}\n",
      "GRO27756\t{'SNA99873': 1, 'ELE91337': 1, 'DAI69239': 1, 'ELE52966': 1, 'GRO99222': 1, 'SNA14713': 1, 'SNA93641': 1}\n",
      "FRO36081\t{'ELE96863': 1, 'FRO92261': 1, 'GRO56989': 1, 'FRO75418': 1, 'SNA80192': 1, 'ELE17451': 1, 'ELE89019': 1, 'ELE73246': 1, 'ELE66067': 1, 'DAI48891': 1, 'DAI35347': 1, 'GRO36567': 1, 'SNA55952': 1, 'DAI93692': 1, 'GRO49037': 1, 'SNA63157': 1, 'ELE37798': 1}\n",
      "ELE20196\t{'DAI22177': 1, 'DAI59119': 1, 'GRO94047': 1, 'FRO31317': 1, 'ELE89019': 1, 'FRO60023': 1, 'ELE17451': 1}\n",
      "DAI84001\t{'SNA45677': 1, 'GRO17442': 1, 'SNA77101': 1, 'SNA58915': 1, 'ELE91337': 1, 'DAI52318': 1, 'SNA30579': 1, 'ELE11468': 1, 'FRO35353': 1, 'ELE20166': 1}\n"
     ]
    }
   ],
   "source": [
    "# test our mapper in the command line\n",
    "!chmod +x mapper.py\n",
    "!cat data/ProductPurchaseData_test.txt | ~/w261/mapper.py > Outputs/testing.txt\n",
    "!head Outputs/testing.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "The reducer function takes the dictionaries outputted by the mapper and returns a sorted list for each co-occurring pair:\n",
    "- item1\n",
    "- item2\n",
    "- support count (number of times this pair occurs)\n",
    "- support (portion of baskets this pair occurs in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW3.5\n",
    "\n",
    "# import the system library to read from\n",
    "# the input\n",
    "# import the numpy library to help us sort\n",
    "# before output\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# create a dictionary to store the product pairs\n",
    "# and their respective counts\n",
    "products = {}\n",
    "\n",
    "# keep a running count of how many records we are\n",
    "# dealing with\n",
    "records = 0\n",
    "\n",
    "# write a line to the standard error to keep\n",
    "# track of how often this function is called\n",
    "sys.stderr.write(\"reporter:counter:MyJob,Reducer,1\\n\")\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by the tab\n",
    "    line = line.split(\"\\t\")\n",
    "    \n",
    "    # check to see if this is our records counter\n",
    "    if line[0].strip() == \"Records:\":\n",
    "        records = records + int(line[1])\n",
    "    \n",
    "    # otherwise we know its a pair we need to\n",
    "    # process\n",
    "    else:\n",
    "        \n",
    "        # set the first item\n",
    "        item1 = line[0]\n",
    "        \n",
    "        # first let's convert it back into \n",
    "        # a dictionary\n",
    "        paired = eval(line[1])\n",
    "        \n",
    "        # loop through each of the paired words\n",
    "        for pair in paired.keys():\n",
    "            \n",
    "            # set the count for the pair\n",
    "            count = paired[pair]\n",
    "            \n",
    "            # create the pair\n",
    "            pairs = item1 + \",\" + pair\n",
    "            \n",
    "            # check to see if it's already in\n",
    "            # the dictionary\n",
    "            if pairs not in products.keys():\n",
    "                products[pairs] = 0\n",
    "            \n",
    "            # add to the count of the dictionary\n",
    "            products[pairs] = products[pairs] + \\\n",
    "            count\n",
    "\n",
    "# create a set of array to store those pairs that\n",
    "# frequently occur together\n",
    "FREQUENT = 100\n",
    "\n",
    "# loop through all the pairs in the dictionary\n",
    "for pair in products.keys():\n",
    "    \n",
    "    # if the pair occurs more than 100 times\n",
    "    if int(products[pair]) > FREQUENT:\n",
    "        \n",
    "        # split the items and store them\n",
    "        item1 = pair.split(',')[0]\n",
    "        item2 = pair.split(',')[1]\n",
    "        \n",
    "        # get the count and relative \n",
    "        # frequency\n",
    "        count = products[pair]\n",
    "        frequ = float(count)/float(records)\n",
    "        \n",
    "        # create the string to print\n",
    "        info = item1 + \"\\t\" + item2 + \"\\t\" + \\\n",
    "        str(count) + \"\\t\" + str(frequ)\n",
    "        print info\n",
    "        \n",
    "# # convert our arrays to numpy arrays\n",
    "# item1s = np.array(item1s)\n",
    "# item2s = np.array(item2s)\n",
    "# counts = np.array(counts)\n",
    "# frequs = np.array(frequs)\n",
    "\n",
    "# # get the indices for the sorted list\n",
    "# indexs = np.argsort(frequs)[::-1]\n",
    "\n",
    "# # print the elements to the output\n",
    "# print \"Item1\\t\\tItem2\\t\\tCount\\tSupport\"\n",
    "# for index in indexs:\n",
    "    \n",
    "#     # gather all the information\n",
    "#     item1 = item1s[index]\n",
    "#     item2 = item2s[index]\n",
    "#     count = counts[index]\n",
    "#     frequ = frequs[index]\n",
    "    \n",
    "#     # create the line to print out\n",
    "#     line = str(item1) + \"\\t\" + str(item2) + \\\n",
    "#     \"\\t\" + str(count) + \"\\t\" + str(frequ)\n",
    "    \n",
    "#     # print the line out\n",
    "#     print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:MyJob,Reducer,1\n",
      "ELE17451\tGRO99222\t15\t0.3\n",
      "ELE17451\tGRO73461\t11\t0.22\n",
      "GRO73461\tELE17451\t11\t0.22\n",
      "GRO99222\tELE17451\t15\t0.3\n"
     ]
    }
   ],
   "source": [
    "# test the reducer with the mapper's output\n",
    "# slightly modify the reducer to lower the\n",
    "# frequency to 10\n",
    "!chmod +x reducer.py\n",
    "!cat Outputs/testing.txt | ~/w261/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the test data in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 21:04:16 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchaseData.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchaseData.txt1465099456855\n",
      "16/06/04 21:04:29 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-5' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-51465099469400\n",
      "16/06/04 21:04:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5725157001918088713.jar tmpDir=null\n",
      "16/06/04 21:04:35 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 21:04:36 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 21:04:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 21:04:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 21:04:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465098020484_0003\n",
      "16/06/04 21:04:39 INFO impl.YarnClientImpl: Submitted application application_1465098020484_0003\n",
      "16/06/04 21:04:39 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465098020484_0003/\n",
      "16/06/04 21:04:39 INFO mapreduce.Job: Running job: job_1465098020484_0003\n",
      "16/06/04 21:04:59 INFO mapreduce.Job: Job job_1465098020484_0003 running in uber mode : false\n",
      "16/06/04 21:04:59 INFO mapreduce.Job:  map 0% reduce 0%\n"
     ]
    }
   ],
   "source": [
    "# write the current time to a file\n",
    "!echo $(date) > Outputs/Time_3_5\n",
    "\n",
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/ProductPurchaseData_test2.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-5\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_5\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_5\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-5\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-5/part-00000 /home/cloudera/w261/Outputs/Out_3_5\n",
    "\n",
    "# add the ending time to the file\n",
    "!echo $(date) >> Outputs/Time_3_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the full data through Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 20:43:43 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/ProductPurchaseData_test.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/ProductPurchaseData_test.txt1465098223712\n",
      "16/06/04 20:43:52 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-3-5' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-3-51465098232312\n",
      "16/06/04 20:43:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4761734599335409084.jar tmpDir=null\n",
      "16/06/04 20:43:56 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 20:43:57 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/04 20:43:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 20:43:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 20:43:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465098020484_0001\n",
      "16/06/04 20:43:59 INFO impl.YarnClientImpl: Submitted application application_1465098020484_0001\n",
      "16/06/04 20:43:59 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465098020484_0001/\n",
      "16/06/04 20:43:59 INFO mapreduce.Job: Running job: job_1465098020484_0001\n",
      "16/06/04 20:44:09 INFO mapreduce.Job: Job job_1465098020484_0001 running in uber mode : false\n",
      "16/06/04 20:44:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 20:44:22 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/04 20:44:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 20:44:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 20:44:34 INFO mapreduce.Job: Job job_1465098020484_0001 completed successfully\n",
      "16/06/04 20:44:34 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=28067\n",
      "\t\tFILE: Number of bytes written=415836\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8046\n",
      "\t\tHDFS: Number of bytes written=102\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2945024\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=963840\n",
      "\t\tTotal time spent by all map tasks (ms)=23008\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7530\n",
      "\t\tTotal vcore-seconds taken by all map tasks=23008\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7530\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=2945024\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=963840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=268\n",
      "\t\tMap output bytes=86896\n",
      "\t\tMap output materialized bytes=26198\n",
      "\t\tInput split bytes=262\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=219\n",
      "\t\tReduce shuffle bytes=26198\n",
      "\t\tReduce input records=268\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=536\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=212\n",
      "\t\tCPU time spent (ms)=2640\n",
      "\t\tPhysical memory (bytes) snapshot=416141312\n",
      "\t\tVirtual memory (bytes) snapshot=2192523264\n",
      "\t\tTotal committed heap usage (bytes)=143130624\n",
      "\tMyJob\n",
      "\t\tMapper=2\n",
      "\t\tReducer=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7784\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=102\n",
      "16/06/04 20:44:34 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-3-5\n"
     ]
    }
   ],
   "source": [
    "# write the current time to a file\n",
    "!echo $(date) > Outputs/Time_3_5\n",
    "\n",
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/ProductPurchaseData_test.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-3-5\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_3_5\n",
    "\n",
    "# make the directory to store the output\n",
    "!mkdir /home/cloudera/w261/Outputs/Out_3_5\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/cloudera/w261/mapper.py,/home/cloudera/w261/reducer.py \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-3-5\n",
    "\n",
    "# copy the output files to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-3-5/part-00000 /home/cloudera/w261/Outputs/Out_3_5\n",
    "\n",
    "# add the ending time to the file\n",
    "!echo $(date) >> Outputs/Time_3_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use of counters\n",
    "We used a total of 3 counters, 2 for the mappers and 1 for the reducer. We would have had more instances of the reducer counter had we used a combiner. However, in this case, we elected to not use a combiner.<br>\n",
    "![Counters_3.5](https://dl.dropboxusercontent.com/u/37624818/W261_Week3/HW3.5_Counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elapsed time for the stripes approach \n",
      "is 0:01:13\n"
     ]
    }
   ],
   "source": [
    "elapsed_3_5 = CalcTime(\"Outputs/Time_3_5\")\n",
    "print \"The elapsed time for the stripes approach \\\n",
    "\\nis\",elapsed_3_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "| Approach |   Time  |\n",
      "+----------+---------+\n",
      "|  Pairs   | 0:01:21 |\n",
      "| Stripes  | 0:01:13 |\n",
      "+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "# let's compare the times\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# create a new table\n",
    "pretty = PrettyTable([\"Approach\",\"Time\"])\n",
    "pretty.add_row([\"Pairs\",elapsed_3_4])\n",
    "pretty.add_row([\"Stripes\",elapsed_3_5])\n",
    "\n",
    "# show the table\n",
    "print pretty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
