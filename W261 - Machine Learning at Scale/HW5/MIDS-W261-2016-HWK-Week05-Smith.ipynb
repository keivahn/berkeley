{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "**Alex Smith**<br>\n",
    "June 11, 2016<br>\n",
    "MIDS261 - Machine Learning at Scale<br>\n",
    "Professor Shanahan<br>\n",
    "Due: June 19, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Useful resources\n",
    "The following resources were particularly useful.\n",
    "- [Wikipedia article on data warehouses](https://en.wikipedia.org/wiki/Data_warehouse)\n",
    "- Async 5.4\n",
    "\n",
    "### Libraries\n",
    "The following libraries must be installed before running the below code. They can all be installed through [Pip](https://github.com/pypa/pip).\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "- [Regular Expression](https://docs.python.org/2/library/re.html)\n",
    "- [Pretty Table](https://pypi.python.org/pypi/PrettyTable)\n",
    "- [Random](https://docs.python.org/2/library/random.html)\n",
    "- [Datetime](https://docs.python.org/2/library/datetime.html)\n",
    "- [NLTK](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 5.0\n",
    "*What is a data warehouse? What is a Star schema? When is it used?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **data warehouse** is a central repository of data from diverse sources. Traditionally, data warehouse have stored relational data. However, they are increasingly also storing semi-structured (e.g. logs) and unstructured (e.g. tweets) data. Data warehouses have historically formed the foundation of business intelligence. Now, they are increasingly used for data science. More simply, we can think of a data warehouse as a system for reporting and data analysis. <br>\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.0_Warehouse.png\" width=\"400\">\n",
    "*Source: Async slides 5.4*<br>\n",
    "<br>\n",
    "A **star schema** is a manner of organizing data by using one or more fact tables to reference any number of dimension tables (Async 5.4). For example, in the image below we see a a single fact table that stores the sales. Each sales record has a date, store, product, and units sold field. The date, store, and product fields all reference dimension tables. For example, the store_id for a given transaction will be linked to information on the store number and the location. Star schemas are used when needing to organize large amounts of data for quick querying and editing. By linking to multiple dimension tables, the star schema is easily updatable. Let's continue to consisder the example below. If we realize that our information for a given product is inaccurate and needs to be updated, it is easy to update the data with a star schema. We just need to update the information in the product dimension table for this specific product. If instead, we had stored the information about the product in the fact table, then we would have to search for every instance of that product information in the fact table and update it there. \n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.0_StarSchema.png\" width=\"400\">\n",
    "*Source: Async slides 5.4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW 5.1\n",
    "*In the database world What is 3NF? Does machine learning use data in 3NF? If so why? <br>\n",
    "In what form does machine learning consume data? <br>\n",
    "Why would one use log files that are denormalized?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the database world, **3NF** stands for third normal form. The 3rd normal form is the third step in normalizing a database that builds on the 1st and 2nd normal forms. The 1st normal form requires that a record in a database with attributes that contain only single indivisible elements. For example, if we are attempting to store the telephone information for an individual, we might need to store multiple telephone numbers for a single individual. The simplest, and possibly most intuitive, approach would be to just list two telephone numbers in the telephone field.\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.1_1NF.png\" width=\"300\">\n",
    "*Source: [Wikipedia, First Normal Form](https://en.wikipedia.org/wiki/First_normal_form)*<br><br>\n",
    "However, this would not be in 1st normal form because the entry for the telephone is not a single indivisible element. To make this data compliant with 1st normal form, we could split the record for that individual so that we had two records each with a telephone number.<br>\n",
    "Data is in 2nd normal form if it meets all the requirements of 1st normal form and all the non-key attributes depend on the whole key and not just part of the key. This can best be illustrated by an example. Consider the table below. Each record has a primary key composed of the customer_id and store_id.\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.1_2NF1.png\" width=\"300\">\n",
    "*Source: [Keydata.com](https://www.1keydata.com/database-normalization/second-normal-form-2nf.php)*<br><br>\n",
    "However, the purchase location is dependent only part of the key, just the store_id. This table fails to meet the standards of the 2nd normal form for this reason. To make the data fit into 2nd normal form requires us to split the data into two tables, like below, where each attribute depends on the whole primary key.\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.1_2NF2.png\" width=\"400\">\n",
    "*Source: [Keydata.com](https://www.1keydata.com/database-normalization/second-normal-form-2nf.php)*<br><br>\n",
    "Only after meeting the requirements of the 1st and 2nd normal forms, can we attempt to normalize our data to the 3rd normal form. 3rd normal form adds the additional requirement that all attributes are determined solely by the key value. This is best illustrated through an example. Consider the following table below that shows the winner for a few tournaments. The tournament and the year combined key that is required to identify the row.\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.1_3NF1.png\" width=\"400\">\n",
    "*Source: [Wikipedia, Third Normal Form](https://en.wikipedia.org/wiki/Third_normal_form)*<br><br>\n",
    "However, when looking at the data, it is clear that the year of birth is dependent on the winner, a non-key attribute. This is risky because it introduces the possibility that the years of birth could be different for the same winner. To bring this data into compliance with 3rd normal form, we would need to split the data into 2 tables like below.\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.1_3NF2.png\" width=\"400\">\n",
    "*Source: [Wikipedia, Third Normal Form](https://en.wikipedia.org/wiki/Third_normal_form)*<br><br>\n",
    "**Machine learning** consumes data in denormalized form. This is because we want our model to consume the data with all the featues available. Normalizing the data begins hiding the features in other related tables. If we do get normalized data and need to do some machine learning, we should first de-normalize the data. We can do this by joining the multiple tables together. A denormalized **log file** would be very useful precisely for this reason. A denormalized log file would have all the features for each entry and would provide all the features to the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 5.2\n",
    "*Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.).<br>\n",
    "Justify which table you chose as the Left table in this hashside join.<br>\n",
    "<br>\n",
    "Please report the number of rows resulting from:*\n",
    "1. *Left joining Table Left with Table Right*\n",
    "2. *Right joining Table Left with Table Right*\n",
    "3. *Inner joining Table Left with Table Right*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining our files\n",
    "We have two files that we will be joining: \"MS_weblog.txt\" and \"MS_webpages.txt\". The weblog file has a list of visitor ids and webpage ids. The webpages file has a list of webpage ids and the URLs. We choose the URLs data as the left table that we hold in memory because it is smaller. We want to put the smaller table in memory. See the diagram from Async 5.10 slides:\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week5/HW5.2_MapSideJoin.png\" width=\"300\">\n",
    "*Source: Async 5.10 Slides*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the MRJob class that performs an right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting right_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile right_mapper.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class right_mapper(MRJob):\n",
    "    \n",
    "    # create a value to dictionary\n",
    "    # to hold the data of the left\n",
    "    # table\n",
    "    left_table = {}\n",
    "    \n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                      mapper=self.mapper,\\\n",
    "                      reducer=None)]\n",
    "    \n",
    "    \n",
    "    # we need to initalize our mapper by\n",
    "    # storing the left table in memory\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # open the left table, the list \n",
    "        # urls with their ids\n",
    "        with open('MS_webpages.txt','r') as\\\n",
    "        myfile:\n",
    "            \n",
    "            # read each line in the table\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line by the commas\n",
    "                # and set the id and url values\n",
    "                line = line.split(',')\n",
    "                _id = int(line[0].strip())\n",
    "                _url = line[1].strip()\n",
    "                \n",
    "                # add the url and id to the \n",
    "                # dictionary\n",
    "                self.left_table[_id] = _url\n",
    "        \n",
    "    \n",
    "    # we read each line being fed into the\n",
    "    # the mapper and output the record and \n",
    "    # also include the webpage url if it \n",
    "    # exists\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by commas\n",
    "        # and set the web_id and customer_id\n",
    "        # values\n",
    "        line = line.split(',')\n",
    "        web_id = int(line[1].strip())\n",
    "        cust_id = int(line[4].strip())\n",
    "        \n",
    "        # set a blank URL value because\n",
    "        # with a right join we want to \n",
    "        # output all log values regarless\n",
    "        # of whether we can actually find\n",
    "        # a URL\n",
    "        url = \"\"\n",
    "        \n",
    "        # if the web_id exists in the url\n",
    "        # list, set to the url variable to\n",
    "        # that value\n",
    "        if web_id in self.left_table.keys():\n",
    "            url = self.left_table[web_id]\n",
    "            \n",
    "        # set the keys and value we want to lead\n",
    "        # where the key is tuple of web_id and \n",
    "        # url and the value is the customer_id\n",
    "        _key = web_id,url\n",
    "        _value = cust_id\n",
    "        \n",
    "        # yield key and value\n",
    "        yield _key,_value\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    right_mapper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the runner to run the class within this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after a right join: 98654\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from right_mapper import right_mapper \n",
    "\n",
    "# import the pandas library to help us \n",
    "# export our data\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = right_mapper(args=['MS_weblog.txt',\\\n",
    "                        '--file=MS_webpages.txt']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run() \n",
    "\n",
    "    # set a counter for counting the lines of\n",
    "    # output\n",
    "    count = 0\n",
    "    \n",
    "    # create an array to hold the output\n",
    "    output = []\n",
    "    \n",
    "    # stream_output: get access to the output \n",
    "    for line in runner.stream_output():\n",
    "        \n",
    "        # increment the counter\n",
    "        count = count + 1\n",
    "        \n",
    "        # grab the key,value\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        \n",
    "        # set the key,value to terms we \n",
    "        # understand\n",
    "        web_id,web_url = key\n",
    "        cust_id = value\n",
    "        \n",
    "        # set the output line we want and append\n",
    "        # to our output array\n",
    "        output_line = [web_id,web_url,cust_id]\n",
    "        output.append(output_line)\n",
    "        \n",
    "    # convert the output to a pandas and export\n",
    "    # it as a CSV\n",
    "    output_pd = pd.DataFrame(output)\n",
    "    output_pd.to_csv('HW5.2_Output_Right',\\\n",
    "                    header=False,index=False)\n",
    "    \n",
    "    # print out the number of lines we processed\n",
    "    print \"Total records after a right join:\",count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an MRJob class that performs a left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting left_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile left_mapper.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    " \n",
    "class left_mapper(MRJob):\n",
    "    \n",
    "    # create a value to dictionary\n",
    "    # to hold the data of the left\n",
    "    # table\n",
    "    left_table = {}\n",
    "    \n",
    "    # create an output array to store\n",
    "    # our data\n",
    "    outputs = []\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        \n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                      mapper_final=self.mapper_final),\\\n",
    "               MRStep(reducer=self.reducer)]\n",
    "    \n",
    "    # set 2 mappers for this function\n",
    "    def jobconf(self):\n",
    "\n",
    "        orig_jobconf = super(left_mapper, self).jobconf()\n",
    "        custom_jobconf = {\n",
    "            'mapreduce.job.maps': '2',\n",
    "        }\n",
    "\n",
    "        return mrjob.conf.combine_dicts(orig_jobconf, custom_jobconf)\n",
    "    \n",
    "    \n",
    "    # we need to initalize our mapper by\n",
    "    # storing the left table in memory\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # open the left table, the list \n",
    "        # urls with their ids\n",
    "        with open('MS_webpages.txt','r') as\\\n",
    "        myfile:\n",
    "            \n",
    "            # read each line in the table\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line by the commas\n",
    "                # and set the id and url values\n",
    "                line = line.split(',')\n",
    "                _id = int(line[0].strip())\n",
    "                _url = line[1].strip()\n",
    "                \n",
    "                # add the url and id to the \n",
    "                # dictionary. we also add a flag\n",
    "                # indicator that tells us whether\n",
    "                # this particular site has been \n",
    "                # outputted or not\n",
    "                self.left_table[_id] = [_url,False]\n",
    "        \n",
    "    \n",
    "    # we read each line being fed into the\n",
    "    # the mapper and update the outputs array\n",
    "    # and the left-table dictionary\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by commas\n",
    "        # and set the web_id and customer_id\n",
    "        # values\n",
    "        line = line.split(',')\n",
    "        web_id = int(line[1].strip())\n",
    "        cust_id = int(line[4].strip())\n",
    "        \n",
    "        # if the web_id exists in the url\n",
    "        # list, set to the url variable to\n",
    "        # that value, remember we only \n",
    "        # want to consider values that have a\n",
    "        # match in the left table\n",
    "        if web_id in self.left_table.keys():\n",
    "            url = self.left_table[web_id][0]\n",
    "            \n",
    "            # set the indicator in the table\n",
    "            # to true now that we've used this\n",
    "            # web page\n",
    "            self.left_table[web_id][1] = True\n",
    "            \n",
    "            # set the keys and value we want to \n",
    "            # use to update the output array\n",
    "            _key = web_id,url\n",
    "            _value = cust_id\n",
    "        \n",
    "            # update the outputs table\n",
    "            yield _key,_value\n",
    "        \n",
    "    \n",
    "    # we want to now add any webpages that\n",
    "    # we were missing, we'll yield them\n",
    "    # with a very special key\n",
    "    def mapper_final(self):\n",
    "\n",
    "        # set the special key\n",
    "        special_key = 99\n",
    "        \n",
    "        # loop through the left table dictionary\n",
    "        for web_id in self.left_table.keys():\n",
    "            \n",
    "            # if the page has not been viewed,\n",
    "            # output it\n",
    "            if self.left_table[web_id][1]==False:\n",
    "                url = self.left_table[web_id][0]\n",
    "                _value = web_id,url\n",
    "                \n",
    "                yield special_key,_value\n",
    "                \n",
    "    \n",
    "    # our final reducer really just passes\n",
    "    # the outputs from the mappers and does\n",
    "    # very little work on its own\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        # set the special key\n",
    "        special_key = 99\n",
    "        \n",
    "        # if it's our special key, we'll add\n",
    "        # the websites to our dictionary\n",
    "        if key == special_key:\n",
    "            \n",
    "            # create a dictionary for the \n",
    "            # unseen websites\n",
    "            unseen = {}    \n",
    "            \n",
    "            # loop through all the websites\n",
    "            # and add them to the dictionary\n",
    "            for web_id,url in values:             \n",
    "                \n",
    "                # check to see if we've already \n",
    "                # added it\n",
    "                if web_id not in unseen.keys():\n",
    "                    unseen[web_id] = [url,0]\n",
    "                    \n",
    "                # increment the count for the \n",
    "                # unseen\n",
    "                unseen[web_id][1] = \\\n",
    "                unseen[web_id][1] + 1\n",
    "            \n",
    "            # loop through the dictionary\n",
    "            # and yield each unseen webpage\n",
    "            for web_id in unseen.keys():\n",
    "                \n",
    "                # check to see if the unseen has a\n",
    "                # value of 2 since we want it to \n",
    "                # be unseen by both mappers\n",
    "                if unseen[web_id][1] == 2:\n",
    "                    _key = web_id,unseen[web_id][0]\n",
    "                    _value = \"\"\n",
    "                    yield _key,_value\n",
    "        \n",
    "        # otherwise we're just going to yield\n",
    "        # each website\n",
    "        else:\n",
    "            \n",
    "            # set the web id and the web url\n",
    "            web_id,web_url = key\n",
    "            _key = web_id,web_url\n",
    "            \n",
    "            # loop through each customer visit\n",
    "            for cust in values:\n",
    "                _value = cust\n",
    "                \n",
    "                yield _key,_value\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    left_mapper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the runner and run the job in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after a left join: 98663\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from left_mapper import left_mapper \n",
    "\n",
    "# import the pandas library to help us \n",
    "# export our data\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = left_mapper(args=['MS_weblog.txt',\\\n",
    "                           '--file=MS_webpages.txt']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run() \n",
    "    \n",
    "    # initalize a counter to keep track of where we\n",
    "    # are\n",
    "    count = 0\n",
    "    \n",
    "    # create an output array to hold our findings\n",
    "    output = []\n",
    "    \n",
    "    # stream_output: get access to the output \n",
    "    for line in runner.stream_output():\n",
    "        \n",
    "        # increment the counter\n",
    "        count = count + 1\n",
    "        \n",
    "        # grab the key,value\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "\n",
    "        # set the key,value to terms we \n",
    "        # understand\n",
    "        web_id,web_url = key\n",
    "        cust_id = value\n",
    "        \n",
    "        # set the output line we want and append\n",
    "        # to our output array\n",
    "        output_line = [web_id,web_url,cust_id]\n",
    "        output.append(output_line)\n",
    "        \n",
    "    # convert the output to a pandas and export\n",
    "    # it as a CSV\n",
    "    output_pd = pd.DataFrame(output)\n",
    "    output_pd.to_csv('HW5.2_Output_Left',\\\n",
    "                    header=False,index=False)\n",
    "    \n",
    "    # print out the number of lines we processed\n",
    "    print \"Total records after a left join:\",len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an MRJob class that complete an inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inner_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inner_mapper.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class inner_mapper(MRJob):\n",
    "    \n",
    "    # create a value to dictionary\n",
    "    # to hold the data of the left\n",
    "    # table\n",
    "    left_table = {}\n",
    "    \n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init,\\\n",
    "                      mapper=self.mapper,\\\n",
    "                      reducer=None)]\n",
    "    \n",
    "    \n",
    "    # we need to initalize our mapper by\n",
    "    # storing the left table in memory\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # open the left table, the list \n",
    "        # urls with their ids\n",
    "        with open('MS_webpages.txt','r') as\\\n",
    "        myfile:\n",
    "            \n",
    "            # read each line in the table\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line by the commas\n",
    "                # and set the id and url values\n",
    "                line = line.split(',')\n",
    "                _id = int(line[0].strip())\n",
    "                _url = line[1].strip()\n",
    "                \n",
    "                # add the url and id to the \n",
    "                # dictionary\n",
    "                self.left_table[_id] = _url\n",
    "        \n",
    "    \n",
    "    # we read each line being fed into the\n",
    "    # the mapper and output the record only \n",
    "    # if the webpage url of it also exists\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by commas\n",
    "        # and set the web_id and customer_id\n",
    "        # values\n",
    "        line = line.split(',')\n",
    "        web_id = int(line[1].strip())\n",
    "        cust_id = int(line[4].strip())\n",
    "        \n",
    "        # if the web_id exists in the url\n",
    "        # list, set to the url variable to\n",
    "        # that value\n",
    "        if web_id in self.left_table.keys():\n",
    "            url = self.left_table[web_id]\n",
    "            \n",
    "            # set the keys and value we want to lead\n",
    "            # where the key is tuple of web_id and \n",
    "            # url and the value is the customer_id\n",
    "            _key = web_id,url\n",
    "            _value = cust_id\n",
    "        \n",
    "            # yield key and value\n",
    "            yield _key,_value\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inner_mapper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a runner and run the job in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after an inner join: 98654\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from inner_mapper import inner_mapper\n",
    "\n",
    "# import the pandas library to help us \n",
    "# export our data\n",
    "import pandas as pd\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = inner_mapper(args=['MS_weblog.txt',\\\n",
    "                            '--file=MS_webpages.txt']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run() \n",
    "\n",
    "    # set a counter for counting the lines of\n",
    "    # output\n",
    "    count = 0\n",
    "    \n",
    "    # create an array to hold the output\n",
    "    output = []\n",
    "    \n",
    "    # stream_output: get access to the output \n",
    "    for line in runner.stream_output():\n",
    "        \n",
    "        # increment the counter\n",
    "        count = count + 1\n",
    "        \n",
    "        # grab the key,value\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        \n",
    "        # set the key,value to terms we \n",
    "        # understand\n",
    "        web_id,web_url = key\n",
    "        cust_id = value\n",
    "        \n",
    "        # set the output line we want and append\n",
    "        # to our output array\n",
    "        output_line = [web_id,web_url,cust_id]\n",
    "        output.append(output_line)\n",
    "        \n",
    "    # convert the output to a pandas and export\n",
    "    # it as a CSV\n",
    "    output_pd = pd.DataFrame(output)\n",
    "    output_pd.to_csv('HW5.2_Output_Inner',\\\n",
    "                    header=False,index=False)\n",
    "    \n",
    "    # print out the number of lines we processed\n",
    "    print \"Total records after an inner join:\",count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the joins we completed above, we can see that every customer log record has an associated website (hence right join equals inner join), but not all websites have a customer visit (hence left join greater than right join).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### <span style=\"color:green\">Disclaimer: All MRJobs below use multiple reducers, with the exception of any final sorts. Any final sorts use a single reducer and an identity mapper only for the very last step of sorting.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 5.3  Exploratory data analysis of Google n-grams dataset\n",
    "*A large [subset](https://aws.amazon.com/datasets/google-books-ngrams/) of the Google n-grams dataset, which we have placed in a bucket/folder on [Dropbox](https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 ) on [s3](s3://filtered-5grams/).<br>\n",
    "<br>\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:<br>\n",
    "(ngram) \\t (count) \\t (pages_count) \\t (books_count)<br>\n",
    "<br>\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the first 10 lines of the following file: googlebooks-eng-all-5gram-20090715-0-filtered.txt <br>\n",
    "Once you are happy with your test results proceed to generating your results on the Google n-grams dataset.<br>\n",
    "<br>\n",
    "Do some EDA on this dataset using mrjob, e.g.:*<br>\n",
    "- *Longest 5-gram (number of characters)*\n",
    "- *Top 10 most frequent words (please use the count information), i.e., unigrams*\n",
    "- *20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency*\n",
    "- *Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a testing data set that we can test locally and is relatively small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating test data\n"
     ]
    }
   ],
   "source": [
    "!head -100 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt > test.txt\n",
    "print \"Finished creating test data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./test.txt to s3://aks-w261-hw5/test.txt\r\n"
     ]
    }
   ],
   "source": [
    "# move the the testing data to s3\n",
    "!aws s3 cp test.txt s3://aks-w261-hw5/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRJob Class that finds the longest n-gram\n",
    "We have multiple choices for how to implement this MRJob problem. We choose one that outputs multiple maximums, one per reducer. We can then find the maximum of these. This will be trivial because now finding the maximum will be over a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longestNgram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longestNgram.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class longestNgram(MRJob):\n",
    "    \n",
    "    # longest ngram length and the \n",
    "    # actual ngram\n",
    "    max_length = 0\n",
    "    max_ngram = None\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                      reducer=self.reducer,\\\n",
    "                      reducer_final=self.reducer_final)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # calculates the length and returns the \n",
    "    # length of the line and the ngram\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        ngram = line[0]\n",
    "        \n",
    "        # calculate the length of the ngram\n",
    "        length = len(ngram)\n",
    "\n",
    "        # yield the length and the ngram\n",
    "        # as a key,value pair\n",
    "        yield length,ngram\n",
    "        \n",
    "    \n",
    "    # our reducer reads in the ngrams for \n",
    "    # each length\n",
    "    def reducer(self, length, ngrams):\n",
    "        \n",
    "        # compare the length to the maximum\n",
    "        # length and only execute if its bigger\n",
    "        if length > self.max_length:\n",
    "            \n",
    "            # set the new max length and ngram\n",
    "            self.max_length = length\n",
    "            \n",
    "            # set the max ngram to an array\n",
    "            # and fill with all the ngrams that\n",
    "            # meet this max\n",
    "            self.max_ngram = []\n",
    "            for ngram in ngrams:\n",
    "                self.max_ngram.append(ngram)\n",
    "                \n",
    "    \n",
    "    # our reducer final then outputs the maximum\n",
    "    # ngram and the ngrams that make up this length\n",
    "    def reducer_final(self):\n",
    "        \n",
    "        # yield the maximum ngram and its length\n",
    "        yield self.max_length,self.max_ngram\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    longestNgram.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a runner to test on a small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest ngrams had a length of 34\n",
      "These ngrams are: ['A HANDBOOK ON THEODOLITE SURVEYING']\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from longestNgram import longestNgram\n",
    "\n",
    "# import numpy to help us with the last step of\n",
    "# finding the maximum\n",
    "import numpy as np\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = longestNgram(args=['test.txt']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create arrays to hold the outputs from\n",
    "    # our reducers, we allow multiple reducers\n",
    "    # so that our program is easily scalable\n",
    "    lengths = []\n",
    "    ngrams = []\n",
    "    \n",
    "    # stream_output: get access to the output \n",
    "    for line in runner.stream_output():\n",
    "        \n",
    "        # grab the key,value\n",
    "        _length,_ngrams =  mr_job.parse_output_line(line)\n",
    "        \n",
    "        # add the length to the lengths and add \n",
    "        # the ngrams to our ngram list\n",
    "        lengths.append(_length)\n",
    "        ngrams.append(list(_ngrams))\n",
    "    \n",
    "    # get the index of the maximum ngram by \n",
    "    # converting our arrays to numpy arrays\n",
    "    lengths = np.array(lengths)\n",
    "    ngrams = np.array(ngrams)\n",
    "    max_index = np.argmax(lengths)\n",
    "    \n",
    "    # use the index to grab the maximum values\n",
    "    max_length = lengths[max_index]\n",
    "    max_ngrams = ngrams[max_index]\n",
    "    \n",
    "    # print out the maximum ngram and its length\n",
    "    print \"The longest ngrams had a length of\",\\\n",
    "    max_length\n",
    "    print \"These ngrams are:\",max_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the runner on the test data but on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/no_script.Alex.20160618.174830.395929\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/no_script.Alex.20160618.174830.395929/files/...\n",
      "j-1V97UD5N6422Z\n"
     ]
    }
   ],
   "source": [
    "# create the cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-1 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/longestNgram.Alex.20160618.175957.065679\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/longestNgram.Alex.20160618.175957.065679/files/...\n",
      "Adding our job to existing cluster j-1V97UD5N6422Z\n",
      "Waiting for step 1 of 1 (s-C77A3L5EDWFP) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40458/cluster\n",
      "  RUNNING for 19.2s\n",
      "     0.0% complete\n",
      "  RUNNING for 54.2s\n",
      "    10.6% complete\n",
      "  RUNNING for 84.9s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-C77A3L5EDWFP on ec2-54-183-30-12.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-30-12.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-C77A3L5EDWFP/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45912\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1835\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2750\n",
      "\t\tFILE: Number of bytes written=3639121\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1872\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=45912\n",
      "\t\tS3: Number of bytes written=1835\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=24\n",
      "\t\tLaunched map tasks=24\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=799482240\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=388719360\n",
      "\t\tTotal time spent by all map tasks (ms)=555196\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24983820\n",
      "\t\tTotal time spent by all reduce tasks (ms)=134972\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12147480\n",
      "\t\tTotal vcore-seconds taken by all map tasks=555196\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=134972\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=34780\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=8998\n",
      "\t\tInput split bytes=1872\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=3119\n",
      "\t\tMap output materialized bytes=7417\n",
      "\t\tMap output records=100\n",
      "\t\tMerged Map outputs=264\n",
      "\t\tPhysical memory (bytes) snapshot=14714015744\n",
      "\t\tReduce input groups=18\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=7417\n",
      "\t\tShuffled Maps =264\n",
      "\t\tSpilled Records=200\n",
      "\t\tTotal committed heap usage (bytes)=16750477312\n",
      "\t\tVirtual memory (bytes) snapshot=82899214336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/longestNgram.Alex.20160618.175957.065679/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/longestNgram.Alex.20160618.175957.065679...\n",
      "Killing our SSH tunnel (pid 15589)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python longestNgram.py -r emr s3://aks-w261-hw5/test.txt \\\n",
    "    --cluster-id=j-1V97UD5N6422Z \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTERS\tj-1V97UD5N6422Z\tno_script.Alex.20160618.174830.395929\t26\r\n",
      "STATUS\tWAITING\r\n",
      "STATECHANGEREASON\tCluster ready after last step completed.\r\n",
      "TIMELINE\t1466272124.47\t1466272475.9\r\n",
      "CLUSTERS\tj-2ZBRZB00G8QK0\tno_script.Alex.20160618.174309.971368\t0\r\n",
      "STATUS\tTERMINATED_WITH_ERRORS\r\n",
      "STATECHANGEREASON\tVALIDATION_ERROR\tSubnet is required : The specified instance type c4.large can only be used in a VPC.\r\n",
      "TIMELINE\t1466271809.74\t1466271814.99\r\n",
      "CLUSTERS\tj-1AKN0BR62Y9V3\tW261\t8\r\n",
      "STATUS\tTERMINATED\r\n",
      "STATECHANGEREASON\tUSER_REQUEST\tTerminated by user request\r\n",
      "TIMELINE\t1466093499.95\t1466096435.34\t1466094081.38\r\n",
      "CLUSTERS\tj-2N44R4Q31HKC3\tW261\t0\r\n",
      "STATUS\tTERMINATED_WITH_ERRORS\r\n",
      "STATECHANGEREASON\tVALIDATION_ERROR\tService role  not authorized to call EC2\r\n",
      "TIMELINE\t1466088056.78\t1466088170.12\r\n"
     ]
    }
   ],
   "source": [
    "# list our clusters\n",
    "!aws emr list-clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the entire corpus of data\n",
    "We've completed unit testing both locally and in the could. Let's try running our program over the entire thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_3/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00000      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00001      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00006      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00007      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00003      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00008      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00005      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00004       \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00002       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/longestNgram.Alex.20160618.184415.363713\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/longestNgram.Alex.20160618.184415.363713/files/...\n",
      "Adding our job to existing cluster j-1V97UD5N6422Z\n",
      "Waiting for step 1 of 1 (s-1KUZSP23LJJ5O) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40458/cluster\n",
      "  RUNNING for 14.4s\n",
      "   100.0% complete\n",
      "  RUNNING for 47.4s\n",
      "     5.0% complete\n",
      "  RUNNING for 78.6s\n",
      "     7.6% complete\n",
      "  RUNNING for 109.8s\n",
      "    11.3% complete\n",
      "  RUNNING for 141.4s\n",
      "    16.0% complete\n",
      "  RUNNING for 172.8s\n",
      "    19.8% complete\n",
      "  RUNNING for 204.0s\n",
      "    23.7% complete\n",
      "  RUNNING for 235.6s\n",
      "    28.1% complete\n",
      "  RUNNING for 267.0s\n",
      "    33.1% complete\n",
      "  RUNNING for 298.8s\n",
      "    37.1% complete\n",
      "  RUNNING for 329.8s\n",
      "    40.8% complete\n",
      "  RUNNING for 360.9s\n",
      "    44.4% complete\n",
      "  RUNNING for 391.8s\n",
      "    48.4% complete\n",
      "  RUNNING for 422.5s\n",
      "    52.3% complete\n",
      "  RUNNING for 453.4s\n",
      "    56.0% complete\n",
      "  RUNNING for 484.6s\n",
      "    80.2% complete\n",
      "  RUNNING for 515.2s\n",
      "    82.0% complete\n",
      "  RUNNING for 546.9s\n",
      "    93.4% complete\n",
      "  RUNNING for 578.2s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1KUZSP23LJJ5O on ec2-54-183-30-12.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-30-12.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1KUZSP23LJJ5O/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1759\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1019827603\n",
      "\t\tFILE: Number of bytes written=2062294771\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=1759\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tKilled reduce tasks=2\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=13\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8443088640\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6837145920\n",
      "\t\tTotal time spent by all map tasks (ms)=5863256\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=263846520\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2374009\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=213660810\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5863256\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2374009\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2623420\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=63082\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=1862272363\n",
      "\t\tMap output materialized bytes=1021611542\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=103364632576\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce input records=58682266\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=1021611542\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=117364532\n",
      "\t\tTotal committed heap usage (bytes)=124261498880\n",
      "\t\tVirtual memory (bytes) snapshot=409737420800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/longestNgram.Alex.20160618.184415.363713/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/longestNgram.Alex.20160618.184415.363713...\n",
      "Killing our SSH tunnel (pid 15695)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python longestNgram.py -r emr s3://filtered-5grams/ \\\n",
    "    --cluster-id=j-1V97UD5N6422Z \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Output the longest ngram</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_longest: File exists\n",
      "_SUCCESS   part-00001 part-00003 part-00005 part-00007 part-00009\n",
      "part-00000 part-00002 part-00004 part-00006 part-00008 part-00010\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_longest\n",
    "\n",
    "# sync the files to a local directory\n",
    "!aws s3 sync s3://aks-w261-hw5/out_5_3 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_longest\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_longest\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_longest/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each reducer has already spit out the ngram it finds as the longest. We now simply loop through the output files and grab the longest one. This task is quicker done locally than in the cloud because the number of outputs is small enough that we can easily hold the potential longest ngrams in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum ngram has a length of 159\n",
      "The ngrams of this length are [['ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT', 'AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR']]\n"
     ]
    }
   ],
   "source": [
    "# import the os and ast libraries\n",
    "# to help us get the files and read\n",
    "# in the files\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# create a dictionary to hold the longest\n",
    "# ngrams outputted by each reducer\n",
    "longest_ngrams = {}\n",
    "\n",
    "# loop through our files and add each the longest\n",
    "# ngrams to the dictionary, where the key is the \n",
    "# length and the value is the ngrams\n",
    "indir = '/Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_longest/'\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    \n",
    "    # loop through each file\n",
    "    for filename in filenames:\n",
    "        \n",
    "        # set the filename\n",
    "        filename = indir+filename\n",
    "        \n",
    "        # open the file\n",
    "        with open(filename,'r') as myfile:\n",
    "            \n",
    "            # read the line in the file\n",
    "            line = myfile.readline()\n",
    "            \n",
    "            # split the line by tabs\n",
    "            line = line.split(\"\\t\")\n",
    "            \n",
    "            # set the count and the ngrams\n",
    "            count = int(line[0])\n",
    "            ngrams = ast.literal_eval(line[1])\n",
    "            \n",
    "            # check to see if we have this key\n",
    "            # in the dictionary and if not, let's\n",
    "            # add it\n",
    "            if count not in longest_ngrams:\n",
    "                longest_ngrams[count] = []\n",
    "                \n",
    "            # add the ngrams to this list\n",
    "            longest_ngrams[count].append(ngrams)\n",
    "            \n",
    "# initalize a maximum value that we'll overwrite\n",
    "max_count = 0\n",
    "max_ngrams = None\n",
    "\n",
    "# loop through each element in the dictionary\n",
    "# compare it to the max value and if it's a new\n",
    "# maximum update our values\n",
    "for count in longest_ngrams:\n",
    "    if count > max_count:\n",
    "        max_count = count\n",
    "        max_ngrams = longest_ngrams[count]\n",
    "\n",
    "# print out the maximum ngram\n",
    "print \"The maximum ngram has a length of\",max_count\n",
    "print \"The ngrams of this length are\",max_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most frequently occuring words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the MRJob class that will perform a word count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol   \n",
    "    \n",
    "class wordcount(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        \n",
    "        # set how we want to sort, we use a single reducer\n",
    "        # only so that we can do a total sort, we use multiple\n",
    "        # reducers for all other steps\n",
    "        JOBCONF = {        \n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       combiner=self.combiner,\\\n",
    "                       reducer=self.reducer),\\\n",
    "                MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=None,\\\n",
    "                       reducer=self.reducer_final)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # splits each line into multiple words\n",
    "    # and then returns the word with a count\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # ngram and lower-case it\n",
    "        line = line.split(\"\\t\")\n",
    "        ngram = line[0].lower()\n",
    "        count = int(line[1])\n",
    "        \n",
    "        # split the ngram by the spaces\n",
    "        ngram = ngram.split()\n",
    "\n",
    "        # loop through each word in the \n",
    "        # ngram and yield the word and\n",
    "        # its count\n",
    "        for word in ngram: \n",
    "            yield word, str(count)\n",
    "            \n",
    "    \n",
    "    # our combiner sums the values for each\n",
    "    # word\n",
    "    def combiner(self, word, counts):\n",
    "        \n",
    "        # loop through the counts, summing\n",
    "        # as we go along\n",
    "        sum_count = 0\n",
    "        for count in counts:\n",
    "            sum_count = sum_count+int(count)\n",
    "        \n",
    "        # yield the counts for each word\n",
    "        yield word, str(sum_count)\n",
    "        \n",
    "    \n",
    "    # our reducer performs the final count\n",
    "    # for all words\n",
    "    def reducer(self, word, counts):\n",
    "        \n",
    "        # loop through the counts, summing\n",
    "        # as we go along\n",
    "        sum_count = 0\n",
    "        for count in counts:\n",
    "            sum_count = sum_count+int(count)\n",
    "        \n",
    "        # convert the count to a string\n",
    "        sum_count = str(sum_count)\n",
    "        \n",
    "        # we want to make sure that the count \n",
    "        # string is at least 10 characters, if\n",
    "        # its not, we add leading zeros. this\n",
    "        # helps us sort\n",
    "        extend = \"0\"\n",
    "        \n",
    "        # while the count lenght is less than 8\n",
    "        while len(sum_count) < 10:\n",
    "            sum_count = extend + sum_count\n",
    "        \n",
    "        # yield the counts as the key so that\n",
    "        # we can the maximum\n",
    "        yield str(sum_count), word\n",
    "                \n",
    "    \n",
    "    # our reducer final then outputs a sorted\n",
    "    # list of the words \n",
    "    def reducer_final(self, count, words):\n",
    "        \n",
    "        # loop through the list with the words\n",
    "        for word in words:\n",
    "            yield str(int(count)), word\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    wordcount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.010610.690478\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.010610.690478/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.010610.690478...\n",
      "42\tcritique\n",
      "44\tbased\n",
      "44\tmethod\n",
      "45\tassessment\n",
      "45\tmethodology\n",
      "48\tdevelop\n",
      "51\tfurther\n",
      "51\ther\n",
      "51\tlakota\n",
      "51\tlook\n"
     ]
    }
   ],
   "source": [
    "# Test the program on the small dataset\n",
    "!python wordcount.py test.txt > 5_3_frequentWords\n",
    "!head 5_3_frequentWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the runner on the test data but on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/no_script.Alex.20160619.003604.699420\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/no_script.Alex.20160619.003604.699420/files/...\n",
      "j-2C5FHH6GSAKSB\n"
     ]
    }
   ],
   "source": [
    "# create the cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-1 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_3/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00000      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00006      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00002      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00005      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00003      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00008      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00001      \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00004       \n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00007       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.010639.049441\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/wordcount.Alex.20160619.010639.049441/files/...\n",
      "Adding our job to existing cluster j-2C5FHH6GSAKSB\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Waiting for step 1 of 2 (s-2HKMIDN787KO6) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40531/cluster\n",
      "  RUNNING for 27.9s\n",
      "     5.0% complete\n",
      "  RUNNING for 61.5s\n",
      "    46.3% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2HKMIDN787KO6 on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2HKMIDN787KO6/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45912\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3957\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3878\n",
      "\t\tFILE: Number of bytes written=3671830\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1872\n",
      "\t\tHDFS: Number of bytes written=3957\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=81\n",
      "\t\tHDFS: Number of write operations=22\n",
      "\t\tS3: Number of bytes read=45912\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=24\n",
      "\t\tLaunched map tasks=24\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=883091520\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=291297600\n",
      "\t\tTotal time spent by all map tasks (ms)=613258\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27596610\n",
      "\t\tTotal time spent by all reduce tasks (ms)=101145\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9103050\n",
      "\t\tTotal vcore-seconds taken by all map tasks=613258\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=101145\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=153870\n",
      "\t\tCombine input records=500\n",
      "\t\tCombine output records=347\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=8007\n",
      "\t\tInput split bytes=1872\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=4879\n",
      "\t\tMap output materialized bytes=8308\n",
      "\t\tMap output records=500\n",
      "\t\tMerged Map outputs=264\n",
      "\t\tPhysical memory (bytes) snapshot=14131720192\n",
      "\t\tReduce input groups=214\n",
      "\t\tReduce input records=347\n",
      "\t\tReduce output records=214\n",
      "\t\tReduce shuffle bytes=8308\n",
      "\t\tShuffled Maps =264\n",
      "\t\tSpilled Records=694\n",
      "\t\tTotal committed heap usage (bytes)=16125001728\n",
      "\t\tVirtual memory (bytes) snapshot=82820009984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-3I1UOHMXF5VS0) to complete...\n",
      "  RUNNING for 57.0s\n",
      "    39.1% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3I1UOHMXF5VS0 on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3I1UOHMXF5VS0/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6492\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2594\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2610\n",
      "\t\tFILE: Number of bytes written=3137349\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10871\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=58\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=2594\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=25\n",
      "\t\tLaunched map tasks=29\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=927367200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=54207360\n",
      "\t\tTotal time spent by all map tasks (ms)=644005\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28980225\n",
      "\t\tTotal time spent by all reduce tasks (ms)=18822\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1693980\n",
      "\t\tTotal vcore-seconds taken by all map tasks=644005\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=18822\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=31540\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=12652\n",
      "\t\tInput split bytes=4379\n",
      "\t\tMap input records=214\n",
      "\t\tMap output bytes=4171\n",
      "\t\tMap output materialized bytes=4020\n",
      "\t\tMap output records=214\n",
      "\t\tMerged Map outputs=29\n",
      "\t\tPhysical memory (bytes) snapshot=15663816704\n",
      "\t\tReduce input groups=214\n",
      "\t\tReduce input records=214\n",
      "\t\tReduce output records=214\n",
      "\t\tReduce shuffle bytes=4020\n",
      "\t\tShuffled Maps =29\n",
      "\t\tSpilled Records=428\n",
      "\t\tTotal committed heap usage (bytes)=16690708480\n",
      "\t\tVirtual memory (bytes) snapshot=60293427200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/wordcount.Alex.20160619.010639.049441/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.010639.049441...\n",
      "Killing our SSH tunnel (pid 16409)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python wordcount.py -r emr s3://aks-w261-hw5/test.txt \\\n",
    "    --cluster-id=j-2C5FHH6GSAKSB \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the word count on the entire corpus using AWS\n",
    "Now that we've finished testing on a subset of the data, let's try it on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_3/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00000     \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.011047.204054\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/wordcount.Alex.20160619.011047.204054/files/...\n",
      "Adding our job to existing cluster j-2C5FHH6GSAKSB\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Waiting for step 1 of 2 (s-2CR3SF4BXE3GG) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40531/cluster\n",
      "  RUNNING for 1.1s\n",
      "   100.0% complete\n",
      "  RUNNING for 33.4s\n",
      "     5.0% complete\n",
      "  RUNNING for 64.3s\n",
      "     5.3% complete\n",
      "  RUNNING for 96.1s\n",
      "     8.1% complete\n",
      "  RUNNING for 127.7s\n",
      "     9.0% complete\n",
      "  RUNNING for 158.8s\n",
      "    11.1% complete\n",
      "  RUNNING for 190.0s\n",
      "    13.6% complete\n",
      "  RUNNING for 221.4s\n",
      "    14.6% complete\n",
      "  RUNNING for 252.4s\n",
      "    17.0% complete\n",
      "  RUNNING for 284.5s\n",
      "    19.2% complete\n",
      "  RUNNING for 315.2s\n",
      "    20.6% complete\n",
      "  RUNNING for 346.4s\n",
      "    23.0% complete\n",
      "  RUNNING for 377.3s\n",
      "    25.4% complete\n",
      "  RUNNING for 408.1s\n",
      "    27.3% complete\n",
      "  RUNNING for 439.0s\n",
      "    29.1% complete\n",
      "  RUNNING for 470.4s\n",
      "    30.5% complete\n",
      "  RUNNING for 501.6s\n",
      "    32.8% complete\n",
      "  RUNNING for 532.8s\n",
      "    34.2% complete\n",
      "  RUNNING for 563.6s\n",
      "    35.7% complete\n",
      "  RUNNING for 595.4s\n",
      "    37.5% complete\n",
      "  RUNNING for 626.6s\n",
      "    38.7% complete\n",
      "  RUNNING for 659.3s\n",
      "    40.5% complete\n",
      "  RUNNING for 690.2s\n",
      "    42.2% complete\n",
      "  RUNNING for 721.9s\n",
      "    43.7% complete\n",
      "  RUNNING for 754.6s\n",
      "    45.4% complete\n",
      "  RUNNING for 785.4s\n",
      "    47.2% complete\n",
      "  RUNNING for 817.6s\n",
      "    48.6% complete\n",
      "  RUNNING for 849.6s\n",
      "    50.3% complete\n",
      "  RUNNING for 881.7s\n",
      "    52.0% complete\n",
      "  RUNNING for 912.9s\n",
      "    53.3% complete\n",
      "  RUNNING for 943.9s\n",
      "    55.2% complete\n",
      "  RUNNING for 975.7s\n",
      "    63.3% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2CR3SF4BXE3GG on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2CR3SF4BXE3GG/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5467696\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=38202070\n",
      "\t\tFILE: Number of bytes written=142843203\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=5467696\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=413\n",
      "\t\tHDFS: Number of write operations=22\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19188744480\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=10445578560\n",
      "\t\tTotal time spent by all map tasks (ms)=13325517\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=599648265\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3626937\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=326424330\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13325517\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3626937\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5990990\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=89585\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=2843318430\n",
      "\t\tMap output materialized bytes=83609265\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=128929230848\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=83609265\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=144772694016\n",
      "\t\tVirtual memory (bytes) snapshot=409824456704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-3GFHSPFPB67HD) to complete...\n",
      "  RUNNING for 45.8s\n",
      "     5.0% complete\n",
      "  RUNNING for 76.4s\n",
      "    54.1% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3GFHSPFPB67HD on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3GFHSPFPB67HD/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6217412\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3889400\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3167675\n",
      "\t\tFILE: Number of bytes written=10151479\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6222395\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=66\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=3889400\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=31\n",
      "\t\tLaunched map tasks=33\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1148302080\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=97966080\n",
      "\t\tTotal time spent by all map tasks (ms)=797432\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35884440\n",
      "\t\tTotal time spent by all reduce tasks (ms)=34016\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3061440\n",
      "\t\tTotal vcore-seconds taken by all map tasks=797432\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=34016\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=75760\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=14627\n",
      "\t\tInput split bytes=4983\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=5737035\n",
      "\t\tMap output materialized bytes=3435644\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=33\n",
      "\t\tPhysical memory (bytes) snapshot=17235947520\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=3435644\n",
      "\t\tShuffled Maps =33\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=18669371392\n",
      "\t\tVirtual memory (bytes) snapshot=68186292224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/wordcount.Alex.20160619.011047.204054/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordcount.Alex.20160619.011047.204054...\n",
      "Killing our SSH tunnel (pid 16434)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python wordcount.py -r emr s3://filtered-5grams/ \\\n",
    "    --cluster-id=j-2C5FHH6GSAKSB \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aks-w261-hw5/out_5_3/_SUCCESS to 5_3_wordcount/_SUCCESS\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00000 to 5_3_wordcount/part-00000\n",
      "_SUCCESS   part-00000\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_wordcount\n",
    "\n",
    "# sync the files to a local directory\n",
    "!aws s3 sync s3://aks-w261-hw5/out_5_3 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_wordcount\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_wordcount\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_wordcount/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the top 10 words by word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5490815394\tthe\t\r\n",
      "3698583299\tof\t\r\n",
      "2227866570\tto\t\r\n",
      "1421312776\tin\t\r\n",
      "1361123022\ta\t\r\n",
      "1149577477\tand\t\r\n",
      "802921147\tthat\t\r\n",
      "758328796\tis\t\r\n",
      "688707130\tbe\t\r\n",
      "492170314\tas\t\r\n"
     ]
    }
   ],
   "source": [
    "!head 5_3_wordcount/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an MRJob class to calculate the density of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordDensity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordDensity.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import ast\n",
    "from mrjob.protocol import RawProtocol\n",
    " \n",
    "class wordDensity(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        \n",
    "        # set how we want to sort, we use a single reducer\n",
    "        # only so that we can do a total sort, we use multiple\n",
    "        # reducers for all other steps\n",
    "        JOBCONF = {        \n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       combiner=self.combiner,\\\n",
    "                       reducer=self.reducer),\\\n",
    "               MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=None,\\\n",
    "                       reducer=self.reducer_final)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # splits each line into multiple words\n",
    "    # and then returns the word with a count\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # ngram and lower-case it\n",
    "        line = line.split(\"\\t\")\n",
    "        ngram = line[0].lower()\n",
    "        \n",
    "        # set the counts of interest\n",
    "        count = int(line[1])\n",
    "        page_count = int(line[2])\n",
    "        counts = (count,page_count)\n",
    "        \n",
    "        # split the ngram by the spaces\n",
    "        ngram = ngram.split()\n",
    "\n",
    "        # loop through each word in the \n",
    "        # ngram and yield the word and a\n",
    "        # count of 1\n",
    "        for word in ngram: \n",
    "            yield word, str(counts)\n",
    "            \n",
    "    \n",
    "    # our combiner sums the values for each\n",
    "    # word\n",
    "    def combiner(self, word, counts):\n",
    "        \n",
    "        # create an array to hold the counts\n",
    "        # for each element and convert it to\n",
    "        # a numpy array to allow summing\n",
    "        totals = [0,0]\n",
    "        totals = np.array(totals)\n",
    "        \n",
    "        # loop through each element in the\n",
    "        # counts generator\n",
    "        for _count in counts:\n",
    "            \n",
    "            # read in the count\n",
    "            _count = ast.literal_eval(_count)\n",
    "            count,page_count = _count\n",
    "            \n",
    "            # create an array to hold the\n",
    "            # count and page count\n",
    "            sub_total = [count,page_count]\n",
    "            sub_total = np.array(sub_total)\n",
    "            \n",
    "            # add the values to our total \n",
    "            # array\n",
    "            totals = totals + sub_total\n",
    "        \n",
    "        # conver the counts list to a tuple\n",
    "        counts = (totals[0],totals[1])\n",
    "        \n",
    "        # yield the counts for each word\n",
    "        yield word, str(counts)\n",
    "        \n",
    "    \n",
    "    # our reducer performs the final count\n",
    "    # for all words\n",
    "    def reducer(self, word, counts):\n",
    "        \n",
    "        # create an array to hold the counts\n",
    "        # for each element and convert it to\n",
    "        # a numpy array to allow summing\n",
    "        totals = [0,0]\n",
    "        totals = np.array(totals)\n",
    "        \n",
    "        # loop through each element in the\n",
    "        # counts generator\n",
    "        for _count in counts:\n",
    "            \n",
    "            # read in the count\n",
    "            _count = ast.literal_eval(_count)\n",
    "            count,page_count = _count\n",
    "            \n",
    "            # create an array to hold the\n",
    "            # count and page count\n",
    "            sub_total = [count,page_count]\n",
    "            sub_total = np.array(sub_total)\n",
    "            \n",
    "            # add the values to our total \n",
    "            # array\n",
    "            totals = totals + sub_total\n",
    "        \n",
    "        # for each word divide the count\n",
    "        # by the page_count to calculate\n",
    "        # the density\n",
    "        density = float(totals[0])/\\\n",
    "        float(totals[1])\n",
    "        \n",
    "        # we want to make sure that the count \n",
    "        # string is at least 15 characters, if\n",
    "        # its not, we add leading zeros. this\n",
    "        # helps us sort\n",
    "        extend = \"0\"\n",
    "        \n",
    "        # convert density to a string\n",
    "        density = str(density)\n",
    "        \n",
    "        # while the count lenght is less than 15\n",
    "        while len(density) < 15:\n",
    "            density = density + extend\n",
    "        \n",
    "        # yield each word with its density\n",
    "        yield str(density),word\n",
    "    \n",
    "    \n",
    "    # our reducer final simply yields the inputs\n",
    "    # from the indentity mapper and outputs a\n",
    "    # completely sorted list\n",
    "    def reducer_final(self, density, words):\n",
    "        \n",
    "        # loop through the list with the words\n",
    "        for word in words:\n",
    "            yield str(density), word    \n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    wordDensity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.024917.633110\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.024917.633110/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.024917.633110...\n",
      "1.0000000000000\taerial\n",
      "1.0000000000000\tamerica's\n",
      "1.0000000000000\tanalysis\n",
      "1.0000000000000\tapology\n",
      "1.0000000000000\tapproximation\n",
      "1.0000000000000\tare\n",
      "1.0000000000000\tarithmetic\n",
      "1.0000000000000\taspiration\n",
      "1.0000000000000\tassessment\n",
      "1.0000000000000\tb\n"
     ]
    }
   ],
   "source": [
    "# Test the program on the small dataset\n",
    "!python wordDensity.py test.txt > 5_3_density_prelim\n",
    "!head 5_3_density_prelim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on the cloud with a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_3/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00000     \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.025039.146114\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/wordDensity.Alex.20160619.025039.146114/files/...\n",
      "Adding our job to existing cluster j-2C5FHH6GSAKSB\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Waiting for step 1 of 2 (s-HAGLW978UQGO) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40531/cluster\n",
      "  RUNNING for 19.5s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 52.0s\n",
      "  RUNNING for 83.3s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-HAGLW978UQGO on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-HAGLW978UQGO/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45912\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5027\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5547\n",
      "\t\tFILE: Number of bytes written=3676424\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1872\n",
      "\t\tHDFS: Number of bytes written=5027\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=81\n",
      "\t\tHDFS: Number of write operations=22\n",
      "\t\tS3: Number of bytes read=45912\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=24\n",
      "\t\tLaunched map tasks=24\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=927961920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=304044480\n",
      "\t\tTotal time spent by all map tasks (ms)=644418\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28998810\n",
      "\t\tTotal time spent by all reduce tasks (ms)=105571\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9501390\n",
      "\t\tTotal vcore-seconds taken by all map tasks=644418\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=105571\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=157320\n",
      "\t\tCombine input records=500\n",
      "\t\tCombine output records=347\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=7596\n",
      "\t\tInput split bytes=1872\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=8139\n",
      "\t\tMap output materialized bytes=10540\n",
      "\t\tMap output records=500\n",
      "\t\tMerged Map outputs=264\n",
      "\t\tPhysical memory (bytes) snapshot=14180884480\n",
      "\t\tReduce input groups=214\n",
      "\t\tReduce input records=347\n",
      "\t\tReduce output records=214\n",
      "\t\tReduce shuffle bytes=10540\n",
      "\t\tShuffled Maps =264\n",
      "\t\tSpilled Records=694\n",
      "\t\tTotal committed heap usage (bytes)=16625172480\n",
      "\t\tVirtual memory (bytes) snapshot=82872950784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-GIEET9FFSCJ6) to complete...\n",
      "  RUNNING for 74.4s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-GIEET9FFSCJ6 on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-GIEET9FFSCJ6/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8232\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5241\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2887\n",
      "\t\tFILE: Number of bytes written=3138665\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=12669\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=58\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=5241\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=25\n",
      "\t\tLaunched map tasks=29\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=895520160\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=52917120\n",
      "\t\tTotal time spent by all map tasks (ms)=621889\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27985005\n",
      "\t\tTotal time spent by all reduce tasks (ms)=18374\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1653660\n",
      "\t\tTotal vcore-seconds taken by all map tasks=621889\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=18374\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=29150\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=10036\n",
      "\t\tInput split bytes=4437\n",
      "\t\tMap input records=214\n",
      "\t\tMap output bytes=5241\n",
      "\t\tMap output materialized bytes=4557\n",
      "\t\tMap output records=214\n",
      "\t\tMerged Map outputs=29\n",
      "\t\tPhysical memory (bytes) snapshot=15172284416\n",
      "\t\tReduce input groups=214\n",
      "\t\tReduce input records=214\n",
      "\t\tReduce output records=214\n",
      "\t\tReduce shuffle bytes=4557\n",
      "\t\tShuffled Maps =29\n",
      "\t\tSpilled Records=428\n",
      "\t\tTotal committed heap usage (bytes)=16104030208\n",
      "\t\tVirtual memory (bytes) snapshot=60257116160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/wordDensity.Alex.20160619.025039.146114/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.025039.146114...\n",
      "Killing our SSH tunnel (pid 16729)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python wordDensity.py -r emr s3://aks-w261-hw5/test.txt \\\n",
    "    --cluster-id=j-2C5FHH6GSAKSB \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the word count on the entire corpus using AWS\n",
    "Now that we've finished testing on a subset of the data, let's try it on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_3/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00000     \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.025656.535057\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/wordDensity.Alex.20160619.025656.535057/files/...\n",
      "Adding our job to existing cluster j-2C5FHH6GSAKSB\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Waiting for step 1 of 2 (s-23RT1DKUT3Z8P) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40531/cluster\n",
      "  RUNNING for 9.3s\n",
      "   100.0% complete\n",
      "  RUNNING for 41.7s\n",
      "     5.0% complete\n",
      "  RUNNING for 73.0s\n",
      "     6.1% complete\n",
      "  RUNNING for 104.4s\n",
      "     8.3% complete\n",
      "  RUNNING for 136.0s\n",
      "     8.5% complete\n",
      "  RUNNING for 167.4s\n",
      "     8.5% complete\n",
      "  RUNNING for 198.5s\n",
      "     8.5% complete\n",
      "  RUNNING for 230.3s\n",
      "     8.9% complete\n",
      "  RUNNING for 261.2s\n",
      "     9.5% complete\n",
      "  RUNNING for 293.3s\n",
      "    11.1% complete\n",
      "  RUNNING for 324.4s\n",
      "    11.9% complete\n",
      "  RUNNING for 356.2s\n",
      "    13.5% complete\n",
      "  RUNNING for 387.4s\n",
      "    13.5% complete\n",
      "  RUNNING for 418.2s\n",
      "    14.0% complete\n",
      "  RUNNING for 450.3s\n",
      "    14.6% complete\n",
      "  RUNNING for 481.6s\n",
      "    15.4% complete\n",
      "  RUNNING for 512.8s\n",
      "    15.8% complete\n",
      "  RUNNING for 544.4s\n",
      "    17.1% complete\n",
      "  RUNNING for 575.9s\n",
      "    17.9% complete\n",
      "  RUNNING for 608.3s\n",
      "    19.0% complete\n",
      "  RUNNING for 640.0s\n",
      "    19.6% complete\n",
      "  RUNNING for 671.1s\n",
      "    20.4% complete\n",
      "  RUNNING for 702.9s\n",
      "    21.3% complete\n",
      "  RUNNING for 734.5s\n",
      "    22.1% complete\n",
      "  RUNNING for 766.4s\n",
      "    22.6% complete\n",
      "  RUNNING for 797.6s\n",
      "    23.8% complete\n",
      "  RUNNING for 828.8s\n",
      "    25.0% complete\n",
      "  RUNNING for 860.9s\n",
      "    25.6% complete\n",
      "  RUNNING for 892.2s\n",
      "    25.6% complete\n",
      "  RUNNING for 924.1s\n",
      "    26.9% complete\n",
      "  RUNNING for 955.7s\n",
      "    28.2% complete\n",
      "  RUNNING for 987.2s\n",
      "    29.1% complete\n",
      "  RUNNING for 1018.5s\n",
      "    29.8% complete\n",
      "  RUNNING for 1050.4s\n",
      "    30.7% complete\n",
      "  RUNNING for 1082.1s\n",
      "    32.3% complete\n",
      "  RUNNING for 1113.6s\n",
      "    32.5% complete\n",
      "  RUNNING for 1144.9s\n",
      "    32.8% complete\n",
      "  RUNNING for 1176.9s\n",
      "    34.0% complete\n",
      "  RUNNING for 1208.0s\n",
      "    35.4% complete\n",
      "  RUNNING for 1239.8s\n",
      "    35.9% complete\n",
      "  RUNNING for 1271.2s\n",
      "    36.4% complete\n",
      "  RUNNING for 1302.9s\n",
      "    36.4% complete\n",
      "  RUNNING for 1334.4s\n",
      "    37.1% complete\n",
      "  RUNNING for 1366.3s\n",
      "    38.7% complete\n",
      "  RUNNING for 1397.5s\n",
      "    39.6% complete\n",
      "  RUNNING for 1428.9s\n",
      "    39.6% complete\n",
      "  RUNNING for 1460.0s\n",
      "    40.7% complete\n",
      "  RUNNING for 1491.2s\n",
      "    41.3% complete\n",
      "  RUNNING for 1522.4s\n",
      "    41.3% complete\n",
      "  RUNNING for 1553.9s\n",
      "    42.3% complete\n",
      "  RUNNING for 1585.2s\n",
      "    43.9% complete\n",
      "  RUNNING for 1616.7s\n",
      "    44.5% complete\n",
      "  RUNNING for 1648.4s\n",
      "    44.8% complete\n",
      "  RUNNING for 1681.9s\n",
      "    45.1% complete\n",
      "  RUNNING for 1713.4s\n",
      "    45.7% complete\n",
      "  RUNNING for 1744.6s\n",
      "    47.2% complete\n",
      "  RUNNING for 1775.8s\n",
      "    48.3% complete\n",
      "  RUNNING for 1807.3s\n",
      "    48.4% complete\n",
      "  RUNNING for 1838.6s\n",
      "    49.0% complete\n",
      "  RUNNING for 1870.5s\n",
      "    49.9% complete\n",
      "  RUNNING for 1901.8s\n",
      "    50.3% complete\n",
      "  RUNNING for 1933.3s\n",
      "    51.1% complete\n",
      "  RUNNING for 1965.1s\n",
      "    52.6% complete\n",
      "  RUNNING for 1996.6s\n",
      "    53.1% complete\n",
      "  RUNNING for 2027.6s\n",
      "    53.4% complete\n",
      "  RUNNING for 2059.0s\n",
      "    53.8% complete\n",
      "  RUNNING for 2090.4s\n",
      "    54.4% complete\n",
      "  RUNNING for 2121.2s\n",
      "    58.8% complete\n",
      "  RUNNING for 2152.5s\n",
      "    60.4% complete\n",
      "  RUNNING for 2183.7s\n",
      "    60.5% complete\n",
      "  RUNNING for 2214.8s\n",
      "    82.1% complete\n",
      "  RUNNING for 2246.3s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-23RT1DKUT3Z8P on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-23RT1DKUT3Z8P/syslog.2016-06-19-02\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-23RT1DKUT3Z8P/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6814391\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=61032463\n",
      "\t\tFILE: Number of bytes written=192933161\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=6814391\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=413\n",
      "\t\tHDFS: Number of write operations=22\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=45363044160\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=23658759360\n",
      "\t\tTotal time spent by all map tasks (ms)=31502114\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1417595130\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8214847\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=739336230\n",
      "\t\tTotal vcore-seconds taken by all map tasks=31502114\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8214847\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=18484720\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=94201\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=4703685170\n",
      "\t\tMap output materialized bytes=110864887\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=138155126784\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=110864887\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=143625027584\n",
      "\t\tVirtual memory (bytes) snapshot=409769627648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-1M1VY9KIS6BNS) to complete...\n",
      "  RUNNING for 74.3s\n",
      "    80.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1M1VY9KIS6BNS on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1M1VY9KIS6BNS/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7773723\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7083730\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3398114\n",
      "\t\tFILE: Number of bytes written=10677793\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7778772\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=66\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=7083730\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=32\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=34\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1125407520\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=67694400\n",
      "\t\tTotal time spent by all map tasks (ms)=781533\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35168985\n",
      "\t\tTotal time spent by all reduce tasks (ms)=23505\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2115450\n",
      "\t\tTotal vcore-seconds taken by all map tasks=781533\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=23505\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=73680\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=13042\n",
      "\t\tInput split bytes=5049\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=7083730\n",
      "\t\tMap output materialized bytes=3730948\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=33\n",
      "\t\tPhysical memory (bytes) snapshot=17034039296\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=3730948\n",
      "\t\tShuffled Maps =33\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=18386780160\n",
      "\t\tVirtual memory (bytes) snapshot=68171264000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/wordDensity.Alex.20160619.025656.535057/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/wordDensity.Alex.20160619.025656.535057...\n",
      "Killing our SSH tunnel (pid 16757)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python wordDensity.py -r emr s3://filtered-5grams/ \\\n",
    "    --cluster-id=j-2C5FHH6GSAKSB \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_worddensity: File exists\n",
      "download: s3://aks-w261-hw5/out_5_3/_SUCCESS to 5_3_worddensity/_SUCCESS\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00000 to 5_3_worddensity/part-00000\n",
      "_SUCCESS   part-00000\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_worddensity\n",
    "\n",
    "# sync the files to a local directory\n",
    "!aws s3 sync s3://aks-w261-hw5/out_5_3 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_worddensity\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_worddensity\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_worddensity/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 and bottom 20 words by density\n",
    "Print out the top and bottom 20 words by density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words by density\n",
      "\n",
      "11.557291666700\txxxx\t\n",
      "8.0741599073000\tblah\t\n",
      "7.5333333333300\tnnn\t\n",
      "6.2017491314200\tna\t\n",
      "4.9218750000000\toooooooooooooooo\t\n",
      "4.8543057272400\tnd\t\n",
      "4.5116279069800\tllll\t\n",
      "4.1696500133600\toooooo\t\n",
      "3.8586371934700\tooooo\t\n",
      "3.7624521072800\tlillelu\t\n",
      "3.5769230769200\tpfeffermann\t\n",
      "3.5769230769200\tmadarassy\t\n",
      "3.5600000000000\tmeteoritical\t\n",
      "3.5000000000000\txxxxxxxx\t\n",
      "3.2290388548100\tbeep\t\n",
      "3.1886792452800\tlatha\t\n",
      "2.9191176470600\tiyengar\t\n",
      "2.8250000000000\tcounterfeiteth\t\n",
      "2.8198198198200\tnonsquamous\t\n",
      "2.8198198198200\tnonmorular\t\n",
      "\n",
      "\n",
      "Botton 20 words by density\n",
      "\n",
      "1.0000000000000\tabdulmejid\t\n",
      "1.0000000000000\tabdominales\t\n",
      "1.0000000000000\tabdolola\t\n",
      "1.0000000000000\tabderitish\t\n",
      "1.0000000000000\tabcs\t\n",
      "1.0000000000000\tabbott's\t\n",
      "1.0000000000000\tabbiamo\t\n",
      "1.0000000000000\tabbes\t\n",
      "1.0000000000000\tabbaside\t\n",
      "1.0000000000000\tabbanysh\t\n",
      "1.0000000000000\tabbagliato\t\n",
      "1.0000000000000\tabasheth\t\n",
      "1.0000000000000\tabases\t\n",
      "1.0000000000000\tabare\t\n",
      "1.0000000000000\tinterkinetic\t\n",
      "1.0000000000000\taaya\t\n",
      "1.0000000000000\taaws\t\n",
      "1.0000000000000\tgladded\t\n",
      "1.0000000000000\tglacken's\t\n",
      "1.0000000000000\tconcertino\t\n"
     ]
    }
   ],
   "source": [
    "print \"Top 20 words by density\\n\"\n",
    "!head -20 5_3_worddensity/part-00000\n",
    "print \"\\n\"\n",
    "print \"Botton 20 words by density\\n\"\n",
    "!tail -20 5_3_worddensity/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distSize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distSize.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class distSize(MRJob):\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       combiner=self.combiner,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # splits each line, calculates the length\n",
    "    # of the line and returns the length with\n",
    "    # a count of 1\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # ngram and lower-case it\n",
    "        line = line.split(\"\\t\")\n",
    "        ngram = line[0].lower()\n",
    "        count = int(line[1])\n",
    "        \n",
    "        # grab the ngram length\n",
    "        length = len(ngram)\n",
    "\n",
    "        # yield the length and the count \n",
    "        yield length,count\n",
    "            \n",
    "    \n",
    "    # our combiner sums the counts for each\n",
    "    # length\n",
    "    def combiner(self, length, counts):\n",
    "        \n",
    "        # yield the counts for each ngram\n",
    "        yield length, sum(counts)\n",
    "        \n",
    "    \n",
    "    # our reducer performs the final count\n",
    "    # for all words\n",
    "    def reducer(self, length, counts):\n",
    "        \n",
    "        # yield the final lengths for \n",
    "        # each ngram\n",
    "        yield length, sum(counts)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    distSize.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a runner to test our MRJob on a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of length distribution file:\n",
      "17\t114\r\n",
      "18\t146\r\n",
      "19\t528\r\n",
      "20\t888\r\n",
      "21\t678\r\n",
      "22\t3804\r\n",
      "23\t27144\r\n",
      "24\t518\r\n",
      "25\t1404\r\n",
      "26\t1489\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from distSize import distSize\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = distSize(args=['test.txt']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create a file to store the output of this\n",
    "    # step\n",
    "    with open('length_dist','w') as myfile:\n",
    "    \n",
    "        # stream_output: get access to the output \n",
    "        for line in runner.stream_output():\n",
    "\n",
    "            # grab the key,value\n",
    "            length,count =  \\\n",
    "            mr_job.parse_output_line(line)\n",
    "            \n",
    "            # set the information we want to\n",
    "            # write\n",
    "            info = str(length)+\"\\t\"+str(count)+\"\\n\"\n",
    "            \n",
    "            # write the word,density to an\n",
    "            # preliminary output file\n",
    "            myfile.write(info)\n",
    "            \n",
    "# preview the top of the preliminary file \n",
    "# that we created\n",
    "print \"Preview of length distribution file:\"\n",
    "!head length_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a histogram of the length distribution for just a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJQAAAJZCAYAAAD/DG8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuUAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ddbd350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# import pandas and matplotlib to load\n",
    "# the data and plot the histogram\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# read in the data as a data frame\n",
    "data = pd.read_table('length_dist',header=None)\n",
    "data_count = data.iloc[:,1]\n",
    "data_lengt = data.iloc[:,0]\n",
    "\n",
    "# gives histogram bars a width\n",
    "width = 1.0 \n",
    "\n",
    "# plot the histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(data_lengt,data_count, width, color='b')\n",
    "plt.title(\"Distribution of 5Gram Lengths-Sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the program on the complete data\n",
    "Now that we've unit tested, let's move on to actually running the analysis on the complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_3/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_3/part-00000     \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/distSize.Alex.20160619.034106.675317\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/distSize.Alex.20160619.034106.675317/files/...\n",
      "Adding our job to existing cluster j-2C5FHH6GSAKSB\n",
      "Waiting for step 1 of 1 (s-YGA8ZOFVHT7K) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40531/cluster\n",
      "  RUNNING for 16.1s\n",
      "   100.0% complete\n",
      "  RUNNING for 48.8s\n",
      "     5.0% complete\n",
      "  RUNNING for 79.6s\n",
      "     7.8% complete\n",
      "  RUNNING for 111.5s\n",
      "    10.4% complete\n",
      "  RUNNING for 143.2s\n",
      "    13.7% complete\n",
      "  RUNNING for 174.9s\n",
      "    16.2% complete\n",
      "  RUNNING for 206.2s\n",
      "    19.4% complete\n",
      "  RUNNING for 237.9s\n",
      "    22.7% complete\n",
      "  RUNNING for 270.0s\n",
      "    25.8% complete\n",
      "  RUNNING for 301.9s\n",
      "    29.2% complete\n",
      "  RUNNING for 333.1s\n",
      "    32.6% complete\n",
      "  RUNNING for 364.5s\n",
      "    34.8% complete\n",
      "  RUNNING for 395.9s\n",
      "    38.1% complete\n",
      "  RUNNING for 429.4s\n",
      "    40.7% complete\n",
      "  RUNNING for 460.9s\n",
      "    43.2% complete\n",
      "  RUNNING for 492.5s\n",
      "    46.2% complete\n",
      "  RUNNING for 523.4s\n",
      "    48.8% complete\n",
      "  RUNNING for 555.2s\n",
      "    51.9% complete\n",
      "  RUNNING for 586.8s\n",
      "    54.3% complete\n",
      "  RUNNING for 619.5s\n",
      "    77.2% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-YGA8ZOFVHT7K on ec2-54-183-220-200.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-220-200.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-YGA8ZOFVHT7K/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=780\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=56725\n",
      "\t\tFILE: Number of bytes written=21113283\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=780\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11853904320\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6227314560\n",
      "\t\tTotal time spent by all map tasks (ms)=8231878\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=370434510\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2162262\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=194603580\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8231878\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2162262\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2545500\n",
      "\t\tCombine input records=58682266\n",
      "\t\tCombine output records=9172\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=55284\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=372255868\n",
      "\t\tMap output materialized bytes=136284\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=101322788864\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce input records=9172\n",
      "\t\tReduce output records=80\n",
      "\t\tReduce shuffle bytes=136284\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=18344\n",
      "\t\tTotal committed heap usage (bytes)=123371782144\n",
      "\t\tVirtual memory (bytes) snapshot=409652019200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/distSize.Alex.20160619.034106.675317/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/distSize.Alex.20160619.034106.675317...\n",
      "Killing our SSH tunnel (pid 16867)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_3\n",
    "!python distSize.py -r emr s3://filtered-5grams/ \\\n",
    "    --cluster-id=j-2C5FHH6GSAKSB \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_3 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aks-w261-hw5/out_5_3/_SUCCESS to 5_3_distribution/_SUCCESS\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00000 to 5_3_distribution/part-00000\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00004 to 5_3_distribution/part-00004\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00002 to 5_3_distribution/part-00002\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00005 to 5_3_distribution/part-00005\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00006 to 5_3_distribution/part-00006\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00001 to 5_3_distribution/part-00001\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00008 to 5_3_distribution/part-00008\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00003 to 5_3_distribution/part-00003\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00007 to 5_3_distribution/part-00007\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00009 to 5_3_distribution/part-00009\n",
      "download: s3://aks-w261-hw5/out_5_3/part-00010 to 5_3_distribution/part-00010\n",
      "_SUCCESS   part-00001 part-00003 part-00005 part-00007 part-00009\n",
      "part-00000 part-00002 part-00004 part-00006 part-00008 part-00010\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_distribution\n",
    "\n",
    "# sync the files to a local directory\n",
    "!aws s3 sync s3://aks-w261-hw5/out_5_3 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_distribution\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_distribution\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_3_distribution/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\t91\r\n",
      "12\t114018\r\n",
      "24\t894422079\r\n",
      "36\t60137979\r\n",
      "48\t802590\r\n",
      "61\t4984\r\n",
      "73\t182\r\n",
      "9\t14140\r\n",
      "128\t92\r\n",
      "13\t888866\r\n"
     ]
    }
   ],
   "source": [
    "# import the os and pandas libraries\n",
    "# to help us get the files and read them in\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# loop through our files and add each the longest\n",
    "# ngrams to the dictionary, where the key is the \n",
    "# length and the value is the ngrams\n",
    "indir = '/Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_3_distribution/'\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    \n",
    "    # create a file that stores all the distriubtions\n",
    "    with open('5_3_length_distr','w') as myfile:\n",
    "    \n",
    "        # loop through each file\n",
    "        for filename in filenames:\n",
    "\n",
    "            # set the filename\n",
    "            filename = indir+filename\n",
    "\n",
    "            # write every line in the file to\n",
    "            # our new file\n",
    "            with open(filename, 'r') as myoldfile:\n",
    "                for line in myoldfile.readlines():\n",
    "                    \n",
    "                    # split the line and set the values\n",
    "                    line = line.split()\n",
    "                    length = int(line[0])\n",
    "                    count = int(line[1])\n",
    "                    \n",
    "                    # set the string we're going to write\n",
    "                    info = str(length) + \"\\t\" + str(count) + \"\\n\"\n",
    "                    \n",
    "                    myfile.write(info)\n",
    "                    \n",
    "# preview the file we've created\n",
    "!head 5_3_length_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the histogram for the complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAJZCAYAAAAzlVX+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nErwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f2e3590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# import pandas and matplotlib to load\n",
    "# the data and plot the histogram\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# read in the data\n",
    "data = pd.read_table('5_3_length_distr',header=None)\n",
    "\n",
    "# read in the data as a data frame\n",
    "data_count = data.iloc[:,1]\n",
    "data_lengt = data.iloc[:,0]\n",
    "\n",
    "# gives histogram bars a width\n",
    "width = 1.0 \n",
    "\n",
    "# plot the histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(data_lengt,data_count,width,color='b')\n",
    "plt.title(\"Distribution of 5Gram Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "*For the remainder of this assignment you will work with two datasets:<br>\n",
    "**1: unit/systems test data set: SYSTEMS TEST DATASET**<br>\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms<br>\n",
    "DocA {X:20, Y:30, Z:5}<br>\n",
    "DocB {X:100, Y:20}<br>\n",
    "DocC {M:5, N:20, Z:5}<br>\n",
    "<br>\n",
    "**2: A large subset of the Google n-grams dataset as was described above**<br>\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect to SYSTEMS TEST DATASET and show the results. Please compute the expected answer by hand and show your hand calculations for the SYSTEMS TEST DATASET. Then show the results you get with you system.<br>\n",
    "<br>\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script two main tasks using MRJob:<br>\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms), and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).<br>\n",
    "(2) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file in your bucket on s3.<br>\n",
    "<br>\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2 (feel free to use the solution as reference). To total the word counts across the 5-grams, output the support from the mappers using the total order inversion pattern:<br>\n",
    "(\\*word,count)<br>\n",
    "to ensure that the support arrives before the cooccurrences.<br>\n",
    "<br>\n",
    "In addition to ensuring the determination of the total word counts, the mapper must also output co-occurrence counts for the pairs of words inside of each 5-gram. Treat these words as a basket, as we have in HW 3, but count all stripes or pairs in both orders, i.e., count both orderings: (word1,word2), and (word2,word1), to preserve symmetry in our output for (2).<br>\n",
    "==Design notes for (2)==<br>\n",
    "For this task you will have to determine a method of comparison. Here are a few that you might consider:*<br>\n",
    "- *Jaccard*\n",
    "- *Cosine similarity*\n",
    "- *Spearman correlation*\n",
    "- *Euclidean distance*\n",
    "- *Taxicab (Manhattan) distance*\n",
    "- *Shortest path graph distance (a graph, because our data is symmetric!)*\n",
    "- *Pearson correlation*\n",
    "- *Kendall correlation*\n",
    "...<br>\n",
    "\n",
    "*However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.<br>\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. <br>\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your Cluster configuration!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the systems testing data set\n",
    "This is borrowed and slightly modified from Annie's notebook posted to the GoogleGroup.\n",
    "```\n",
    "DOC         WORDS-STRIPE (count of occurrences of words M,N,X,Y,Z in documents A,B,C)\n",
    "\n",
    "A           {X:20, Y: 30, Z:5}\n",
    "B           {X:100, Y:20 }\n",
    "C           {M:5, N:20, Z:5, Y:1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systems_test.txt file generated\n",
      "A\t{'X':20, 'Y':30, 'Z':5}\r\n",
      "B\t{'X':100, 'Y':20}\r\n",
      "C\t{'M':5, 'N':20, 'Z':5, 'Y':1}\r\n"
     ]
    }
   ],
   "source": [
    "# generate our system_test file\n",
    "!echo \\\n",
    "\"A\\t{'X':20, 'Y':30, 'Z':5}\\n\"\\\n",
    "\"B\\t{'X':100, 'Y':20}\\n\"\\\n",
    "\"C\\t{'M':5, 'N':20, 'Z':5, 'Y':1}\" > systems_test.txt\n",
    "print \"systems_test.txt file generated\"\n",
    "!cat systems_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binarize the stripes so that we can run cosine similarity and jaccard similarity metrics on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import abstract synthax trees to help\n",
    "# read in the data\n",
    "import ast\n",
    "\n",
    "# Create a method that binarizes the stripes.\n",
    "def binarizeStripes():\n",
    "    \n",
    "    # Create an output file to write binary stripes to.\n",
    "    with open ('binary_stripes.txt', 'w') as output_file:\n",
    "    \n",
    "        # Read each line of stripes file.\n",
    "        for line in open('systems_test.txt', 'r'):\n",
    "            line = line.strip()\n",
    "            contents = line.split(\"\\t\")\n",
    "            word = contents[0]\n",
    "\n",
    "            # Read in stripe as a dictionary\n",
    "            stripe = dict(ast.literal_eval(contents[1]))\n",
    "\n",
    "            # Iterate through each value in \n",
    "            # stripes dictionary.\n",
    "            for key in stripe:\n",
    "\n",
    "                # Set all values in stripe to \n",
    "                # binary value (aka 1).\n",
    "                stripe[key] = 1\n",
    "\n",
    "            # write to the output file each words\n",
    "            # binarized document appearance\n",
    "            output_file.write(\"\\t\".join([word, str(stripe)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t{'Y': 1, 'X': 1, 'Z': 1}\r\n",
      "B\t{'Y': 1, 'X': 1}\r\n",
      "C\t{'Y': 1, 'Z': 1, 'M': 1, 'N': 1}\r\n"
     ]
    }
   ],
   "source": [
    "binarizeStripes()\n",
    "!cat binary_stripes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize the stripes\n",
    "The normalization is the inverse of the square root of the sum of squares of the items in the vector. In our case, each item is equal to 1. So, we end up with the square root of the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "\n",
    "# Create a method that normalizes the binary stripes.\n",
    "def NormalizeStripes():\n",
    "    \n",
    "    # Create an output file to write normalized \n",
    "    # stripes to.\n",
    "    with open('normalized_stripes.txt', 'w') \\\n",
    "    as output_file:\n",
    "    \n",
    "        # Read each line of binarized stripes file.\n",
    "        for line in open('binary_stripes.txt', 'r'):\n",
    "            line = line.strip()\n",
    "            contents = line.split(\"\\t\")\n",
    "            word = contents[0]\n",
    "\n",
    "            # Read in stripe as a dictionary\n",
    "            stripe = dict(ast.literal_eval(contents[1]))\n",
    "\n",
    "            # Get length of stripe to use for normalization.\n",
    "            stripeLen = len(stripe)\n",
    "\n",
    "            # Iterate through each value in stripes dictionary.\n",
    "            for key in stripe:\n",
    "\n",
    "                # Normalize each value by dividing by \n",
    "                # sqrt(stripeLength).\n",
    "                stripe[key] = float(1)/math.sqrt(stripeLen)\n",
    "\n",
    "            # write to the output file\n",
    "            output_file.write(\"\\t\".join([word, str(stripe)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t{'Y': 0.5773502691896258, 'X': 0.5773502691896258, 'Z': 0.5773502691896258}\r\n",
      "B\t{'Y': 0.7071067811865475, 'X': 0.7071067811865475}\r\n",
      "C\t{'Y': 0.5, 'Z': 0.5, 'M': 0.5, 'N': 0.5}\r\n"
     ]
    }
   ],
   "source": [
    "NormalizeStripes()\n",
    "!cat normalized_stripes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the inverted index\n",
    "The inverted index is organized by word and shows what document each word appears on.\n",
    "```\n",
    "WORD              DOCUMENT LIST (POSTINGS LIST) \n",
    "\n",
    "M                 {C: 1/2}\n",
    "N                 {C: 1/2}\n",
    "X                 {A:1/sqrt(3), B: 1/sqrt(2)}\n",
    "Y                 {A:1/sqrt(3), B:1/sqrt(2), C: 1/2}\n",
    "Z                 {A:1/sqrt(3), C:1/2}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "\n",
    "# Create a method that computes the inverted index.\n",
    "def CreateInvertedIndex():\n",
    "    \n",
    "    # Create an output file to write inverted index to.\n",
    "    with open('inverted_index.txt', 'w') as outputFile:\n",
    "    \n",
    "        # In-memory inverted index dictionary.\n",
    "        invertedIndex = {}\n",
    "\n",
    "        # Read each line of normalized stripes file.\n",
    "        for line in open('normalized_stripes.txt', 'r'):\n",
    "            line = line.strip()\n",
    "            contents = line.split(\"\\t\")\n",
    "            word = contents[0]\n",
    "\n",
    "            # Read in stripe as a dictionary\n",
    "            stripe = dict(ast.literal_eval(contents[1]))\n",
    "\n",
    "            # Iterate through each value in stripes dictionary and invert.\n",
    "            for key,value in stripe.items():\n",
    "\n",
    "                # Create inverted index for each value in stripe.\n",
    "                if key in invertedIndex:\n",
    "                    invertedIndex[key][word] = value\n",
    "                else:\n",
    "                    invertedIndex[key] = {word: value}\n",
    "\n",
    "        # write to the output file\n",
    "        for key,value in invertedIndex.iteritems():\n",
    "            outputFile.write(\"\\t\".join([key, str(value)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\t{'A': 0.5773502691896258, 'C': 0.5, 'B': 0.7071067811865475}\r\n",
      "X\t{'A': 0.5773502691896258, 'B': 0.7071067811865475}\r\n",
      "Z\t{'A': 0.5773502691896258, 'C': 0.5}\r\n",
      "M\t{'C': 0.5}\r\n",
      "N\t{'C': 0.5}\r\n"
     ]
    }
   ],
   "source": [
    "CreateInvertedIndex()\n",
    "!cat inverted_index.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the cosine similarity\n",
    "To calculate the cosine similarity of two stripe documents, say A and C, compute partial similarities using dot products and aggregating partial similarities to get overall cosine similarity.\n",
    "\n",
    "cosine(stripe A, stripe C) = A[Y]*C[Y] + A[Z]* C[Z]\n",
    "\n",
    "Note: A[M] = A[N] = 0; C[X] = 0, hence M,N,X do not contribute to the cosine similarity.\n",
    "\n",
    "```\n",
    "Cosine(A,B) = (1/sqrt(3))*(1/sqrt(2)) + (1/sqrt(3))*(1/sqrt(2)) + (1/sqrt(3))*0 = 2/sqrt(6)\n",
    "Cosine(A,C) = 0*(1/sqrt(2)) + 0*(1/sqrt(2)) + (1/sqrt(3))*0 + (1/sqrt(3))*1/2 + (1/sqrt(3))*1/2 = 1/sqrt(3)\n",
    "Cosine(B,C) = 0*(1/sqrt(2)) + 0*(1/sqrt(2)) + (1/sqrt(2))*0 + (1/sqrt(2))*1/2 + 0*1/2 = 1/2sqrt(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "\n",
    "# Create a method that computes Cosine similarity from inverted index.\n",
    "def ComputeCosineSim():\n",
    "    \n",
    "    # Create an output file to write cosine similarities to.\n",
    "    with open('cosine_sim.txt', 'w') as outputFile:\n",
    "    \n",
    "        # In-memory inverted index dictionary.\n",
    "        cosineSim = {}\n",
    "\n",
    "        # Read each line of inverted index file.\n",
    "        for line in open('inverted_index.txt', 'r'):\n",
    "            line = line.strip()\n",
    "            contents = line.split(\"\\t\")\n",
    "            word = contents[0]\n",
    "\n",
    "            # Read in stripe as a dictionary\n",
    "            stripe = dict(ast.literal_eval(contents[1]))\n",
    "\n",
    "            # Iterate through each pair in stripes to compute dot product.\n",
    "            for key1,value1 in stripe.items():\n",
    "                for key2,value2 in stripe.items():\n",
    "                    if key1 != key2:\n",
    "                        if (key1, key2) not in cosineSim:\n",
    "                            cosineSim[(key1, key2)] = value1*value2\n",
    "                        else:\n",
    "                            cosineSim[(key1, key2)] += value1*value2\n",
    "\n",
    "        # write the cosine similarities to the output file\n",
    "        for key,value in cosineSim.iteritems():\n",
    "            outputFile.write(\"\\t\".join([str(key), str(value)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B', 'C')\t0.353553390593\r\n",
      "('B', 'A')\t0.816496580928\r\n",
      "('C', 'A')\t0.57735026919\r\n",
      "('C', 'B')\t0.353553390593\r\n",
      "('A', 'B')\t0.816496580928\r\n",
      "('A', 'C')\t0.57735026919\r\n"
     ]
    }
   ],
   "source": [
    "ComputeCosineSim()\n",
    "!cat cosine_sim.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sur that we are on the same page as our program, we also calculate this by one, on paper. We upload the pictures here:\n",
    "![step1](https://dl.dropboxusercontent.com/u/37624818/W261_Week5/Step1.JPG)\n",
    "![step2](https://dl.dropboxusercontent.com/u/37624818/W261_Week5/Step2.JPG)\n",
    "![step3](https://dl.dropboxusercontent.com/u/37624818/W261_Week5/Step3.JPG)\n",
    "![step4](https://dl.dropboxusercontent.com/u/37624818/W261_Week5/Step4.JPG)\n",
    "![step5](https://dl.dropboxusercontent.com/u/37624818/W261_Week5/Step5.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our top 10,000 and synonym dictionary lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created.\n"
     ]
    }
   ],
   "source": [
    "!head -10000 5_3_wordcount/part-00000 > top10000\n",
    "!tail -1000 top10000 > topVocab\n",
    "print \"Files created.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of cluster\n",
    "We use a four m3.xlarge instances on Amazon Web Services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:magenta\"> We now build our MRJob functions based around the simple Python functions we did above.</span>\n",
    "\n",
    "### MRJob class to convert each document into a stripe\n",
    "We remove the stop words and output the inverted index for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripeMaker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripeMaker.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    " \n",
    "class stripeMaker(MRJob):\n",
    "    \n",
    "    # we initalize arrays to hold the words we care\n",
    "    # about. stop_words holds stop words that we wish\n",
    "    # to exclude. vocab holds the vocabularly that we\n",
    "    # care about each word forming stripes with. \n",
    "    # words_interest holds the words we want to form\n",
    "    # stripes for\n",
    "    stop_words = []\n",
    "    vocab = []\n",
    "    words_interest = []\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       combiner=self.reducer,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # set the stop words that we want to compare\n",
    "    # all the words to\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # read in the file of stop words\n",
    "        with open('stopwords.txt','r') as myfile:\n",
    "            \n",
    "            # read in all the lines\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # if the line is not blank, append\n",
    "                # it to our list of stop words\n",
    "                if line.strip() != \"\":\n",
    "                    self.stop_words.append(line.strip())\n",
    "        \n",
    "        # read in the file of vocab\n",
    "        with open('topVocab','r') as myfile:\n",
    "            \n",
    "            # read all the lines\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line and read\n",
    "                # in the vocab\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[1].strip()\n",
    "                \n",
    "                # append the word to the list\n",
    "                self.vocab.append(word)\n",
    "                \n",
    "        # read in the file of words of interest\n",
    "        with open('top10000','r') as myfile:\n",
    "            \n",
    "            # read all the lines\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line and read\n",
    "                # in the word\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[1].strip()\n",
    "                \n",
    "                # append the word to the list\n",
    "                self.words_interest.append(word)\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and splits out the count for each word\n",
    "    # and the associated document\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # ngram and lower-case it\n",
    "        line = line.split(\"\\t\")\n",
    "        ngram = line[0].lower()\n",
    "        count = int(line[1])\n",
    "\n",
    "        # set the words\n",
    "        words = ngram.split()\n",
    "        \n",
    "        # loop through each word in the ngram\n",
    "        for word in words:\n",
    "\n",
    "            # check and make sure it's in our \n",
    "            # words of interests\n",
    "            if word in self.words_interest:\n",
    "            \n",
    "                # make sure it's not in our stop \n",
    "                # words\n",
    "                if word not in self.stop_words:\n",
    "\n",
    "                    # create a dictionary to hold the\n",
    "                    # word\n",
    "                    word_coocur = {}\n",
    "\n",
    "                    # loop through all the other words\n",
    "                    for other_word in words:\n",
    "\n",
    "                        # make sure the other_word is in\n",
    "                        # our vocab\n",
    "                        if other_word in self.vocab:\n",
    "                        \n",
    "                            # make sure word not in our stop\n",
    "                            # words\n",
    "                            if other_word not in self.stop_words:\n",
    "\n",
    "                                # if it's not the same word\n",
    "                                if word != other_word:\n",
    "\n",
    "\n",
    "                                    # if we haven't aready found\n",
    "                                    # this other word for this word\n",
    "                                    if other_word not in word_coocur:\n",
    "                                        word_coocur[other_word] = count\n",
    "                                    else:\n",
    "                                        word_coocur[other_word] = \\\n",
    "                                        word_coocur[other_word] + count\n",
    "\n",
    "                    # yield word and its associated\n",
    "                    # co-occurences if we've actually \n",
    "                    # add a co-occurence that's not a stop word\n",
    "                    if len(word_coocur) > 0:\n",
    "                        yield word, str(word_coocur)\n",
    "        \n",
    "    \n",
    "    # our reducer combines the counts for all\n",
    "    # words\n",
    "    def reducer(self, word, cooccurs):\n",
    "        \n",
    "        # create a dictionary to hold the \n",
    "        # co-occurence counts for each word\n",
    "        cooccur = {}\n",
    "        \n",
    "        # loop through the coocurrences \n",
    "        # dictionaries, combining as we \n",
    "        # go along\n",
    "        for _cooccur in cooccurs:\n",
    "            \n",
    "            # read in the dictionary\n",
    "            _cooccur = ast.literal_eval(_cooccur)\n",
    "            \n",
    "            # loop through each other word\n",
    "            for other_word in _cooccur:\n",
    "            \n",
    "                # check to see if the cooccurring words\n",
    "                # are already in the coocurring dictionary,\n",
    "                # if not, then add them\n",
    "                if other_word not in cooccur:\n",
    "                    cooccur[other_word] = 0\n",
    "                \n",
    "                # increment the count for the word\n",
    "                cooccur[other_word] = \\\n",
    "                cooccur[other_word] + \\\n",
    "                _cooccur[other_word]\n",
    "        \n",
    "        # yield the word and its cooccruence\n",
    "        yield word, str(cooccur)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    stripeMaker.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done making test.txt\n"
     ]
    }
   ],
   "source": [
    "# make a larger file to test\n",
    "!head -1000 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt > test.txt\n",
    "print \"done making test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview the sripes file:\n",
      "additions\t{'sanctioned': 51}\r\n",
      "african\t{'campaigns': 50}\r\n",
      "age\t{'franklin': 77}\r\n",
      "albert\t{'belgium': 46}\r\n",
      "author\t{'sanctioned': 51}\r\n",
      "cells\t{'predominance': 44}\r\n",
      "clinical\t{'pathological': 257}\r\n",
      "coast\t{'peru': 206}\r\n",
      "complete\t{'catalog': 49}\r\n",
      "evidence\t{'admitting': 42}\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from stripeMaker import stripeMaker\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = stripeMaker(args=['test.txt','--file=stopwords.txt',\\\n",
    "                          '--file=top10000','--file=topVocab']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create a file to store the stripes for each\n",
    "    # word\n",
    "    with open('stripe_coocur','w') as myfile:\n",
    "    \n",
    "        # stream_output: get access to the output \n",
    "        for line in runner.stream_output():\n",
    "\n",
    "            # grab the key,value\n",
    "            word,coocur =  \\\n",
    "            mr_job.parse_output_line(line)\n",
    "            \n",
    "            # set the information we want to\n",
    "            # write\n",
    "            info = word+\"\\t\"+str(coocur)+\"\\n\"\n",
    "            \n",
    "            # write the word,density to an\n",
    "            # preliminary output file\n",
    "            myfile.write(info)\n",
    "            \n",
    "# preview the top of the preliminary file \n",
    "# that we created\n",
    "print \"Preview the sripes file:\"\n",
    "!head stripe_coocur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.153449.596917\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.153449.596917/output...\n",
      "\"additions\"\t\"{'sanctioned': 51}\"\n",
      "\"african\"\t\"{'campaigns': 50}\"\n",
      "\"age\"\t\"{'franklin': 77}\"\n",
      "\"albert\"\t\"{'belgium': 46}\"\n",
      "\"author\"\t\"{'sanctioned': 51}\"\n",
      "\"cells\"\t\"{'predominance': 44}\"\n",
      "\"clinical\"\t\"{'pathological': 257}\"\n",
      "\"coast\"\t\"{'peru': 206}\"\n",
      "\"complete\"\t\"{'catalog': 49}\"\n",
      "\"evidence\"\t\"{'admitting': 42}\"\n",
      "\"glance\"\t\"{'hasty': 79}\"\n",
      "\"hundred\"\t\"{'crowns': 54}\"\n",
      "\"manufacturing\"\t\"{'establishments': 44}\"\n",
      "\"narrative\"\t\"{'voyages': 532}\"\n",
      "\"opera\"\t\"{'extant': 47}\"\n",
      "\"response\"\t\"{'secreted': 51}\"\n",
      "\"roosevelt\"\t\"{'franklin': 77}\"\n",
      "\"section\"\t\"{'appointments': 79}\"\n",
      "\"shaft\"\t\"{'penetrating': 45}\"\n",
      "\"tendon\"\t\"{'attaches': 44}\"\n",
      "\"thousand\"\t\"{'crowns': 54}\"\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.153449.596917...\n"
     ]
    }
   ],
   "source": [
    "!python stripeMaker.py test.txt \\\n",
    "--file=stopwords.txt \\\n",
    "--file=top10000 \\\n",
    "--file=topVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stripe the entire file on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/no_script.Alex.20160619.151659.982911\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/no_script.Alex.20160619.151659.982911/files/...\n",
      "j-25Y2I2CYBIZC3\n"
     ]
    }
   ],
   "source": [
    "# create the cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-1 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./stopwords.txt to s3://aks-w261-hw5/stopwords.txt\n",
      "upload: ./top10000 to s3://aks-w261-hw5/top10000\n",
      "upload: ./topVocab to s3://aks-w261-hw5/topVocab\n"
     ]
    }
   ],
   "source": [
    "# add my files to s3\n",
    "!aws s3 cp stopwords.txt s3://aks-w261-hw5/stopwords.txt\n",
    "!aws s3 cp top10000 s3://aks-w261-hw5/top10000\n",
    "!aws s3 cp topVocab s3://aks-w261-hw5/topVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.153535.785909\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/stripeMaker.Alex.20160619.153535.785909/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-UAZ6NDT24I3W) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 11.0s\n",
      "  RUNNING for 45.1s\n",
      "     5.0% complete\n",
      "  RUNNING for 75.9s\n",
      "    58.2% complete\n",
      "  RUNNING for 107.6s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-UAZ6NDT24I3W on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-UAZ6NDT24I3W/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45912\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=253\n",
      "\t\tFILE: Number of bytes written=3677482\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1872\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=45912\n",
      "\t\tS3: Number of bytes written=31\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=24\n",
      "\t\tLaunched map tasks=24\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=802637280\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=384693120\n",
      "\t\tTotal time spent by all map tasks (ms)=557387\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25082415\n",
      "\t\tTotal time spent by all reduce tasks (ms)=133574\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12021660\n",
      "\t\tTotal vcore-seconds taken by all map tasks=557387\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=133574\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=34540\n",
      "\t\tCombine input records=1\n",
      "\t\tCombine output records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=9096\n",
      "\t\tInput split bytes=1872\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=31\n",
      "\t\tMap output materialized bytes=4257\n",
      "\t\tMap output records=1\n",
      "\t\tMerged Map outputs=264\n",
      "\t\tPhysical memory (bytes) snapshot=14723215360\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=4257\n",
      "\t\tShuffled Maps =264\n",
      "\t\tSpilled Records=2\n",
      "\t\tTotal committed heap usage (bytes)=16626221056\n",
      "\t\tVirtual memory (bytes) snapshot=82893606912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/stripeMaker.Alex.20160619.153535.785909/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.153535.785909...\n",
      "Killing our SSH tunnel (pid 17253)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python stripeMaker.py -r emr s3://aks-w261-hw5/test.txt \\\n",
    "    --file=s3://aks-w261-hw5/stopwords.txt \\\n",
    "    --file=s3://aks-w261-hw5/top10000 \\\n",
    "    --file=s3://aks-w261-hw5/topVocab \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the word count on the entire corpus using AWS\n",
    "Now that we've finished testing on a subset of the data, let's try it on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00000      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00001      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.154118.501113\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/stripeMaker.Alex.20160619.154118.501113/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-7HCF6V7VKGG2) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 24.5s\n",
      "   100.0% complete\n",
      "  RUNNING for 56.4s\n",
      "     5.0% complete\n",
      "  RUNNING for 88.2s\n",
      "     5.4% complete\n",
      "  RUNNING for 119.2s\n",
      "     5.9% complete\n",
      "  RUNNING for 150.6s\n",
      "     6.4% complete\n",
      "  RUNNING for 182.2s\n",
      "     6.9% complete\n",
      "  RUNNING for 213.2s\n",
      "     7.4% complete\n",
      "  RUNNING for 244.8s\n",
      "     8.3% complete\n",
      "  RUNNING for 275.8s\n",
      "     8.7% complete\n",
      "  RUNNING for 307.6s\n",
      "     9.5% complete\n",
      "  RUNNING for 338.5s\n",
      "    10.6% complete\n",
      "  RUNNING for 369.5s\n",
      "    11.1% complete\n",
      "  RUNNING for 400.7s\n",
      "    11.6% complete\n",
      "  RUNNING for 431.8s\n",
      "    12.1% complete\n",
      "  RUNNING for 462.5s\n",
      "    13.0% complete\n",
      "  RUNNING for 494.4s\n",
      "    13.6% complete\n",
      "  RUNNING for 526.0s\n",
      "    14.1% complete\n",
      "  RUNNING for 557.3s\n",
      "    14.7% complete\n",
      "  RUNNING for 588.8s\n",
      "    16.0% complete\n",
      "  RUNNING for 620.4s\n",
      "    17.1% complete\n",
      "  RUNNING for 651.6s\n",
      "    17.6% complete\n",
      "  RUNNING for 683.9s\n",
      "    18.1% complete\n",
      "  RUNNING for 714.9s\n",
      "    18.6% complete\n",
      "  RUNNING for 745.9s\n",
      "    19.5% complete\n",
      "  RUNNING for 777.4s\n",
      "    20.4% complete\n",
      "  RUNNING for 809.9s\n",
      "    21.3% complete\n",
      "  RUNNING for 840.6s\n",
      "    22.1% complete\n",
      "  RUNNING for 872.7s\n",
      "    22.9% complete\n",
      "  RUNNING for 903.7s\n",
      "    23.7% complete\n",
      "  RUNNING for 934.9s\n",
      "    24.4% complete\n",
      "  RUNNING for 965.8s\n",
      "    24.9% complete\n",
      "  RUNNING for 996.8s\n",
      "    25.5% complete\n",
      "  RUNNING for 1028.5s\n",
      "    26.8% complete\n",
      "  RUNNING for 1060.0s\n",
      "    27.4% complete\n",
      "  RUNNING for 1091.0s\n",
      "    27.8% complete\n",
      "  RUNNING for 1122.8s\n",
      "    28.9% complete\n",
      "  RUNNING for 1153.7s\n",
      "    29.7% complete\n",
      "  RUNNING for 1184.5s\n",
      "    30.8% complete\n",
      "  RUNNING for 1216.0s\n",
      "    31.2% complete\n",
      "  RUNNING for 1247.7s\n",
      "    31.5% complete\n",
      "  RUNNING for 1279.0s\n",
      "    32.4% complete\n",
      "  RUNNING for 1310.1s\n",
      "    32.7% complete\n",
      "  RUNNING for 1340.9s\n",
      "    33.1% complete\n",
      "  RUNNING for 1372.1s\n",
      "    33.5% complete\n",
      "  RUNNING for 1403.1s\n",
      "    34.9% complete\n",
      "  RUNNING for 1435.0s\n",
      "    35.2% complete\n",
      "  RUNNING for 1467.2s\n",
      "    35.6% complete\n",
      "  RUNNING for 1499.5s\n",
      "    36.2% complete\n",
      "  RUNNING for 1531.0s\n",
      "    36.7% complete\n",
      "  RUNNING for 1562.5s\n",
      "    37.1% complete\n",
      "  RUNNING for 1593.4s\n",
      "    37.5% complete\n",
      "  RUNNING for 1625.8s\n",
      "    38.2% complete\n",
      "  RUNNING for 1657.3s\n",
      "    39.0% complete\n",
      "  RUNNING for 1688.3s\n",
      "    39.6% complete\n",
      "  RUNNING for 1720.7s\n",
      "    40.1% complete\n",
      "  RUNNING for 1751.7s\n",
      "    40.7% complete\n",
      "  RUNNING for 1782.8s\n",
      "    41.1% complete\n",
      "  RUNNING for 1813.8s\n",
      "    41.6% complete\n",
      "  RUNNING for 1844.8s\n",
      "    42.3% complete\n",
      "  RUNNING for 1876.4s\n",
      "    42.7% complete\n",
      "  RUNNING for 1908.1s\n",
      "    43.0% complete\n",
      "  RUNNING for 1939.7s\n",
      "    44.1% complete\n",
      "  RUNNING for 1970.6s\n",
      "    44.8% complete\n",
      "  RUNNING for 2002.5s\n",
      "    45.1% complete\n",
      "  RUNNING for 2033.4s\n",
      "    45.5% complete\n",
      "  RUNNING for 2065.2s\n",
      "    46.2% complete\n",
      "  RUNNING for 2096.0s\n",
      "    46.7% complete\n",
      "  RUNNING for 2127.6s\n",
      "    47.1% complete\n",
      "  RUNNING for 2158.3s\n",
      "    47.4% complete\n",
      "  RUNNING for 2189.8s\n",
      "    48.2% complete\n",
      "  RUNNING for 2220.5s\n",
      "    49.2% complete\n",
      "  RUNNING for 2252.2s\n",
      "    49.5% complete\n",
      "  RUNNING for 2283.1s\n",
      "    49.9% complete\n",
      "  RUNNING for 2314.7s\n",
      "    50.6% complete\n",
      "  RUNNING for 2346.2s\n",
      "    51.0% complete\n",
      "  RUNNING for 2377.7s\n",
      "    51.5% complete\n",
      "  RUNNING for 2408.9s\n",
      "    52.1% complete\n",
      "  RUNNING for 2439.6s\n",
      "    52.5% complete\n",
      "  RUNNING for 2470.8s\n",
      "    53.1% complete\n",
      "  RUNNING for 2501.7s\n",
      "    53.9% complete\n",
      "  RUNNING for 2533.7s\n",
      "    56.4% complete\n",
      "  RUNNING for 2565.1s\n",
      "    56.6% complete\n",
      "  RUNNING for 2596.4s\n",
      "    56.8% complete\n",
      "  RUNNING for 2628.1s\n",
      "    58.4% complete\n",
      "  RUNNING for 2659.2s\n",
      "    59.9% complete\n",
      "  RUNNING for 2690.3s\n",
      "    60.0% complete\n",
      "  RUNNING for 2721.0s\n",
      "    82.7% complete\n",
      "  RUNNING for 2752.2s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-7HCF6V7VKGG2 on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-7HCF6V7VKGG2/syslog.2016-06-19-15\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-7HCF6V7VKGG2/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8054521\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12190479\n",
      "\t\tFILE: Number of bytes written=53710803\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=8054521\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=54455063040\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=29115541440\n",
      "\t\tTotal time spent by all map tasks (ms)=37816016\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1701720720\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10109563\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=909860670\n",
      "\t\tTotal vcore-seconds taken by all map tasks=37816016\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10109563\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=21108040\n",
      "\t\tCombine input records=1284587\n",
      "\t\tCombine output records=627653\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=66408\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=37265318\n",
      "\t\tMap output materialized bytes=20411684\n",
      "\t\tMap output records=1284587\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=114165063680\n",
      "\t\tReduce input groups=9582\n",
      "\t\tReduce input records=627653\n",
      "\t\tReduce output records=9582\n",
      "\t\tReduce shuffle bytes=20411684\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=1255306\n",
      "\t\tTotal committed heap usage (bytes)=123632353280\n",
      "\t\tVirtual memory (bytes) snapshot=409766621184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/stripeMaker.Alex.20160619.154118.501113/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker.Alex.20160619.154118.501113...\n",
      "Killing our SSH tunnel (pid 17274)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, still on the test data\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python stripeMaker.py -r emr s3://filtered-5grams/ \\\n",
    "    --file=s3://aks-w261-hw5/stopwords.txt \\\n",
    "    --file=s3://aks-w261-hw5/top10000 \\\n",
    "    --file=s3://aks-w261-hw5/topVocab \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes1: File exists\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_stripes1/part-00000\n",
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_stripes1/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00006 to s3://aks-w261-hw5/5_4_stripes1/part-00006\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00003 to s3://aks-w261-hw5/5_4_stripes1/part-00003\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00002 to s3://aks-w261-hw5/5_4_stripes1/part-00002\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00007 to s3://aks-w261-hw5/5_4_stripes1/part-00007\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00005 to s3://aks-w261-hw5/5_4_stripes1/part-00005\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00008 to s3://aks-w261-hw5/5_4_stripes1/part-00008\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00004 to s3://aks-w261-hw5/5_4_stripes1/part-00004\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00001 to s3://aks-w261-hw5/5_4_stripes1/part-00001\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00010 to s3://aks-w261-hw5/5_4_stripes1/part-00010\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00009 to s3://aks-w261-hw5/5_4_stripes1/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/_SUCCESS to 5_4_stripes1/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00000 to 5_4_stripes1/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00008 to 5_4_stripes1/part-00008\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00002 to 5_4_stripes1/part-00002\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00007 to 5_4_stripes1/part-00007\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00009 to 5_4_stripes1/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00003 to 5_4_stripes1/part-00003\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00006 to 5_4_stripes1/part-00006\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00001 to 5_4_stripes1/part-00001\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00004 to 5_4_stripes1/part-00004\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00010 to 5_4_stripes1/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_stripes1/part-00005 to 5_4_stripes1/part-00005\n",
      "_SUCCESS   part-00001 part-00003 part-00005 part-00007 part-00009\n",
      "part-00000 part-00002 part-00004 part-00006 part-00008 part-00010\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes1\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_stripes1/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_stripes1 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes1\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes1\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes1/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My stripe maker took 45 minutes running on a 4 node cluster, 1 master at m1.medium  and 3 cores at m3.xlarge. This is by far my most intensive MRJob.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binarize the stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting binarizeStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile binarizeStripes.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    " \n",
    "class binarizeStripes(MRJob):\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and binarizes each stripe\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # we use a try in case in case any \n",
    "        # data go written in a weird format\n",
    "        try:\n",
    "            # split the line by the tabs, set the\n",
    "            # word and its cooccurence words\n",
    "            line = line.split(\"\\t\")\n",
    "            word = line[0].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = line[1].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = ast.literal_eval(other_words)\n",
    "\n",
    "            # loop through each other word in \n",
    "            # the cooccurence matrix\n",
    "            for other_word in other_words:\n",
    "\n",
    "                # set the value equal to 1\n",
    "                other_words[other_word]=1\n",
    "\n",
    "            # yield the word with its cooccurence\n",
    "            # matrix\n",
    "            yield word, str(other_words)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    binarizeStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview the binarized stripes file:\n",
      "additions\t{'sanctioned': 1}\r\n",
      "african\t{'campaigns': 1}\r\n",
      "age\t{'franklin': 1}\r\n",
      "albert\t{'belgium': 1}\r\n",
      "author\t{'sanctioned': 1}\r\n",
      "cells\t{'predominance': 1}\r\n",
      "clinical\t{'pathological': 1}\r\n",
      "coast\t{'peru': 1}\r\n",
      "complete\t{'catalog': 1}\r\n",
      "evidence\t{'admitting': 1}\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from binarizeStripes import binarizeStripes\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = binarizeStripes(args=['stripe_coocur']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create a file to store the stripes for each\n",
    "    # word\n",
    "    with open('stripe_binarized','w') as myfile:\n",
    "    \n",
    "        # stream_output: get access to the output \n",
    "        for line in runner.stream_output():\n",
    "\n",
    "            # grab the key,value\n",
    "            word,coocur =  \\\n",
    "            mr_job.parse_output_line(line)\n",
    "            \n",
    "            # set the information we want to\n",
    "            # write\n",
    "            info = word+\"\\t\"+str(coocur)+\"\\n\"\n",
    "            \n",
    "            # write the word,density to an\n",
    "            # preliminary output file\n",
    "            myfile.write(info)\n",
    "            \n",
    "# preview the top of the preliminary file \n",
    "# that we created\n",
    "print \"Preview the binarized stripes file:\"\n",
    "!head stripe_binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the binarize on a subset of the data\n",
    "!head -500 5_4_stripes1/part-00004 > testing_stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/binarizeStripes.Alex.20160620.232750.602429\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/binarizeStripes.Alex.20160620.232750.602429/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/binarizeStripes.Alex.20160620.232750.602429...\n",
      "\"abandoned\"\t\"{'restless': 1, 'humiliation': 1, 'tents': 1, 'defenders': 1, 'pursuits': 1, 'athenians': 1, 'traitor': 1, 'nova': 1, 'habitation': 1, 'anarchy': 1, 'forts': 1, 'metaphysical': 1, 'damp': 1, 'enumeration': 1, 'speedily': 1, 'mate': 1, 'judicious': 1, 'cunning': 1, 'officially': 1, 'exhaustion': 1, 'jungle': 1, 'hue': 1, 'metropolis': 1, 'relic': 1, 'housed': 1, 'cart': 1, 'questioning': 1, 'domains': 1, 'travellers': 1, 'ordinances': 1, 'sinners': 1, 'licence': 1, 'lutheran': 1, 'forfeited': 1, 'arduous': 1, 'detroit': 1, 'fide': 1, 'natures': 1, 'barren': 1, 'conqueror': 1, 'intolerable': 1, 'sinner': 1}\"\n"
     ]
    }
   ],
   "source": [
    "# test the binarize function on this \n",
    "# larger output locally\n",
    "!python binarizeStripes.py testing_stripes > temp.txt\n",
    "!head -1 temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the binarize on the entire dataset in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/part-00000\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00011       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00012       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00001        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00013       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00014       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00015       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00016       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00017       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00018       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00019       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00020       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00021       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/binarizeStripes.Alex.20160619.170721.256749\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/binarizeStripes.Alex.20160619.170721.256749/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-E2I55CB9L0YY) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 22.2s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 55.1s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-E2I55CB9L0YY on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-E2I55CB9L0YY/syslog\n",
      "Counters: 35\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8268288\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3823264\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=3197418\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2883\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=31\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=8268288\n",
      "\t\tS3: Number of bytes written=3823264\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=31\n",
      "\t\tLaunched map tasks=31\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=935320320\n",
      "\t\tTotal time spent by all map tasks (ms)=649528\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29228760\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=649528\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=37140\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=8388\n",
      "\t\tInput split bytes=2883\n",
      "\t\tMap input records=9582\n",
      "\t\tMap output records=7251\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=8516853760\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=10136059904\n",
      "\t\tVirtual memory (bytes) snapshot=60884066304\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/binarizeStripes.Alex.20160619.170721.256749/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/binarizeStripes.Alex.20160619.170721.256749...\n",
      "Killing our SSH tunnel (pid 17576)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, on the stripes that we\n",
    "# just made\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python binarizeStripes.py -r emr s3://aks-w261-hw5/5_4_stripes1/ \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the binarize stripes locally and make a folder for them on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_bin_stripes1/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00003 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00003\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00006 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00006\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00001 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00001\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00007 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00007\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00005 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00005\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00002 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00002\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00004 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00004\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00009 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00009\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00008 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00008\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00010 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00010\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00000\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00013 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00013\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00011 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00011\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00012 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00012\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00016 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00016\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00014 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00014\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00017 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00017\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00020 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00020\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00018 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00018\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00024 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00024\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00022 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00022\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00019 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00019\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00025 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00025\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00015 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00015\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00023 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00023\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00021 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00021\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00027 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00027\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00026 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00026\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00030 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00030\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00028 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00028\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00029 to s3://aks-w261-hw5/5_4_bin_stripes1/part-00029\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/_SUCCESS to 5_4_bin_stripes1/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00000 to 5_4_bin_stripes1/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00002 to 5_4_bin_stripes1/part-00002\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00008 to 5_4_bin_stripes1/part-00008\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00006 to 5_4_bin_stripes1/part-00006\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00003 to 5_4_bin_stripes1/part-00003\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00004 to 5_4_bin_stripes1/part-00004\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00014 to 5_4_bin_stripes1/part-00014\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00010 to 5_4_bin_stripes1/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00015 to 5_4_bin_stripes1/part-00015\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00009 to 5_4_bin_stripes1/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00018 to 5_4_bin_stripes1/part-00018\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00005 to 5_4_bin_stripes1/part-00005\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00001 to 5_4_bin_stripes1/part-00001\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00013 to 5_4_bin_stripes1/part-00013\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00020 to 5_4_bin_stripes1/part-00020\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00019 to 5_4_bin_stripes1/part-00019\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00023 to 5_4_bin_stripes1/part-00023\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00024 to 5_4_bin_stripes1/part-00024\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00025 to 5_4_bin_stripes1/part-00025\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00011 to 5_4_bin_stripes1/part-00011\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00017 to 5_4_bin_stripes1/part-00017\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00027 to 5_4_bin_stripes1/part-00027\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00029 to 5_4_bin_stripes1/part-00029\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00030 to 5_4_bin_stripes1/part-00030\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00026 to 5_4_bin_stripes1/part-00026\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00007 to 5_4_bin_stripes1/part-00007\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00022 to 5_4_bin_stripes1/part-00022\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00028 to 5_4_bin_stripes1/part-00028\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00021 to 5_4_bin_stripes1/part-00021\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00016 to 5_4_bin_stripes1/part-00016\n",
      "download: s3://aks-w261-hw5/5_4_bin_stripes1/part-00012 to 5_4_bin_stripes1/part-00012\n",
      "_SUCCESS   part-00004 part-00009 part-00014 part-00019 part-00024 part-00029\n",
      "part-00000 part-00005 part-00010 part-00015 part-00020 part-00025 part-00030\n",
      "part-00001 part-00006 part-00011 part-00016 part-00021 part-00026\n",
      "part-00002 part-00007 part-00012 part-00017 part-00022 part-00027\n",
      "part-00003 part-00008 part-00013 part-00018 part-00023 part-00028\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_bin_stripes1\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_bin_stripes1/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_bin_stripes1 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_bin_stripes1\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_bin_stripes1\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_bin_stripes1/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the MRJob class to normalize the words in the stripes\n",
    "The normalization is the inverse of the square root of the sum of squares of the items in the vector. **Because we have already binarized, we can just take the inverse of the square root of the length of each cooccurence matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting normalizeStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile normalizeStripes.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import math\n",
    " \n",
    "class normalizeStripes(MRJob):\n",
    "    \"\"\"This class normalizes successfully if \n",
    "    we've already binarized\"\"\"\n",
    "    \n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and normalizes each stripe\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # we use a try in case in case any \n",
    "        # data go written in a weird format\n",
    "        try:\n",
    "            # split the line by the tabs, set the\n",
    "            # word and its cooccurence words\n",
    "            line = line.split(\"\\t\")\n",
    "            word = line[0].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = line[1].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = ast.literal_eval(other_words)\n",
    "        \n",
    "            # initalize a sum value\n",
    "            total = len(other_words)\n",
    "\n",
    "            # loop through each other word again,\n",
    "            # this time using the total to normalize\n",
    "            # remember that we normalize by the inverse\n",
    "            # of the square root of the sum of the squares\n",
    "            # of the values, but since we've binarized\n",
    "            # we can just take the inverse of the square\n",
    "            # root of the length of each coocurence matrix\n",
    "            for other_word in other_words:\n",
    "\n",
    "                # set the value equal to the value\n",
    "                # divided by the squareroot of the length\n",
    "                other_words[other_word] = 1.0/math.sqrt(total)\n",
    "        \n",
    "            # yield the word with its cooccurence\n",
    "            # matrix\n",
    "            yield word, str(other_words)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    normalizeStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview the normalized stripes file:\n",
      "additions\t{'sanctioned': 1.0}\r\n",
      "african\t{'campaigns': 1.0}\r\n",
      "age\t{'franklin': 1.0}\r\n",
      "albert\t{'belgium': 1.0}\r\n",
      "author\t{'sanctioned': 1.0}\r\n",
      "cells\t{'predominance': 1.0}\r\n",
      "clinical\t{'pathological': 1.0}\r\n",
      "coast\t{'peru': 1.0}\r\n",
      "complete\t{'catalog': 1.0}\r\n",
      "evidence\t{'admitting': 1.0}\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from normalizeStripes import normalizeStripes\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = normalizeStripes(args=['stripe_binarized']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create a file to store the stripes for each\n",
    "    # word\n",
    "    with open('stripe_normalized','w') as myfile:\n",
    "    \n",
    "        # stream_output: get access to the output \n",
    "        for line in runner.stream_output():\n",
    "\n",
    "            # grab the key,value\n",
    "            word,coocur =  \\\n",
    "            mr_job.parse_output_line(line)\n",
    "            \n",
    "            # set the information we want to\n",
    "            # write\n",
    "            info = word+\"\\t\"+str(coocur)+\"\\n\"\n",
    "            \n",
    "            # write the word,density to an\n",
    "            # preliminary output file\n",
    "            myfile.write(info)\n",
    "            \n",
    "# preview the top of the preliminary file \n",
    "# that we created\n",
    "print \"Preview the normalized stripes file:\"\n",
    "!head stripe_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\r\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/normalizeStripes.Alex.20160619.172649.464379\r\n",
      "Running step 1 of 1...\r\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/normalizeStripes.Alex.20160619.172649.464379/output...\r\n",
      "\"additions\"\t\"{'sanctioned': 1.0}\"\r\n",
      "\"african\"\t\"{'campaigns': 1.0}\"\r\n",
      "\"age\"\t\"{'franklin': 1.0}\"\r\n",
      "\"albert\"\t\"{'belgium': 1.0}\"\r\n",
      "\"author\"\t\"{'sanctioned': 1.0}\"\r\n",
      "\"cells\"\t\"{'predominance': 1.0}\"\r\n",
      "\"clinical\"\t\"{'pathological': 1.0}\"\r\n",
      "\"coast\"\t\"{'peru': 1.0}\"\r\n",
      "\"complete\"\t\"{'catalog': 1.0}\"\r\n",
      "\"evidence\"\t\"{'admitting': 1.0}\"\r\n",
      "\"glance\"\t\"{'hasty': 1.0}\"\r\n",
      "\"hundred\"\t\"{'crowns': 1.0}\"\r\n",
      "\"manufacturing\"\t\"{'establishments': 1.0}\"\r\n",
      "\"narrative\"\t\"{'voyages': 1.0}\"\r\n",
      "\"opera\"\t\"{'extant': 1.0}\"\r\n",
      "\"response\"\t\"{'secreted': 1.0}\"\r\n",
      "\"roosevelt\"\t\"{'franklin': 1.0}\"\r\n",
      "\"section\"\t\"{'appointments': 1.0}\"\r\n",
      "\"shaft\"\t\"{'penetrating': 1.0}\"\r\n",
      "\"tendon\"\t\"{'attaches': 1.0}\"\r\n",
      "\"thousand\"\t\"{'crowns': 1.0}\"\r\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/normalizeStripes.Alex.20160619.172649.464379...\r\n"
     ]
    }
   ],
   "source": [
    "# test on the command line\n",
    "!python normalizeStripes.py stripe_binarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run normalize on the stripes in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/part-00001\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00011       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00012       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00000       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00013        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00014       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00015       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00016       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00017       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00018       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00020       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00019       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00022       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00021       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/normalizeStripes.Alex.20160619.172656.662960\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/normalizeStripes.Alex.20160619.172656.662960/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-1YVWQU33RSGZ0) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 4.0s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 36.4s\n",
      "  RUNNING for 67.3s\n",
      "  RUNNING for 98.5s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1YVWQU33RSGZ0 on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1YVWQU33RSGZ0/syslog\n",
      "Counters: 35\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3876600\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8076942\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=3610423\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3395\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=3876600\n",
      "\t\tS3: Number of bytes written=8076942\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=35\n",
      "\t\tLaunched map tasks=35\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1058451840\n",
      "\t\tTotal time spent by all map tasks (ms)=735036\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33076620\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=735036\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=41100\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=9227\n",
      "\t\tInput split bytes=3395\n",
      "\t\tMap input records=7251\n",
      "\t\tMap output records=7251\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=9631232000\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=11705253888\n",
      "\t\tVirtual memory (bytes) snapshot=68643151872\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/normalizeStripes.Alex.20160619.172656.662960/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/normalizeStripes.Alex.20160619.172656.662960...\n",
      "Killing our SSH tunnel (pid 17693)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, on the stripes that we\n",
    "# just binarized\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python normalizeStripes.py -r emr s3://aks-w261-hw5/5_4_bin_stripes1/ \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the normalized stripes locally and make a folder for them on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_norm_stripes1/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00000\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00007 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00007\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00002 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00002\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00003 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00003\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00004 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00004\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00001 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00001\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00008 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00008\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00006 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00006\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00005 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00005\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00009 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00009\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00011 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00011\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00012 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00012\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00016 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00016\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00010 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00010\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00018 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00018\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00013 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00013\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00015 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00015\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00017 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00017\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00019 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00019\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00014 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00014\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00020 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00020\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00026 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00026\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00027 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00027\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00021 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00021\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00023 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00023\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00028 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00028\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00024 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00024\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00025 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00025\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00030 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00030\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00029 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00029\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00031 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00031\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00033 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00033\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00034 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00034\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00032 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00032\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00022 to s3://aks-w261-hw5/5_4_norm_stripes1/part-00022\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/_SUCCESS to 5_4_norm_stripes1/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00001 to 5_4_norm_stripes1/part-00001\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00002 to 5_4_norm_stripes1/part-00002\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00000 to 5_4_norm_stripes1/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00007 to 5_4_norm_stripes1/part-00007\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00009 to 5_4_norm_stripes1/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00005 to 5_4_norm_stripes1/part-00005\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00008 to 5_4_norm_stripes1/part-00008\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00010 to 5_4_norm_stripes1/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00003 to 5_4_norm_stripes1/part-00003\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00006 to 5_4_norm_stripes1/part-00006\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00012 to 5_4_norm_stripes1/part-00012\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00015 to 5_4_norm_stripes1/part-00015\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00011 to 5_4_norm_stripes1/part-00011\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00004 to 5_4_norm_stripes1/part-00004\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00017 to 5_4_norm_stripes1/part-00017\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00020 to 5_4_norm_stripes1/part-00020\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00013 to 5_4_norm_stripes1/part-00013\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00021 to 5_4_norm_stripes1/part-00021\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00014 to 5_4_norm_stripes1/part-00014\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00016 to 5_4_norm_stripes1/part-00016\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00029 to 5_4_norm_stripes1/part-00029\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00026 to 5_4_norm_stripes1/part-00026\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00025 to 5_4_norm_stripes1/part-00025\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00030 to 5_4_norm_stripes1/part-00030\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00031 to 5_4_norm_stripes1/part-00031\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00027 to 5_4_norm_stripes1/part-00027\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00024 to 5_4_norm_stripes1/part-00024\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00028 to 5_4_norm_stripes1/part-00028\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00022 to 5_4_norm_stripes1/part-00022\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00033 to 5_4_norm_stripes1/part-00033\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00034 to 5_4_norm_stripes1/part-00034\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00032 to 5_4_norm_stripes1/part-00032\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00023 to 5_4_norm_stripes1/part-00023\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00019 to 5_4_norm_stripes1/part-00019\n",
      "download: s3://aks-w261-hw5/5_4_norm_stripes1/part-00018 to 5_4_norm_stripes1/part-00018\n",
      "_SUCCESS   part-00005 part-00011 part-00017 part-00023 part-00029\n",
      "part-00000 part-00006 part-00012 part-00018 part-00024 part-00030\n",
      "part-00001 part-00007 part-00013 part-00019 part-00025 part-00031\n",
      "part-00002 part-00008 part-00014 part-00020 part-00026 part-00032\n",
      "part-00003 part-00009 part-00015 part-00021 part-00027 part-00033\n",
      "part-00004 part-00010 part-00016 part-00022 part-00028 part-00034\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_norm_stripes1\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_norm_stripes1/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_norm_stripes1 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_norm_stripes1\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_norm_stripes1\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_norm_stripes1/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRJob Class to create the inverted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertIndex.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import math\n",
    " \n",
    "class invertIndex(MRJob):\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       combiner=self.reducer,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and inverts each stripe\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # we use a try in case in case any \n",
    "        # data go written in a weird format\n",
    "        try:\n",
    "            # split the line by the tabs, set the\n",
    "            # word and its cooccurence words\n",
    "            line = line.split(\"\\t\")\n",
    "            word = line[0].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = line[1].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = ast.literal_eval(other_words)\n",
    "\n",
    "            # loop through each of the other words\n",
    "            for other_word in other_words:\n",
    "\n",
    "                # create a dictionary for the \n",
    "                # invert, holds the word and \n",
    "                # inverted value\n",
    "                inverted_value = \\\n",
    "                other_words[other_word]\n",
    "                invert = {word:inverted_value}\n",
    "\n",
    "                # yield the inverted word with \n",
    "                # its inverted value\n",
    "                yield other_word, str(invert)\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # our reducer combines the inverted values\n",
    "    # for multiple words\n",
    "    def reducer(self, word, cooccurs):\n",
    "        \n",
    "        # create a dictionary to hold the \n",
    "        # co-occurence counts for each word\n",
    "        cooccur = {}\n",
    "        \n",
    "        # loop through the coocurrences \n",
    "        # dictionaries, combining as we \n",
    "        # go along\n",
    "        for _cooccur in cooccurs:\n",
    "            \n",
    "            # read in the dictionary\n",
    "            _cooccur = ast.literal_eval(_cooccur)\n",
    "            \n",
    "            # loop through each other word\n",
    "            for other_word in _cooccur:\n",
    "            \n",
    "                cooccur[other_word] = \\\n",
    "                _cooccur[other_word]\n",
    "        \n",
    "        # yield the word and its cooccruence\n",
    "        yield word, str(cooccur)      \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    invertIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview the inverted stripes file:\n",
      "addressed\t{'sermon': 1.0}\r\n",
      "aerial\t{'navigation': 0.7071067811865475, 'history': 0.30151134457776363}\r\n",
      "america's\t{'guide': 0.4472135954999579, 'censorship': 0.7071067811865475}\r\n",
      "american\t{'postwar': 0.7071067811865475, 'narrative': 0.5773502691896258, 'history': 0.30151134457776363}\r\n",
      "analysis\t{'physiological': 1.0}\r\n",
      "apology\t{'commentary': 1.0}\r\n",
      "arithmetic\t{'key': 0.5773502691896258}\r\n",
      "aspiration\t{'marrow': 0.7071067811865475, 'bone': 0.7071067811865475}\r\n",
      "assessment\t{'methodology': 1.0}\r\n",
      "based\t{'method': 1.0}\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from invertIndex import invertIndex\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = invertIndex(args=['stripe_normalized']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create a file to store the stripes for each\n",
    "    # word\n",
    "    with open('stripe_invert','w') as myfile:\n",
    "    \n",
    "        # stream_output: get access to the output \n",
    "        for line in runner.stream_output():\n",
    "\n",
    "            # grab the key,value\n",
    "            word,coocur =  \\\n",
    "            mr_job.parse_output_line(line)\n",
    "            \n",
    "            # set the information we want to\n",
    "            # write\n",
    "            info = word+\"\\t\"+str(coocur)+\"\\n\"\n",
    "            \n",
    "            # write the word,density to an\n",
    "            # preliminary output file\n",
    "            myfile.write(info)\n",
    "            \n",
    "# preview the top of the preliminary file \n",
    "# that we created\n",
    "print \"Preview the inverted stripes file:\"\n",
    "!head stripe_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\r\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invertIndex.Alex.20160619.173352.648877\r\n",
      "Running step 1 of 1...\r\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invertIndex.Alex.20160619.173352.648877/output...\r\n",
      "\"admitting\"\t\"{'evidence': 1.0}\"\r\n",
      "\"appointments\"\t\"{'section': 1.0}\"\r\n",
      "\"attaches\"\t\"{'tendon': 1.0}\"\r\n",
      "\"belgium\"\t\"{'albert': 1.0}\"\r\n",
      "\"campaigns\"\t\"{'african': 1.0}\"\r\n",
      "\"catalog\"\t\"{'complete': 1.0}\"\r\n",
      "\"crowns\"\t\"{'thousand': 1.0, 'hundred': 1.0}\"\r\n",
      "\"establishments\"\t\"{'manufacturing': 1.0}\"\r\n",
      "\"extant\"\t\"{'opera': 1.0}\"\r\n",
      "\"franklin\"\t\"{'age': 1.0, 'roosevelt': 1.0}\"\r\n",
      "\"hasty\"\t\"{'glance': 1.0}\"\r\n",
      "\"pathological\"\t\"{'clinical': 1.0}\"\r\n",
      "\"penetrating\"\t\"{'shaft': 1.0}\"\r\n",
      "\"peru\"\t\"{'coast': 1.0}\"\r\n",
      "\"predominance\"\t\"{'cells': 1.0}\"\r\n",
      "\"sanctioned\"\t\"{'additions': 1.0, 'author': 1.0}\"\r\n",
      "\"secreted\"\t\"{'response': 1.0}\"\r\n",
      "\"voyages\"\t\"{'narrative': 1.0}\"\r\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invertIndex.Alex.20160619.173352.648877...\r\n"
     ]
    }
   ],
   "source": [
    "# test on the commandn line\n",
    "!python invertIndex.py stripe_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inversion in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00001       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00000       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00011        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00012        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00015        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00017        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00014        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00013        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00016        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00018        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00020        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00019        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00021        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00022        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00023        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00024        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00026        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00025       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00027       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00028       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00030       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00029       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00031       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00032       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00033       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00034       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invertIndex.Alex.20160619.173519.902821\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/invertIndex.Alex.20160619.173519.902821/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-1WG2HXZ285DRR) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 24.9s\n",
      "     0.0% complete\n",
      "  RUNNING for 58.0s\n",
      "    15.3% complete\n",
      "  RUNNING for 89.4s\n",
      "    41.6% complete\n",
      "  RUNNING for 121.0s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1WG2HXZ285DRR on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1WG2HXZ285DRR/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8076942\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7811660\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3628172\n",
      "\t\tFILE: Number of bytes written=10867590\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3430\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=8076942\n",
      "\t\tS3: Number of bytes written=7811660\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=36\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=36\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1418060160\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=596168640\n",
      "\t\tTotal time spent by all map tasks (ms)=984764\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44314380\n",
      "\t\tTotal time spent by all reduce tasks (ms)=207003\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18630270\n",
      "\t\tTotal vcore-seconds taken by all map tasks=984764\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=207003\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=105330\n",
      "\t\tCombine input records=247430\n",
      "\t\tCombine output records=31787\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=11361\n",
      "\t\tInput split bytes=3430\n",
      "\t\tMap input records=7251\n",
      "\t\tMap output bytes=11274980\n",
      "\t\tMap output materialized bytes=2452420\n",
      "\t\tMap output records=247430\n",
      "\t\tMerged Map outputs=385\n",
      "\t\tPhysical memory (bytes) snapshot=20170137600\n",
      "\t\tReduce input groups=991\n",
      "\t\tReduce input records=31787\n",
      "\t\tReduce output records=991\n",
      "\t\tReduce shuffle bytes=2452420\n",
      "\t\tShuffled Maps =385\n",
      "\t\tSpilled Records=63574\n",
      "\t\tTotal committed heap usage (bytes)=22599958528\n",
      "\t\tVirtual memory (bytes) snapshot=104544759808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/invertIndex.Alex.20160619.173519.902821/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invertIndex.Alex.20160619.173519.902821...\n",
      "Killing our SSH tunnel (pid 17729)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, on the stripes that we\n",
    "# just binarized\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python invertIndex.py -r emr s3://aks-w261-hw5/5_4_norm_stripes1/ \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the inverted index locally and make a folder on S3 for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_invert1/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00003 to s3://aks-w261-hw5/5_4_invert1/part-00003\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00008 to s3://aks-w261-hw5/5_4_invert1/part-00008\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00001 to s3://aks-w261-hw5/5_4_invert1/part-00001\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00004 to s3://aks-w261-hw5/5_4_invert1/part-00004\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00007 to s3://aks-w261-hw5/5_4_invert1/part-00007\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00002 to s3://aks-w261-hw5/5_4_invert1/part-00002\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00006 to s3://aks-w261-hw5/5_4_invert1/part-00006\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_invert1/part-00000\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00009 to s3://aks-w261-hw5/5_4_invert1/part-00009\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00010 to s3://aks-w261-hw5/5_4_invert1/part-00010\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00005 to s3://aks-w261-hw5/5_4_invert1/part-00005\n",
      "download: s3://aks-w261-hw5/5_4_invert1/_SUCCESS to 5_4_invert1/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00001 to 5_4_invert1/part-00001\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00004 to 5_4_invert1/part-00004\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00007 to 5_4_invert1/part-00007\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00010 to 5_4_invert1/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00005 to 5_4_invert1/part-00005\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00003 to 5_4_invert1/part-00003\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00000 to 5_4_invert1/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00002 to 5_4_invert1/part-00002\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00006 to 5_4_invert1/part-00006\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00009 to 5_4_invert1/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_invert1/part-00008 to 5_4_invert1/part-00008\n",
      "_SUCCESS   part-00001 part-00003 part-00005 part-00007 part-00009\n",
      "part-00000 part-00002 part-00004 part-00006 part-00008 part-00010\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert1\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_invert1/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_invert1 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert1\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert1\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert1/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRJob to calculate the cosine similarity between terms\n",
    "To calculate the cosine similarity of two stripe documents, say A and C, compute partial similarities using dot products and aggregating partial similarities to get overall cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cosineSim.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cosineSim.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import math\n",
    "from mrjob.protocol import RawProtocol\n",
    " \n",
    "class cosineSim(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        \n",
    "        # set how we want to sort, we use a single reducer\n",
    "        # only so that we can do a total sort, we use multiple\n",
    "        # reducers for all other steps\n",
    "        JOBCONF = {        \n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer),\\\n",
    "               MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=None,\\\n",
    "                       reducer=self.reducer_final)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and outputs the partial aggregation\n",
    "    # for each pair combination\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        \n",
    "        # we use a try in case in case any \n",
    "        # data go written in a weird format\n",
    "        try:\n",
    "            # split the line by the tabs, set the\n",
    "            # word and its cooccurence words\n",
    "            line = line.split(\"\\t\")\n",
    "            key = line[0].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            words = line[1].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            words = ast.literal_eval(words)\n",
    "\n",
    "            # loop through each of the words\n",
    "            for word in words:\n",
    "\n",
    "                # loop through the rest of the \n",
    "                # rest of the words in words\n",
    "                for other_word in words:\n",
    "\n",
    "                    # as long as it isn't this \n",
    "                    # word\n",
    "                    if word != other_word:\n",
    "\n",
    "                        # get the two word values\n",
    "                        word_val = words[word]\n",
    "                        other_word_val = words[other_word]\n",
    "\n",
    "                        # multiple the two together\n",
    "                        # dot product, for partial \n",
    "                        # similarity\n",
    "                        partial = \\\n",
    "                        word_val * other_word_val\n",
    "\n",
    "                        # set the pair\n",
    "                        pair = word,other_word\n",
    "\n",
    "                        # yeild the pair and its\n",
    "                        # partial similarity\n",
    "                        yield str(pair),str(partial)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    # our reducer combines the partial \n",
    "    # similarities to create an aggregated\n",
    "    # similarity\n",
    "    def reducer(self, pair, similarities):\n",
    "        \n",
    "        # create a sum variable\n",
    "        sum_sim = 0.0\n",
    "        \n",
    "        # loop through the similarities\n",
    "        for similarity in similarities:\n",
    "            sum_sim = sum_sim + float(similarity)\n",
    "        \n",
    "        # we want to make sure that the count \n",
    "        # string is at least 15 characters, if\n",
    "        # its not, we add trailing zeros. this\n",
    "        # helps us sort\n",
    "        extend = \"0\"\n",
    "        \n",
    "        # convert density to a string\n",
    "        sum_sim = str(sum_sim)\n",
    "        \n",
    "        # while the count lenght is less than 15\n",
    "        while len(sum_sim) < 15:\n",
    "            sum_sim = sum_sim + extend\n",
    "        \n",
    "        # yield the pair and its partial\n",
    "        # similarity\n",
    "        yield sum_sim, pair\n",
    "        \n",
    "    # our reducer final simply outputs the sorted\n",
    "    # list that hadoop has conviently shuffled and\n",
    "    # sorted for us\n",
    "    def reducer_final(self, similarity, pairs):\n",
    "        for pair in pairs:\n",
    "            yield pair,similarity\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    cosineSim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview the cosine similarities file:\n",
      "['aerial', 'american']\t0.408248290464\r\n",
      "['aerial', 'eurobond']\t0.707106781187\r\n",
      "['aerial', 'history']\t0.213200716356\r\n",
      "['aerial', 'modern']\t0.408248290464\r\n",
      "['aerial', 'navigation']\t0.5\r\n",
      "['aerial', 'postwar']\t0.5\r\n",
      "['aerial', 'railroads']\t0.408248290464\r\n",
      "['aerial', 'southeast']\t0.5\r\n",
      "['aerial', 'travel']\t0.707106781187\r\n",
      "['aerial', 'united']\t0.707106781187\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJob that we created\n",
    "from cosineSim import cosineSim\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = cosineSim(args=['stripe_invert']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    # create a file to store the stripes for each\n",
    "    # word\n",
    "    with open('cosineSimilarities','w') as myfile:\n",
    "    \n",
    "        # stream_output: get access to the output \n",
    "        for line in runner.stream_output():\n",
    "\n",
    "            # grab the key,value\n",
    "            pair,similarity =  \\\n",
    "            mr_job.parse_output_line(line)\n",
    "            \n",
    "            # set the information we want to\n",
    "            # write\n",
    "            info = str(pair)+\"\\t\"+str(similarity)+\"\\n\"\n",
    "            \n",
    "            # write the word,density to an\n",
    "            # preliminary output file\n",
    "            myfile.write(info)\n",
    "            \n",
    "# preview the top of the preliminary file \n",
    "# that we created\n",
    "print \"Preview the cosine similarities file:\"\n",
    "!head cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/cosineSim.Alex.20160620.232846.271681\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/cosineSim.Alex.20160620.232846.271681/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/cosineSim.Alex.20160620.232846.271681...\n",
      "('life', 'study')\t0.0912870929175\n",
      "('study', 'life')\t0.0912870929175\n",
      "('key', 'study')\t0.1290994448740\n",
      "('study', 'key')\t0.1290994448740\n",
      "('history', 'practical')\t0.1507556722890\n",
      "('practical', 'history')\t0.1507556722890\n",
      "('bibliographical', 'study')\t0.1581138830080\n",
      "('communities', 'study')\t0.1581138830080\n",
      "('comparative', 'study')\t0.1581138830080\n",
      "('conceptual', 'study')\t0.1581138830080\n"
     ]
    }
   ],
   "source": [
    "# run on the command line\n",
    "!python cosineSim.py stripe_invert > temp.txt\n",
    "!head temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the cosine similarity across the entire dataset on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00001      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00000       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/cosineSim.Alex.20160619.180114.892883\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/cosineSim.Alex.20160619.180114.892883/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Waiting for step 1 of 2 (s-1D01BTQMXLWND) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 5.9s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 38.0s\n",
      "  RUNNING for 68.9s\n",
      "  RUNNING for 100.7s\n",
      "  RUNNING for 131.4s\n",
      "  RUNNING for 162.7s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1D01BTQMXLWND on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1D01BTQMXLWND/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7959349\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=463819278\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=239151381\n",
      "\t\tFILE: Number of bytes written=628078955\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2576\n",
      "\t\tHDFS: Number of bytes written=463819278\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=89\n",
      "\t\tHDFS: Number of write operations=22\n",
      "\t\tS3: Number of bytes read=7959349\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=28\n",
      "\t\tLaunched map tasks=28\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1830019680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1473912000\n",
      "\t\tTotal time spent by all map tasks (ms)=1270847\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=57188115\n",
      "\t\tTotal time spent by all reduce tasks (ms)=511775\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=46059750\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1270847\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=511775\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=860340\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=13612\n",
      "\t\tInput split bytes=2576\n",
      "\t\tMap input records=991\n",
      "\t\tMap output bytes=774230900\n",
      "\t\tMap output materialized bytes=384863023\n",
      "\t\tMap output records=19049886\n",
      "\t\tMerged Map outputs=308\n",
      "\t\tPhysical memory (bytes) snapshot=20778643456\n",
      "\t\tReduce input groups=11638896\n",
      "\t\tReduce input records=19049886\n",
      "\t\tReduce output records=11638896\n",
      "\t\tReduce shuffle bytes=384863023\n",
      "\t\tShuffled Maps =308\n",
      "\t\tSpilled Records=38099772\n",
      "\t\tTotal committed heap usage (bytes)=23703584768\n",
      "\t\tVirtual memory (bytes) snapshot=90721210368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-2GLCX0UOUTSOB) to complete...\n",
      "  RUNNING for 75.9s\n",
      "  RUNNING for 106.9s\n",
      "  RUNNING for 138.1s\n",
      "  RUNNING for 168.6s\n",
      "  RUNNING for 199.9s\n",
      "  RUNNING for 230.5s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2GLCX0UOUTSOB on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2GLCX0UOUTSOB/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=463976259\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=475458174\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149205149\n",
      "\t\tFILE: Number of bytes written=340654144\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=463981242\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=66\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=475458174\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=33\n",
      "\t\tLaunched map tasks=33\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1552127040\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=452689920\n",
      "\t\tTotal time spent by all map tasks (ms)=1077866\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=48503970\n",
      "\t\tTotal time spent by all reduce tasks (ms)=157184\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14146560\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1077866\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=157184\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=458010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=19089\n",
      "\t\tInput split bytes=4983\n",
      "\t\tMap input records=11638896\n",
      "\t\tMap output bytes=475458174\n",
      "\t\tMap output materialized bytes=187900821\n",
      "\t\tMap output records=11638896\n",
      "\t\tMerged Map outputs=33\n",
      "\t\tPhysical memory (bytes) snapshot=22045343744\n",
      "\t\tReduce input groups=11638896\n",
      "\t\tReduce input records=11638896\n",
      "\t\tReduce output records=11638896\n",
      "\t\tReduce shuffle bytes=187900821\n",
      "\t\tShuffled Maps =33\n",
      "\t\tSpilled Records=23277792\n",
      "\t\tTotal committed heap usage (bytes)=25322586112\n",
      "\t\tVirtual memory (bytes) snapshot=68117573632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/cosineSim.Alex.20160619.180114.892883/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/cosineSim.Alex.20160619.180114.892883...\n",
      "Killing our SSH tunnel (pid 17847)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up, on the stripes that we\n",
    "# just binarized\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python cosineSim.py -r emr s3://aks-w261-hw5/5_4_invert1/ \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the similarity files locally and create a bucket for them in the cloud\n",
    "This bucket can be accessed here: s3://aks-w261-hw5/5_4_cosineSim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_cosineSim/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_cosineSim/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_cosineSim/_SUCCESS to 5_4_cosineSim/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_cosineSim/part-00000 to 5_4_cosineSim/part-00000\n",
      "_SUCCESS   part-00000\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_cosineSim\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_cosineSim/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_cosineSim /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_cosineSim\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_cosineSim\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_cosineSim/_SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the top similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicated', 'substantive')\t\t0.7071067811870\r\n",
      "('substantive', 'predicated')\t\t0.7071067811870\r\n",
      "('ammonia', 'hydrochloric')\t\t0.5962847940000\r\n",
      "('hydrochloric', 'ammonia')\t\t0.5962847940000\r\n",
      "('oxidation', 'solubility')\t\t0.5685352436150\r\n",
      "('solubility', 'oxidation')\t\t0.5685352436150\r\n",
      "('flexor', 'tendon')\t\t0.5590169943750\r\n",
      "('tendon', 'flexor')\t\t0.5590169943750\r\n",
      "('hydrochloric', 'hydroxide')\t\t0.5477225575050\r\n",
      "('hydroxide', 'hydrochloric')\t\t0.5477225575050\r\n"
     ]
    }
   ],
   "source": [
    "!head ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_cosineSim/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's move on to Jaccard Similarities\n",
    "We've completed cosine similarities. Let's know calculate the Jaccard similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the MRJob class that indexes the data for Jaccard Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing stripeMaker_jac.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripeMaker_jac.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    " \n",
    "class stripeMaker_jac(MRJob):\n",
    "    \n",
    "    # we initalize arrays to hold the words we care\n",
    "    # about. stop_words holds stop words that we wish\n",
    "    # to exclude. vocab holds the vocabularly that we\n",
    "    # care about each word forming stripes with. \n",
    "    # words_interest holds the words we want to form\n",
    "    # stripes for\n",
    "    stop_words = []\n",
    "    vocab = []\n",
    "    words_interest = []\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       combiner=self.reducer,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # set the stop words that we want to compare\n",
    "    # all the words to\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # read in the file of stop words\n",
    "        with open('stopwords.txt','r') as myfile:\n",
    "            \n",
    "            # read in all the lines\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # if the line is not blank, append\n",
    "                # it to our list of stop words\n",
    "                if line.strip() != \"\":\n",
    "                    self.stop_words.append(line.strip())\n",
    "        \n",
    "        # read in the file of vocab\n",
    "        with open('topVocab','r') as myfile:\n",
    "            \n",
    "            # read all the lines\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line and read\n",
    "                # in the vocab\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[1].strip()\n",
    "                \n",
    "                # append the word to the list\n",
    "                self.vocab.append(word)\n",
    "                \n",
    "        # read in the file of words of interest\n",
    "        with open('top10000','r') as myfile:\n",
    "            \n",
    "            # read all the lines\n",
    "            for line in myfile.readlines():\n",
    "                \n",
    "                # split the line and read\n",
    "                # in the word\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[1].strip()\n",
    "                \n",
    "                # append the word to the list\n",
    "                self.words_interest.append(word)\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and splits out the count for each word\n",
    "    # and the associated document\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # ngram and lower-case it\n",
    "        line = line.split(\"\\t\")\n",
    "        ngram = line[0].lower()\n",
    "        count = int(line[1])\n",
    "\n",
    "        # set the words\n",
    "        words = ngram.split()\n",
    "        \n",
    "        # loop through each word in the ngram\n",
    "        for word in words:\n",
    "\n",
    "            # check and make sure it's in our \n",
    "            # words of interests\n",
    "            if word in self.words_interest:\n",
    "            \n",
    "                # make sure it's not in our stop \n",
    "                # words\n",
    "                if word not in self.stop_words:\n",
    "\n",
    "                    # create a dictionary to hold the\n",
    "                    # word\n",
    "                    word_coocur = {}\n",
    "\n",
    "                    # loop through all the other words\n",
    "                    for other_word in words:\n",
    "\n",
    "                        # make sure the other_word is in\n",
    "                        # our vocab\n",
    "                        if other_word in self.vocab:\n",
    "                        \n",
    "                            # make sure word not in our stop\n",
    "                            # words\n",
    "                            if other_word not in self.stop_words:\n",
    "\n",
    "                                # if it's not the same word\n",
    "                                if word != other_word:\n",
    "\n",
    "\n",
    "                                    # if we haven't aready found\n",
    "                                    # this other word for this word\n",
    "                                    if other_word not in word_coocur:\n",
    "                                        word_coocur[other_word] = count\n",
    "                                    else:\n",
    "                                        word_coocur[other_word] = \\\n",
    "                                        word_coocur[other_word] + count\n",
    "                                        \n",
    "                        # create an entry for the word itself, it's\n",
    "                        # cardinality\n",
    "                        word_star = \"*\"+word\n",
    "                        word_coocur[word_star] = count\n",
    "\n",
    "                    # yield word and its associated\n",
    "                    # co-occurences if we've actually \n",
    "                    # add a co-occurence that's not a stop word\n",
    "                    if len(word_coocur) > 0:\n",
    "                        yield word, str(word_coocur)\n",
    "        \n",
    "    \n",
    "    # our reducer combines the counts for all\n",
    "    # words\n",
    "    def reducer(self, word, cooccurs):\n",
    "        \n",
    "        # create a dictionary to hold the \n",
    "        # co-occurence counts for each word\n",
    "        cooccur = {}\n",
    "        \n",
    "        # loop through the coocurrences \n",
    "        # dictionaries, combining as we \n",
    "        # go along\n",
    "        for _cooccur in cooccurs:\n",
    "            \n",
    "            # read in the dictionary\n",
    "            _cooccur = ast.literal_eval(_cooccur)\n",
    "            \n",
    "            # loop through each other word\n",
    "            for other_word in _cooccur:\n",
    "            \n",
    "                # check to see if the cooccurring words\n",
    "                # are already in the coocurring dictionary,\n",
    "                # if not, then add them\n",
    "                if other_word not in cooccur:\n",
    "                    cooccur[other_word] = 0\n",
    "                \n",
    "                # increment the count for the word\n",
    "                cooccur[other_word] = \\\n",
    "                cooccur[other_word] + \\\n",
    "                _cooccur[other_word]\n",
    "        \n",
    "        # yield the word and its cooccruence\n",
    "        yield word, str(cooccur)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    stripeMaker_jac.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on a sample file, first locally and then on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160620.233130.110608\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160620.233130.110608/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160620.233130.110608...\n",
      "\"ab\"\t\"{'*ab': 125}\"\n",
      "\"abandonment\"\t\"{'*abandonment': 71}\"\n",
      "\"ability\"\t\"{'*ability': 41}\"\n",
      "\"able\"\t\"{'*able': 105}\"\n",
      "\"abnormal\"\t\"{'*abnormal': 53}\"\n",
      "\"abolition\"\t\"{'*abolition': 62}\"\n",
      "\"abraham\"\t\"{'*abraham': 109}\"\n",
      "\"absorption\"\t\"{'*absorption': 148}\"\n",
      "\"abstract\"\t\"{'*abstract': 77}\"\n",
      "\"academic\"\t\"{'*academic': 54}\"\n"
     ]
    }
   ],
   "source": [
    "# test locally using the command line\n",
    "!python stripeMaker_jac.py test.txt \\\n",
    "--file=stopwords.txt \\\n",
    "--file=top10000 \\\n",
    "--file=topVocab \\\n",
    "> temp.txt\n",
    "!head temp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00000     \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160619.182212.879141\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/stripeMaker_jac.Alex.20160619.182212.879141/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-1PLHFL5BCDD10) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 21.9s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 54.3s\n",
      "  RUNNING for 84.9s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1PLHFL5BCDD10 on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1PLHFL5BCDD10/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45912\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4412\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3905\n",
      "\t\tFILE: Number of bytes written=3687156\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1872\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=45912\n",
      "\t\tS3: Number of bytes written=4412\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=25\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=25\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=838183680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=370272960\n",
      "\t\tTotal time spent by all map tasks (ms)=582072\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26193240\n",
      "\t\tTotal time spent by all reduce tasks (ms)=128567\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11571030\n",
      "\t\tTotal vcore-seconds taken by all map tasks=582072\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=128567\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=41260\n",
      "\t\tCombine input records=193\n",
      "\t\tCombine output records=173\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=8382\n",
      "\t\tInput split bytes=1872\n",
      "\t\tMap input records=100\n",
      "\t\tMap output bytes=5693\n",
      "\t\tMap output materialized bytes=8991\n",
      "\t\tMap output records=193\n",
      "\t\tMerged Map outputs=264\n",
      "\t\tPhysical memory (bytes) snapshot=14656004096\n",
      "\t\tReduce input groups=147\n",
      "\t\tReduce input records=173\n",
      "\t\tReduce output records=147\n",
      "\t\tReduce shuffle bytes=8991\n",
      "\t\tShuffled Maps =264\n",
      "\t\tSpilled Records=346\n",
      "\t\tTotal committed heap usage (bytes)=16625696768\n",
      "\t\tVirtual memory (bytes) snapshot=82953224192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/stripeMaker_jac.Alex.20160619.182212.879141/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160619.182212.879141...\n",
      "Killing our SSH tunnel (pid 17909)\n"
     ]
    }
   ],
   "source": [
    "# test on the cloud\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python stripeMaker_jac.py -r emr s3://aks-w261-hw5/test.txt \\\n",
    "    --file=s3://aks-w261-hw5/stopwords.txt \\\n",
    "    --file=s3://aks-w261-hw5/top10000 \\\n",
    "    --file=s3://aks-w261-hw5/topVocab \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the index maker on the entire corpus on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/part-00000\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00001      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002      \n",
      "delete: s3://aks-w261-hw5/out_5_4/_SUCCESS        \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160619.182649.857818\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/stripeMaker_jac.Alex.20160619.182649.857818/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-3SQKHHVKFXV9M) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 4.7s\n",
      "   100.0% complete\n",
      "  RUNNING for 37.1s\n",
      "     0.0% complete\n",
      "  RUNNING for 68.1s\n",
      "     5.0% complete\n",
      "  RUNNING for 99.5s\n",
      "     5.5% complete\n",
      "  RUNNING for 130.3s\n",
      "     5.9% complete\n",
      "  RUNNING for 162.3s\n",
      "     6.4% complete\n",
      "  RUNNING for 193.4s\n",
      "     6.9% complete\n",
      "  RUNNING for 224.6s\n",
      "     7.3% complete\n",
      "  RUNNING for 256.0s\n",
      "     7.8% complete\n",
      "  RUNNING for 287.8s\n",
      "     8.1% complete\n",
      "  RUNNING for 319.3s\n",
      "     8.9% complete\n",
      "  RUNNING for 350.2s\n",
      "     9.1% complete\n",
      "  RUNNING for 381.1s\n",
      "     9.3% complete\n",
      "  RUNNING for 411.9s\n",
      "    10.6% complete\n",
      "  RUNNING for 443.0s\n",
      "    11.2% complete\n",
      "  RUNNING for 474.7s\n",
      "    11.7% complete\n",
      "  RUNNING for 505.9s\n",
      "    12.1% complete\n",
      "  RUNNING for 537.9s\n",
      "    12.5% complete\n",
      "  RUNNING for 568.8s\n",
      "    13.0% complete\n",
      "  RUNNING for 600.1s\n",
      "    13.7% complete\n",
      "  RUNNING for 631.4s\n",
      "    14.0% complete\n",
      "  RUNNING for 662.2s\n",
      "    14.8% complete\n",
      "  RUNNING for 693.7s\n",
      "    15.2% complete\n",
      "  RUNNING for 725.2s\n",
      "    15.5% complete\n",
      "  RUNNING for 756.8s\n",
      "    16.4% complete\n",
      "  RUNNING for 788.2s\n",
      "    17.2% complete\n",
      "  RUNNING for 819.5s\n",
      "    17.6% complete\n",
      "  RUNNING for 850.9s\n",
      "    18.4% complete\n",
      "  RUNNING for 881.7s\n",
      "    18.7% complete\n",
      "  RUNNING for 913.5s\n",
      "    19.0% complete\n",
      "  RUNNING for 945.0s\n",
      "    20.3% complete\n",
      "  RUNNING for 976.7s\n",
      "    20.7% complete\n",
      "  RUNNING for 1008.3s\n",
      "    21.6% complete\n",
      "  RUNNING for 1039.8s\n",
      "    22.0% complete\n",
      "  RUNNING for 1071.1s\n",
      "    22.3% complete\n",
      "  RUNNING for 1102.4s\n",
      "    23.3% complete\n",
      "  RUNNING for 1133.3s\n",
      "    23.8% complete\n",
      "  RUNNING for 1164.7s\n",
      "    24.3% complete\n",
      "  RUNNING for 1195.7s\n",
      "    24.7% complete\n",
      "  RUNNING for 1227.4s\n",
      "    25.1% complete\n",
      "  RUNNING for 1259.1s\n",
      "    25.7% complete\n",
      "  RUNNING for 1290.7s\n",
      "    27.2% complete\n",
      "  RUNNING for 1322.3s\n",
      "    27.6% complete\n",
      "  RUNNING for 1353.2s\n",
      "    28.0% complete\n",
      "  RUNNING for 1384.7s\n",
      "    28.8% complete\n",
      "  RUNNING for 1416.3s\n",
      "    29.2% complete\n",
      "  RUNNING for 1447.2s\n",
      "    30.0% complete\n",
      "  RUNNING for 1479.2s\n",
      "    31.2% complete\n",
      "  RUNNING for 1510.1s\n",
      "    31.6% complete\n",
      "  RUNNING for 1541.6s\n",
      "    32.0% complete\n",
      "  RUNNING for 1572.6s\n",
      "    32.4% complete\n",
      "  RUNNING for 1604.2s\n",
      "    32.8% complete\n",
      "  RUNNING for 1635.0s\n",
      "    33.8% complete\n",
      "  RUNNING for 1666.1s\n",
      "    34.3% complete\n",
      "  RUNNING for 1697.6s\n",
      "    34.7% complete\n",
      "  RUNNING for 1729.0s\n",
      "    35.0% complete\n",
      "  RUNNING for 1760.7s\n",
      "    35.6% complete\n",
      "  RUNNING for 1792.2s\n",
      "    35.9% complete\n",
      "  RUNNING for 1824.0s\n",
      "    36.6% complete\n",
      "  RUNNING for 1855.3s\n",
      "    37.0% complete\n",
      "  RUNNING for 1886.4s\n",
      "    37.7% complete\n",
      "  RUNNING for 1917.9s\n",
      "    38.3% complete\n",
      "  RUNNING for 1948.8s\n",
      "    38.7% complete\n",
      "  RUNNING for 1980.7s\n",
      "    39.1% complete\n",
      "  RUNNING for 2011.9s\n",
      "    39.7% complete\n",
      "  RUNNING for 2044.0s\n",
      "    40.3% complete\n",
      "  RUNNING for 2074.9s\n",
      "    40.7% complete\n",
      "  RUNNING for 2106.5s\n",
      "    41.0% complete\n",
      "  RUNNING for 2138.0s\n",
      "    41.4% complete\n",
      "  RUNNING for 2169.0s\n",
      "    42.4% complete\n",
      "  RUNNING for 2200.2s\n",
      "    43.0% complete\n",
      "  RUNNING for 2232.0s\n",
      "    43.3% complete\n",
      "  RUNNING for 2262.8s\n",
      "    43.7% complete\n",
      "  RUNNING for 2294.7s\n",
      "    44.1% complete\n",
      "  RUNNING for 2326.3s\n",
      "    44.6% complete\n",
      "  RUNNING for 2357.1s\n",
      "    45.3% complete\n",
      "  RUNNING for 2388.6s\n",
      "    45.6% complete\n",
      "  RUNNING for 2420.4s\n",
      "    46.2% complete\n",
      "  RUNNING for 2451.3s\n",
      "    46.8% complete\n",
      "  RUNNING for 2483.4s\n",
      "    47.4% complete\n",
      "  RUNNING for 2514.6s\n",
      "    48.0% complete\n",
      "  RUNNING for 2545.4s\n",
      "    48.4% complete\n",
      "  RUNNING for 2576.7s\n",
      "    48.8% complete\n",
      "  RUNNING for 2608.6s\n",
      "    49.3% complete\n",
      "  RUNNING for 2639.9s\n",
      "    49.7% complete\n",
      "  RUNNING for 2670.9s\n",
      "    50.1% complete\n",
      "  RUNNING for 2702.4s\n",
      "    51.1% complete\n",
      "  RUNNING for 2734.6s\n",
      "    51.5% complete\n",
      "  RUNNING for 2766.0s\n",
      "    52.0% complete\n",
      "  RUNNING for 2797.8s\n",
      "    52.4% complete\n",
      "  RUNNING for 2829.1s\n",
      "    52.8% complete\n",
      "  RUNNING for 2860.4s\n",
      "    53.1% complete\n",
      "  RUNNING for 2892.0s\n",
      "    53.9% complete\n",
      "  RUNNING for 2924.0s\n",
      "    54.3% complete\n",
      "  RUNNING for 2954.8s\n",
      "    58.3% complete\n",
      "  RUNNING for 2986.4s\n",
      "    59.9% complete\n",
      "  RUNNING for 3018.0s\n",
      "    60.1% complete\n",
      "  RUNNING for 3049.8s\n",
      "    78.5% complete\n",
      "  RUNNING for 3081.5s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3SQKHHVKFXV9M on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3SQKHHVKFXV9M/syslog.2016-06-19-18\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3SQKHHVKFXV9M/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8253473\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26146092\n",
      "\t\tFILE: Number of bytes written=98175924\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=8253473\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=63586627200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=31497301440\n",
      "\t\tTotal time spent by all map tasks (ms)=44157380\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1987082100\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10936563\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=984290670\n",
      "\t\tTotal vcore-seconds taken by all map tasks=44157380\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10936563\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=26316040\n",
      "\t\tCombine input records=89901041\n",
      "\t\tCombine output records=1800897\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=65848\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=2604755159\n",
      "\t\tMap output materialized bytes=50913871\n",
      "\t\tMap output records=89901041\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=117924012032\n",
      "\t\tReduce input groups=9588\n",
      "\t\tReduce input records=1800897\n",
      "\t\tReduce output records=9588\n",
      "\t\tReduce shuffle bytes=50913871\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=3601794\n",
      "\t\tTotal committed heap usage (bytes)=125491478528\n",
      "\t\tVirtual memory (bytes) snapshot=409768972288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/stripeMaker_jac.Alex.20160619.182649.857818/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/stripeMaker_jac.Alex.20160619.182649.857818...\n",
      "Killing our SSH tunnel (pid 17929)\n"
     ]
    }
   ],
   "source": [
    "# run on the cloud\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python stripeMaker_jac.py -r emr s3://filtered-5grams/ \\\n",
    "    --file=s3://aks-w261-hw5/stopwords.txt \\\n",
    "    --file=s3://aks-w261-hw5/top10000 \\\n",
    "    --file=s3://aks-w261-hw5/topVocab \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My stripe maker took 50 minutes running on a 4 node cluster, 1 master at m1.medium  and 3 cores at m3.xlarge. This is by far my most intensive MRJob. This took 5 minutes longer than my stripe maker for the cosine similarities likely because this stripemaker outputted more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the index locally and save it on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_stripes2/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00003 to s3://aks-w261-hw5/5_4_stripes2/part-00003\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00006 to s3://aks-w261-hw5/5_4_stripes2/part-00006\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00005 to s3://aks-w261-hw5/5_4_stripes2/part-00005\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_stripes2/part-00000\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00008 to s3://aks-w261-hw5/5_4_stripes2/part-00008\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00004 to s3://aks-w261-hw5/5_4_stripes2/part-00004\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00007 to s3://aks-w261-hw5/5_4_stripes2/part-00007\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00002 to s3://aks-w261-hw5/5_4_stripes2/part-00002\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00001 to s3://aks-w261-hw5/5_4_stripes2/part-00001\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00009 to s3://aks-w261-hw5/5_4_stripes2/part-00009\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00010 to s3://aks-w261-hw5/5_4_stripes2/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/_SUCCESS to 5_4_stripes2/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00005 to 5_4_stripes2/part-00005\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00006 to 5_4_stripes2/part-00006\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00002 to 5_4_stripes2/part-00002\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00004 to 5_4_stripes2/part-00004\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00000 to 5_4_stripes2/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00009 to 5_4_stripes2/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00003 to 5_4_stripes2/part-00003\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00010 to 5_4_stripes2/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00001 to 5_4_stripes2/part-00001\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00008 to 5_4_stripes2/part-00008\n",
      "download: s3://aks-w261-hw5/5_4_stripes2/part-00007 to 5_4_stripes2/part-00007\n",
      "_SUCCESS   part-00001 part-00003 part-00005 part-00007 part-00009\n",
      "part-00000 part-00002 part-00004 part-00006 part-00008 part-00010\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes2\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_stripes2/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_stripes2 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes2\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes2\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes2/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stripes for Jaccard created.\n"
     ]
    }
   ],
   "source": [
    "# save a portion of one of the files for testing the next MRJob\n",
    "!head -500 ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_stripes2/part-00000 > test_stripes2\n",
    "print \"Testing stripes for Jaccard created.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invert the index through MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invert_jac.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invert_jac.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import math\n",
    " \n",
    "class invert_jac(MRJob):\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and inverts each stripe\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # we use a try in case in case any \n",
    "        # data go written in a weird format\n",
    "        try:\n",
    "            # split the line by the tabs, set the\n",
    "            # word and its cooccurence words\n",
    "            line = line.split(\"\\t\")\n",
    "            word = line[0].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = line[1].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            other_words = ast.literal_eval(other_words)\n",
    "\n",
    "        \n",
    "            # loop through each of the other words\n",
    "            for other_word in other_words:\n",
    "\n",
    "                # make sure we don't grab a star\n",
    "                # word\n",
    "                if other_word[0] != \"*\":\n",
    "\n",
    "                    # create a dictionary for the \n",
    "                    # invert, holds the word and \n",
    "                    # inverted value, and that words\n",
    "                    # star value\n",
    "                    inverted_value = \\\n",
    "                    other_words[other_word]\n",
    "                    word_star = \"*\"+word\n",
    "                    word_star_val = other_words[word_star]\n",
    "\n",
    "                    # set the dictionary\n",
    "                    invert = {word:inverted_value,\\\n",
    "                             word_star:word_star_val}\n",
    "\n",
    "                    # yield the inverted word with \n",
    "                    # its inverted value\n",
    "                    yield other_word, str(invert)\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # our reducer combines the inverted values\n",
    "    # for multiple words\n",
    "    def reducer(self, word, cooccurs):\n",
    "        \n",
    "        # create a dictionary to hold the \n",
    "        # co-occurence counts for each word\n",
    "        cooccur = {}\n",
    "        \n",
    "        # loop through the coocurrences \n",
    "        # dictionaries, combining as we \n",
    "        # go along\n",
    "        for _cooccur in cooccurs:\n",
    "            \n",
    "            # read in the dictionary\n",
    "            _cooccur = ast.literal_eval(_cooccur)\n",
    "            \n",
    "            # loop through each other word\n",
    "            for other_word in _cooccur:\n",
    "            \n",
    "                # add the element to the dictionary\n",
    "                if other_word not in cooccur:\n",
    "                    cooccur[other_word] = 0\n",
    "                \n",
    "                # add the values to the dictionary\n",
    "                cooccur[other_word] = cooccur[other_word] + \\\n",
    "                _cooccur[other_word]\n",
    "        \n",
    "        # yield the word and its cooccruence\n",
    "        yield word, str(cooccur)      \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    invert_jac.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invert_jac.Alex.20160620.233201.061278\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invert_jac.Alex.20160620.233201.061278/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invert_jac.Alex.20160620.233201.061278...\n",
      "\"abnormalities\"\t\"{'indicated': 141, '*expected': 10810318, '*genesis': 439326, '*glucose': 435882, 'epithelial': 205, 'female': 320, '*connective': 334489, '*indicated': 2714938, 'carbonate': 50, 'connective': 48, '*gastric': 303646, '*intrinsic': 470104, 'expected': 70, 'genesis': 76, 'glucose': 94, 'dominant': 49, '*dominant': 1338319, 'facial': 352, 'intrinsic': 176, 'gastric': 98, '*facial': 242173, '*female': 2074776, '*epithelial': 209755, '*carbonate': 246688}\"\n"
     ]
    }
   ],
   "source": [
    "!python invert_jac.py test_stripes2 > temp.txt\n",
    "!head -1 temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invert the index on the cloud for the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invert_jac.Alex.20160619.193554.228592\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/invert_jac.Alex.20160619.193554.228592/files/...\n",
      "Adding our job to existing cluster j-25Y2I2CYBIZC3\n",
      "Waiting for step 1 of 1 (s-353PE6R70CQLB) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40140/cluster\n",
      "  RUNNING for 1.3s\n",
      "   100.0% complete\n",
      "  RUNNING for 33.7s\n",
      "     5.0% complete\n",
      "  RUNNING for 65.1s\n",
      "    39.8% complete\n",
      "  RUNNING for 96.1s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-353PE6R70CQLB on ec2-54-183-228-122.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-183-228-122.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-353PE6R70CQLB/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8327276\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9028528\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6228713\n",
      "\t\tFILE: Number of bytes written=15991897\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2883\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=31\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=8327276\n",
      "\t\tS3: Number of bytes written=9028528\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=31\n",
      "\t\tLaunched map tasks=31\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=994927680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=566432640\n",
      "\t\tTotal time spent by all map tasks (ms)=690922\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=31091490\n",
      "\t\tTotal time spent by all reduce tasks (ms)=196678\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17701020\n",
      "\t\tTotal vcore-seconds taken by all map tasks=690922\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=196678\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=80250\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=10353\n",
      "\t\tInput split bytes=2883\n",
      "\t\tMap input records=9588\n",
      "\t\tMap output bytes=12472720\n",
      "\t\tMap output materialized bytes=5408277\n",
      "\t\tMap output records=246064\n",
      "\t\tMerged Map outputs=341\n",
      "\t\tPhysical memory (bytes) snapshot=18333659136\n",
      "\t\tReduce input groups=991\n",
      "\t\tReduce input records=246064\n",
      "\t\tReduce output records=991\n",
      "\t\tReduce shuffle bytes=5408277\n",
      "\t\tShuffled Maps =341\n",
      "\t\tSpilled Records=492128\n",
      "\t\tTotal committed heap usage (bytes)=20689453056\n",
      "\t\tVirtual memory (bytes) snapshot=96741961728\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/invert_jac.Alex.20160619.193554.228592/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/invert_jac.Alex.20160619.193554.228592...\n",
      "Killing our SSH tunnel (pid 18079)\n"
     ]
    }
   ],
   "source": [
    "# run on the cloud\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python invert_jac.py -r emr s3://aks-w261-hw5/5_4_stripes2/ \\\n",
    "    --cluster-id=j-25Y2I2CYBIZC3 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the files locally and transfer them to a safe place on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_invert2/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00006 to s3://aks-w261-hw5/5_4_invert2/part-00006\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00007 to s3://aks-w261-hw5/5_4_invert2/part-00007\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00002 to s3://aks-w261-hw5/5_4_invert2/part-00002\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00004 to s3://aks-w261-hw5/5_4_invert2/part-00004\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_invert2/part-00000\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00003 to s3://aks-w261-hw5/5_4_invert2/part-00003\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00005 to s3://aks-w261-hw5/5_4_invert2/part-00005\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00008 to s3://aks-w261-hw5/5_4_invert2/part-00008\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00001 to s3://aks-w261-hw5/5_4_invert2/part-00001\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00009 to s3://aks-w261-hw5/5_4_invert2/part-00009\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00010 to s3://aks-w261-hw5/5_4_invert2/part-00010\n",
      "download: s3://aks-w261-hw5/5_4_invert2/_SUCCESS to 5_4_invert2/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00004 to 5_4_invert2/part-00004\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00006 to 5_4_invert2/part-00006\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00009 to 5_4_invert2/part-00009\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00001 to 5_4_invert2/part-00001\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00005 to 5_4_invert2/part-00005\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00000 to 5_4_invert2/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00007 to 5_4_invert2/part-00007\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00003 to 5_4_invert2/part-00003\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00002 to 5_4_invert2/part-00002\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00008 to 5_4_invert2/part-00008\n",
      "download: s3://aks-w261-hw5/5_4_invert2/part-00010 to 5_4_invert2/part-00010\n",
      "_SUCCESS   part-00001 part-00003 part-00005 part-00007 part-00009\n",
      "part-00000 part-00002 part-00004 part-00006 part-00008 part-00010\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert2\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_invert2/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_invert2 /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert2\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert2\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert2/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test file for jacard inverted indexes\n"
     ]
    }
   ],
   "source": [
    "# save a chunk of the output for testing \n",
    "# the next part\n",
    "!head -250 ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_invert2/part-00000 > jac_invert.txt\n",
    "print \"Created test file for jaccard inverted indexes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRJob class for calculated the jaccard similarity for pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jaccardSim.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jaccardSim.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import math\n",
    "from mrjob.protocol import RawProtocol  \n",
    " \n",
    "class jaccardSim(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        \n",
    "        # set how we want to sort, we use a single reducer\n",
    "        # only so that we can do a total sort, we use multiple\n",
    "        # reducers for all other steps\n",
    "        JOBCONF = {        \n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer),\\\n",
    "                MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=None,\\\n",
    "                       reducer=self.reducer_final)]\n",
    "    \n",
    "    \n",
    "    # our mapper reads in each line of data\n",
    "    # and outputs the partial aggregation\n",
    "    # for each pair combination\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # we use a try in case in case any \n",
    "        # data go written in a weird format\n",
    "        try:\n",
    "            # split the line by the tabs, set the\n",
    "            # word and its cooccurence words\n",
    "            line = line.split(\"\\t\")\n",
    "            key = line[0].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            words = line[1].strip().strip(\"\\\"\").strip(\"'\")\n",
    "            words = ast.literal_eval(words)\n",
    "\n",
    "            # gather the length of this word,\n",
    "            # its cardinality\n",
    "            key_length = 0\n",
    "            for word in words:\n",
    "                # exclude star cardinalities\n",
    "                if word[0] != \"*\":\n",
    "                    key_length = key_length + words[word]\n",
    "\n",
    "            # loop through each of the words\n",
    "            for word in words:\n",
    "\n",
    "                # make sure it's not the star\n",
    "                # word\n",
    "                if word[0] != \"*\":\n",
    "\n",
    "                    # set the star word\n",
    "                    star_word = \"*\"+word\n",
    "\n",
    "                    # as long as it isn't this \n",
    "                    # word\n",
    "                    if word != key:\n",
    "\n",
    "                        # get the intersection value\n",
    "                        intersection = words[word]\n",
    "\n",
    "                        # get the star word's values\n",
    "                        star_word_val = words[star_word]\n",
    "\n",
    "                        # take the intersection divided\n",
    "                        # by the total count of both \n",
    "                        # words minus the intersection\n",
    "                        similarity = float(intersection) / \\\n",
    "                        (float(star_word_val) + \\\n",
    "                         float(key_length) - \\\n",
    "                         float(intersection))\n",
    "\n",
    "                        # set the pair\n",
    "                        pair = key,word\n",
    "                        \n",
    "                        # yeild the pair and its\n",
    "                        # partial similarity\n",
    "                        yield str(pair),str(similarity)\n",
    "                        \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    # our reducer makes sure that all the \n",
    "    # similarities are of a consistent length\n",
    "    # for sorting\n",
    "    def reducer(self, pair, similarity):\n",
    "        \n",
    "        # each word pair should only really\n",
    "        # produce one similarity\n",
    "        similarity=list(similarity)[0]\n",
    "        \n",
    "        # we want to make sure that the count \n",
    "        # string is at least 15 characters, if\n",
    "        # its not, we add leading zeros. this\n",
    "        # helps us sort\n",
    "        extend = \"0\"\n",
    "        \n",
    "        # convert density to a string\n",
    "        similarity = str(similarity)\n",
    "        \n",
    "        # while the count lenght is less than 15\n",
    "        while len(similarity) < 15:\n",
    "            similarity = similarity + extend\n",
    "        \n",
    "        # yield the pair and its partial\n",
    "        # similarity\n",
    "        yield str(similarity),pair\n",
    "        \n",
    "        \n",
    "    # our reducer final sorts these\n",
    "    # similarities\n",
    "    def reducer_final(self,similarity,pairs):\n",
    "        \n",
    "        # simply pass out each pair\n",
    "        for pair in pairs:\n",
    "            yield pair,similarity\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    jaccardSim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/jaccardSim.Alex.20160619.211947.817016\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/jaccardSim.Alex.20160619.211947.817016/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/jaccardSim.Alex.20160619.211947.817016...\n",
      "('schooling', 'americans')\t0.00010000800064\n",
      "('politically', 'ignored')\t0.000100040593395\n",
      "('dew', 'adam')\t0.00010005769994\n",
      "('illustrating', 'colored')\t0.000100076516837\n",
      "('softly', 'lap')\t0.000100100851608\n",
      "('stead', 'universal')\t0.000100110488434\n",
      "('softly', 'song')\t0.0001001124993\n",
      "('conditional', 'granting')\t0.000100114386011\n",
      "('schooling', 'esteem')\t0.000100155240623\n",
      "('imputed', 'suggestions')\t0.000100168113472\n"
     ]
    }
   ],
   "source": [
    "# test using the command line\n",
    "!python jaccardSim.py jac_invert.txt > jac_prelim\n",
    "!head jac_prelim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Create a cluster and let's get this started in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/no_script.Alex.20160619.210835.478883\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/no_script.Alex.20160619.210835.478883/files/...\n",
      "j-1WX5D4SG2HNQB\n"
     ]
    }
   ],
   "source": [
    "# create the cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-1 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://aks-w261-hw5/out_5_4/_SUCCESS\n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00009       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00010       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00002      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00000      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00001      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00005      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00003      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00006      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00007      \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00004       \n",
      "delete: s3://aks-w261-hw5/out_5_4/part-00008       \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/jaccardSim.Alex.20160619.212012.913464\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/jaccardSim.Alex.20160619.212012.913464/files/...\n",
      "Adding our job to existing cluster j-1WX5D4SG2HNQB\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Waiting for step 1 of 2 (s-2N4Z5ONFLOQ7G) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40671/cluster\n",
      "  RUNNING for 26.6s\n",
      "     5.0% complete\n",
      "  RUNNING for 59.1s\n",
      "    32.3% complete\n",
      "  RUNNING for 90.4s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2N4Z5ONFLOQ7G on ec2-52-53-211-37.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-52-53-211-37.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2N4Z5ONFLOQ7G/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9209186\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10631276\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6139305\n",
      "\t\tFILE: Number of bytes written=16434961\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2576\n",
      "\t\tHDFS: Number of bytes written=10631276\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=89\n",
      "\t\tHDFS: Number of write operations=22\n",
      "\t\tS3: Number of bytes read=9209186\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=28\n",
      "\t\tLaunched map tasks=28\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=945288000\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=463320000\n",
      "\t\tTotal time spent by all map tasks (ms)=656450\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29540250\n",
      "\t\tTotal time spent by all reduce tasks (ms)=160875\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14478750\n",
      "\t\tTotal vcore-seconds taken by all map tasks=656450\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=160875\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=213900\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=10667\n",
      "\t\tInput split bytes=2576\n",
      "\t\tMap input records=991\n",
      "\t\tMap output bytes=10631188\n",
      "\t\tMap output materialized bytes=6231307\n",
      "\t\tMap output records=246064\n",
      "\t\tMerged Map outputs=308\n",
      "\t\tPhysical memory (bytes) snapshot=16435113984\n",
      "\t\tReduce input groups=246064\n",
      "\t\tReduce input records=246064\n",
      "\t\tReduce output records=246064\n",
      "\t\tReduce shuffle bytes=6231307\n",
      "\t\tShuffled Maps =308\n",
      "\t\tSpilled Records=492128\n",
      "\t\tTotal committed heap usage (bytes)=18817220608\n",
      "\t\tVirtual memory (bytes) snapshot=90734755840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-1VFFAT6SVE4W7) to complete...\n",
      "  RUNNING for 73.2s\n",
      "    80.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1VFFAT6SVE4W7 on ec2-52-53-211-37.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-52-53-211-37.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1VFFAT6SVE4W7/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10977523\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10631276\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6936953\n",
      "\t\tFILE: Number of bytes written=17481573\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10982506\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=66\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=10631276\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=33\n",
      "\t\tLaunched map tasks=33\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1141312320\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=62853120\n",
      "\t\tTotal time spent by all map tasks (ms)=792578\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35666010\n",
      "\t\tTotal time spent by all reduce tasks (ms)=21824\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1964160\n",
      "\t\tTotal vcore-seconds taken by all map tasks=792578\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=21824\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=72910\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=10083\n",
      "\t\tInput split bytes=4983\n",
      "\t\tMap input records=246064\n",
      "\t\tMap output bytes=10631276\n",
      "\t\tMap output materialized bytes=6996645\n",
      "\t\tMap output records=246064\n",
      "\t\tMerged Map outputs=33\n",
      "\t\tPhysical memory (bytes) snapshot=16387506176\n",
      "\t\tReduce input groups=245673\n",
      "\t\tReduce input records=246064\n",
      "\t\tReduce output records=246064\n",
      "\t\tReduce shuffle bytes=6996645\n",
      "\t\tShuffled Maps =33\n",
      "\t\tSpilled Records=492128\n",
      "\t\tTotal committed heap usage (bytes)=17554735104\n",
      "\t\tVirtual memory (bytes) snapshot=68166922240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/jaccardSim.Alex.20160619.212012.913464/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/jaccardSim.Alex.20160619.212012.913464...\n",
      "Killing our SSH tunnel (pid 717)\n"
     ]
    }
   ],
   "source": [
    "# run on the cloud\n",
    "!aws s3 rm --recursive s3://aks-w261-hw5/out_5_4\n",
    "!python jaccardSim.py -r emr s3://aks-w261-hw5/5_4_invert2/ \\\n",
    "    --cluster-id=j-1WX5D4SG2HNQB \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw5/out_5_4 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the jaccard similarities to a local file and save to a bucket on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim: File exists\n",
      "copy: s3://aks-w261-hw5/out_5_4/_SUCCESS to s3://aks-w261-hw5/5_4_jaccardSim/_SUCCESS\n",
      "copy: s3://aks-w261-hw5/out_5_4/part-00000 to s3://aks-w261-hw5/5_4_jaccardSim/part-00000\n",
      "download: s3://aks-w261-hw5/5_4_jaccardSim/_SUCCESS to 5_4_jaccardSim/_SUCCESS\n",
      "download: s3://aks-w261-hw5/5_4_jaccardSim/part-00000 to 5_4_jaccardSim/part-00000\n",
      "_SUCCESS   part-00000\n"
     ]
    }
   ],
   "source": [
    "# make a directory to hold the longest ngram\n",
    "# files\n",
    "!mkdir /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim\n",
    "\n",
    "# sync the files to a local directory and save them on s3\n",
    "!aws s3 cp s3://aks-w261-hw5/out_5_4/ s3://aks-w261-hw5/5_4_jaccardSim/ --recursive\n",
    "!aws s3 sync s3://aks-w261-hw5/5_4_jaccardSim /Users/Alex/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim\n",
    "!ls ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim\n",
    "!rm ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t('practitioners', 'esteem')\t9.9999733334e-05\r\n",
      "\t('practise', 'duties')\t9.99953259975e-05\r\n",
      "\t('lighting', 'unfortunate')\t9.9993809907e-05\r\n",
      "\t('defines', 'bureau')\t9.9993173543e-05\r\n",
      "\t('expects', 'submission')\t9.9992857653e-05\r\n",
      "\t('crucified', 'hill')\t9.99925621296e-05\r\n",
      "\t('economists', 'useful')\t9.99914419089e-06\r\n",
      "\t('kinship', 'seventh')\t9.99908857865e-05\r\n",
      "\t('israelites', 'possessions')\t9.99902009603e-05\r\n",
      "\t('sac', 'extra')\t9.99900009999e-05\r\n"
     ]
    }
   ],
   "source": [
    "# preview the jaccard similarities\n",
    "!head ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the files\n",
    "The Jaccard similarity scores can be found at s3://aks-w261-hw5/5_4_jaccardSim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "*In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4). Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code: <br>\n",
    "nltk_synonyms.py<br>\n",
    "<br>\n",
    "Note: This will require installing the python nltk package:<br>\n",
    "http://www.nltk.org/install.html<br>\n",
    "and downloading its data with nltk.download().<br>\n",
    "<br>\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, synonyms(word2), and vice-versa. If one of the two is a synonym of the other, then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK's synonyms\n",
    "We use the python code provided to check to see the synomyms of various words that we calculated. We copy the code of the python program into our notebook here because it is easier to read and understand if it's in the notebook. Lukily, it's short and won't take up too much space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile nltk_synonyms.py\n",
    "#!/usr/bin/python2.7\n",
    "''' pass a string to this funciton ( eg 'car') and it will give you a list of\n",
    "words which is related to cat, called lemma of CAT. '''\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for the word dog\n",
      "[u'go_after', u'chase_after', u'pawl', u'dog', u'wiener', u'tag', u'frankfurter', u'hound', u'click', u'chase', u'andiron', u'hot_dog', u'tail', u'Canis_familiaris', u'give_chase', u'wienerwurst', u'bounder', u'domestic_dog', u'track', u'frank', u'trail', u'blackguard', u'weenie', u'frump', u'firedog', u'detent', u'dog-iron', u'cad', u'heel', u'hotdog']\n"
     ]
    }
   ],
   "source": [
    "# let's test it the synonyms with a \n",
    "# sample word, dog\n",
    "print \"Synonyms for the word dog\"\n",
    "print synonyms('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym files created\n"
     ]
    }
   ],
   "source": [
    "# top 1,0000 words are considered synonyms\n",
    "!head -1000 ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_jaccardSim/part-00000 > SYNON_jaccard\n",
    "!head -1000 ~/Documents/Berkeley/1602Summer/W261/HW5/5_4_cosineSim/part-00000 > SYNON_cosine\n",
    "print \"Synonym files created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> COSINE ACCURACY:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRJob class to calculate the percision (FOR COSINE SIMILARITY)\n",
    "We classify the top 1,000 words as synonyms. Before explaining how we calculate this, we define some terms:\n",
    "- TP: true positive, our model placed this pair of words in the top 1,000 words and the NLTK corpus *agreed* that they were synonyms\n",
    "- FP: false positive, our model placed this pair of words in the top 1,000 words and the NLTK corpus *disagreed* that they were synonyms\n",
    "\n",
    "Percision can be calculated as TP / (TP + FP). In this case, we calculate the average percision value for each term that had a synonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting percision.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile percision.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from nltk_synonyms import synonyms\n",
    "import mrjob\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "import math\n",
    " \n",
    "class percision(MRJob):\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "\n",
    "    # our mapper reads in each line of data\n",
    "    # and outputs whether the record is a \n",
    "    # true positive or a false positive\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # word and its cooccurence words\n",
    "        line = line.split(\"\\t\")\n",
    "        word1,word2 = ast.literal_eval(line[0])\n",
    "        simil = float(line[2])\n",
    "\n",
    "        # find the synonyms for the words\n",
    "        synon1 = synonyms(word1)\n",
    "        synon2 = synonyms(word2)\n",
    "\n",
    "        # only do all this work if NLTK \n",
    "        # actually has a synonym for this\n",
    "        # word\n",
    "        if len(synon1) > 0:\n",
    "            \n",
    "            # set a boolean to tell if its a \n",
    "            # true synonym\n",
    "            correct = False\n",
    "\n",
    "            # check to see if any of the synonyms\n",
    "            # match the either word\n",
    "            for word in synon1:\n",
    "                if word == word2:\n",
    "                    correct = True\n",
    "            for word in synon2:\n",
    "                if word == word1:\n",
    "                    correct = True\n",
    "\n",
    "\n",
    "            # if we're correct yield a true \n",
    "            # positive, otherwise yield a false\n",
    "            # positive, for both words, either\n",
    "            # the pair is a hit or its not\n",
    "            if correct:\n",
    "                yield word1,[1,0]\n",
    "                yield word2,[1,0]\n",
    "            else:\n",
    "                yield word1,[0,1]\n",
    "                yield word2,[0,1]\n",
    "            \n",
    "    \n",
    "    # our reducer combines the values\n",
    "    # of the true and false positives \n",
    "    # and sets the class indicator\n",
    "    def reducer(self, word, counts):\n",
    "        \n",
    "        # set an array to store the counts\n",
    "        # for each word, accuracy counts\n",
    "        accuracy = [0]*2\n",
    "        accuracy = np.array(accuracy)\n",
    "        \n",
    "        # loop through each count\n",
    "        for count in counts:\n",
    "            \n",
    "            # convert each count to a numpy\n",
    "            # array and sum it with our existing\n",
    "            # totals\n",
    "            count = np.array(count)\n",
    "            accuracy = accuracy + count\n",
    "        \n",
    "        # send the true positives and false\n",
    "        # positives\n",
    "        TP = float(accuracy[0])\n",
    "        FP = float(accuracy[1])\n",
    "        \n",
    "        # calculate the average percision\n",
    "        # for the word\n",
    "        precis = TP / (TP + FP)\n",
    "        \n",
    "        # pull the the number of synonyms\n",
    "        # for this word\n",
    "        total_synon = len(synonyms(word))\n",
    "        \n",
    "        # only emit for words that actually\n",
    "        # have a chance of having a synonym\n",
    "        if total_synon > 0:\n",
    "        \n",
    "            # recall is the number of correct over\n",
    "            # total number of synonyms\n",
    "            recall = float(TP)/float(total_synon)\n",
    "\n",
    "            # we define the F1 score as \n",
    "            # 2 * percisions * recall, all divided\n",
    "            # by percision plus the recall\n",
    "            if precis + recall !=0:\n",
    "                f1_score = (2 * precis * recall) / \\\n",
    "                (precis + recall)\n",
    "            else:\n",
    "                f1_score = \"NA\"\n",
    "\n",
    "            # turn the precision, recall, and f1 \n",
    "            # score into a tuple\n",
    "            scoring = precis,recall,f1_score\n",
    "\n",
    "            # yield the average percision\n",
    "            yield word, scoring\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    percision.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Calculate the accuracy of cosine similarities\n",
    "Important to note that for a job this small, we don't need to run it on the cluster and spend resources. Our local machine can easily handle this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing files created\n"
     ]
    }
   ],
   "source": [
    "# create a smaller data set\n",
    "!head -250 SYNON_jaccard > SYNON_jaccard_test\n",
    "!head -250 SYNON_cosine > SYNON_cosine_test\n",
    "print \"Testing files created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percision.Alex.20160619.222031.989378\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percision.Alex.20160619.222031.989378/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percision.Alex.20160619.222031.989378...\n",
      "\"tumor\"\t[1.0, 0.6666666666666666, 0.8]\n",
      "\"tumors\"\t[1.0, 0.6666666666666666, 0.8]\n",
      "\"acetic\"\t[0.0, 0.0, \"NA\"]\n",
      "\"alkaline\"\t[0.0, 0.0, \"NA\"]\n",
      "\"amenable\"\t[0.0, 0.0, \"NA\"]\n",
      "\"ammonia\"\t[0.0, 0.0, \"NA\"]\n",
      "\"ammonium\"\t[0.0, 0.0, \"NA\"]\n",
      "\"amplifier\"\t[0.0, 0.0, \"NA\"]\n",
      "\"anterior\"\t[0.0, 0.0, \"NA\"]\n",
      "\"armature\"\t[0.0, 0.0, \"NA\"]\n"
     ]
    }
   ],
   "source": [
    "# unit test the cosine file\n",
    "!python percision.py SYNON_cosine_test > synon_output_test\n",
    "!cat synon_output_test | sort -k3,3nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percision.Alex.20160619.222806.738585\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percision.Alex.20160619.222806.738585/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percision.Alex.20160619.222806.738585...\n",
      "\"abdominal\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accent\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accompaniment\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accomplished\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accustomed\"\t[0.0, 0.0, \"NA\"]\n",
      "\"acquainted\"\t[0.0, 0.0, \"NA\"]\n",
      "\"administering\"\t[0.0, 0.0, \"NA\"]\n",
      "\"adversely\"\t[0.0, 0.0, \"NA\"]\n",
      "\"advertisement\"\t[0.0, 0.0, \"NA\"]\n",
      "\"adviser\"\t[0.0, 0.0, \"NA\"]\n"
     ]
    }
   ],
   "source": [
    "# run on the full data set\n",
    "!python percision.py SYNON_cosine > synon_output_cosine\n",
    "!cat synon_output_test | sort -k3,3nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> JACCARD ACCURACY:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRJob class for jaccard similarity scoring\n",
    "We have already done cosine scoring. Now let's move on to Jaccard similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting percisionJac.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile percisionJac.py\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from nltk_synonyms import synonyms\n",
    "import mrjob\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "import math\n",
    " \n",
    "class percisionJac(MRJob):\n",
    "    \n",
    "    # define the steps of the job and the order in\n",
    "    # which they will be executed\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "\n",
    "    # our mapper reads in each line of data\n",
    "    # and outputs whether the record is a \n",
    "    # true positive or a false positive\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line by the tabs, set the\n",
    "        # word and its cooccurence words\n",
    "        line = line.split(\"\\t\")\n",
    "        word1,word2 = ast.literal_eval(line[1])\n",
    "        simil = float(line[2])\n",
    "\n",
    "        # find the synonyms for the words\n",
    "        synon1 = synonyms(word1)\n",
    "        synon2 = synonyms(word2)\n",
    "\n",
    "        # only do all this work if NLTK \n",
    "        # actually has a synonym for this\n",
    "        # word\n",
    "        if len(synon1) > 0:\n",
    "            \n",
    "            # set a boolean to tell if its a \n",
    "            # true synonym\n",
    "            correct = False\n",
    "\n",
    "            # check to see if any of the synonyms\n",
    "            # match the either word\n",
    "            for word in synon1:\n",
    "                if word == word2:\n",
    "                    correct = True\n",
    "            for word in synon2:\n",
    "                if word == word1:\n",
    "                    correct = True\n",
    "\n",
    "\n",
    "            # if we're correct yield a true \n",
    "            # positive, otherwise yield a false\n",
    "            # positive, for both words, either\n",
    "            # the pair is a hit or its not\n",
    "            if correct:\n",
    "                yield word1,[1,0]\n",
    "                yield word2,[1,0]\n",
    "            else:\n",
    "                yield word1,[0,1]\n",
    "                yield word2,[0,1]\n",
    "            \n",
    "    \n",
    "    # our reducer combines the values\n",
    "    # of the true and false positives \n",
    "    # and sets the class indicator\n",
    "    def reducer(self, word, counts):\n",
    "        \n",
    "        # set an array to store the counts\n",
    "        # for each word, accuracy counts\n",
    "        accuracy = [0]*2\n",
    "        accuracy = np.array(accuracy)\n",
    "        \n",
    "        # loop through each count\n",
    "        for count in counts:\n",
    "            \n",
    "            # convert each count to a numpy\n",
    "            # array and sum it with our existing\n",
    "            # totals\n",
    "            count = np.array(count)\n",
    "            accuracy = accuracy + count\n",
    "        \n",
    "        # send the true positives and false\n",
    "        # positives\n",
    "        TP = float(accuracy[0])\n",
    "        FP = float(accuracy[1])\n",
    "        \n",
    "        # calculate the average percision\n",
    "        # for the word\n",
    "        precis = TP / (TP + FP)\n",
    "        \n",
    "        # pull the the number of synonyms\n",
    "        # for this word\n",
    "        total_synon = len(synonyms(word))\n",
    "        \n",
    "        # only emit for words that actually\n",
    "        # have a chance of having a synonym\n",
    "        if total_synon > 0:\n",
    "        \n",
    "            # recall is the number of correct over\n",
    "            # total number of synonyms\n",
    "            recall = float(TP)/float(total_synon)\n",
    "\n",
    "            # we define the F1 score as \n",
    "            # 2 * percisions * recall, all divided\n",
    "            # by percision plus the recall\n",
    "            if precis + recall !=0:\n",
    "                f1_score = (2 * precis * recall) / \\\n",
    "                (precis + recall)\n",
    "            else:\n",
    "                f1_score = \"NA\"\n",
    "\n",
    "            # turn the precision, recall, and f1 \n",
    "            # score into a tuple\n",
    "            scoring = precis,recall,f1_score\n",
    "\n",
    "            # yield the average percision\n",
    "            yield word, scoring\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    percisionJac.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percisionJac.Alex.20160619.222504.010377\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percisionJac.Alex.20160619.222504.010377/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percisionJac.Alex.20160619.222504.010377...\n",
      "\"abdominal\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accent\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accompaniment\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accomplished\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accustomed\"\t[0.0, 0.0, \"NA\"]\n",
      "\"acquainted\"\t[0.0, 0.0, \"NA\"]\n",
      "\"administering\"\t[0.0, 0.0, \"NA\"]\n",
      "\"adversely\"\t[0.0, 0.0, \"NA\"]\n",
      "\"advertisement\"\t[0.0, 0.0, \"NA\"]\n",
      "\"adviser\"\t[0.0, 0.0, \"NA\"]\n"
     ]
    }
   ],
   "source": [
    "# unit test the cosine file\n",
    "!python percisionJac.py SYNON_jaccard_test > synon_output_test\n",
    "!cat synon_output_test | sort -k3,3nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percisionJac.Alex.20160619.222755.927357\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percisionJac.Alex.20160619.222755.927357/output...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/percisionJac.Alex.20160619.222755.927357...\n",
      "\"abdominal\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accent\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accompaniment\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accomplished\"\t[0.0, 0.0, \"NA\"]\n",
      "\"accustomed\"\t[0.0, 0.0, \"NA\"]\n",
      "\"acquainted\"\t[0.0, 0.0, \"NA\"]\n",
      "\"administering\"\t[0.0, 0.0, \"NA\"]\n",
      "\"adversely\"\t[0.0, 0.0, \"NA\"]\n",
      "\"advertisement\"\t[0.0, 0.0, \"NA\"]\n",
      "\"adviser\"\t[0.0, 0.0, \"NA\"]\n"
     ]
    }
   ],
   "source": [
    "# run the whole file\n",
    "!python percisionJac.py SYNON_jaccard > synon_output_jaccard\n",
    "!cat synon_output_test | sort -k3,3nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
