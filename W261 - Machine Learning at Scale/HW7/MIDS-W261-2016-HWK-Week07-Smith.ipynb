{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "**Alex Smith**<br>\n",
    "July 2, 2016<br>\n",
    "MIDS261 - Machine Learning at Scale<br>\n",
    "Professor Shanahan<br>\n",
    "Due: July 10, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Useful resources\n",
    "The following resources were particularly useful.\n",
    "- [Wikipedia: Longest Path Problem](https://en.wikipedia.org/wiki/Longest_path_problem)\n",
    "\n",
    "### Libraries\n",
    "The following libraries must be installed before running the below code. They can all be installed through [Pip](https://github.com/pypa/pip).\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "- [Regular Expression](https://docs.python.org/2/library/re.html)\n",
    "- [Pretty Table](https://pypi.python.org/pypi/PrettyTable)\n",
    "- [Random](https://docs.python.org/2/library/random.html)\n",
    "- [Datetime](https://docs.python.org/2/library/datetime.html)\n",
    "- [Matplotlib](http://matplotlib.org/)\n",
    "- [MRJob](https://pythonhosted.org/mrjob/)\n",
    "- [Pandas](http://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## General Description\n",
    "In this assignment you will explore networks and develop MRJob code for \n",
    "finding shortest path graph distances. To build up to large data \n",
    "you will develop your code on some very simple, toy networks.\n",
    "After this you will take your developed code forward and modify it and \n",
    "apply it to two larger datasets (performing EDA along the way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Undirected toy network dataset\n",
    "\n",
    "In an undirected network all links are symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' both of the links:\n",
    "\n",
    "A -> B and B -> A\n",
    "\n",
    "will exist. \n",
    "\n",
    "The toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "(node) \\t (dictionary of links)\n",
    "\n",
    "on AWS/Dropbox via the url:\n",
    "\n",
    "s3://ucb-mids-mls-networks/undirected_toy.txt\n",
    "On under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "The Data folder is in: https://db.tt/Kxu48mL1\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted).\n",
    "\n",
    "***\n",
    "## Directed toy network dataset\n",
    "\n",
    "In a directed network all links are not necessarily symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' it is possible for only one of:\n",
    "\n",
    "A -> B or B -> A\n",
    "\n",
    "to exist. \n",
    "\n",
    "These toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "(node) \\t (dictionary of links)\n",
    "\n",
    "on AWS/Dropbox via the url:\n",
    "\n",
    "s3://ucb-mids-mls-networks/directed_toy.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name\n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 7.0: Shortest path graph distances (toy networks)\n",
    "\n",
    "In this part of your assignment you will develop the base of your code for the week.\n",
    "\n",
    "Write MRJob classes to find shortest path graph distances, \n",
    "as described in the lectures. In addition to finding the distances, \n",
    "your code should also output a distance-minimizing path between the source and target.\n",
    "Work locally for this part of the assignment, and use \n",
    "both of the undirected and directed toy networks.\n",
    "\n",
    "To prove you code's function, run the following jobs\n",
    "\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "Solution: 1,5,4. NOTE: There is another shortest path also (HINT: 1->5->4)! Either will suffice (you will find this also in the remaining problems. E.g., 7.2 and 7.4.\n",
    " \n",
    "\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "Solution: 1,2,4,5\n",
    "\n",
    "and report your output---make sure it is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the graph\n",
    "First we set up the graph by creating the data structure that assigns to each node:\n",
    "- the distance from the source\n",
    "- the path from each node\n",
    "- the edges from that node\n",
    "- the status of that node as:\n",
    "    - 'Q': enqueue\n",
    "    - 'U': unvisited\n",
    "    - 'V': visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRgraphset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRgraphset.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRgraphset(MRJob):\n",
    "    \n",
    "    # set the source node\n",
    "    source = None\n",
    "    \n",
    "    # configure the options so that we can \n",
    "    # pass in the source node of interest\n",
    "    def configure_options(self):\n",
    "        super(MRgraphset, self).configure_options()\n",
    "        self.add_passthrough_option(\"--indx\", type='int', default=1)\n",
    "\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\\\n",
    "                        mapper=self.mapper)] \n",
    "    \n",
    "    \n",
    "    # the mapper init sets the source node\n",
    "    def mapper_init(self):\n",
    "        self.source = self.options.indx        \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # outputs the node, its path to \n",
    "    # the source, its distance to the \n",
    "    # source, it's linked nodes, and its\n",
    "    # status as visited\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        \n",
    "        # initalize distance, path, and\n",
    "        # status\n",
    "        dist = 0\n",
    "        path = []\n",
    "        status = None\n",
    "        \n",
    "        # if this is the source node\n",
    "        if node == self.source:\n",
    "            \n",
    "            # set the distance to 0\n",
    "            # the path to none and \n",
    "            # the status to 'Q' for\n",
    "            # queue\n",
    "            dist = 0\n",
    "            path = []\n",
    "            status = 'Q'\n",
    "            \n",
    "        # else if this is not the source node\n",
    "        else:\n",
    "            \n",
    "            # set the distance to a max\n",
    "            # value, the path to null, and \n",
    "            # the status to 'U' for unvisited\n",
    "            dist = sys.maxint\n",
    "            path = []\n",
    "            status = 'U'\n",
    "            \n",
    "        # for each node yield the initial\n",
    "        # graph state\n",
    "        key = node\n",
    "        value = (edges,dist,path,status)\n",
    "        yield key,value\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRgraphset.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"2\": 1, \"5\": 1}, 0, [], \"Q\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"5\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "3\t[{\"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "4\t[{\"3\": 1, \"2\": 1, \"5\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n"
     ]
    }
   ],
   "source": [
    "# run the MRJob class to create initial graph\n",
    "# that we'll iterate through\n",
    "!python MRgraphset.py undirected_toy.txt --indx=1 --quiet > undirected_ready.txt\n",
    "!cat undirected_ready.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"2\": 1, \"6\": 1}, 0, [], \"Q\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "3\t[{\"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "4\t[{\"2\": 1, \"5\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n"
     ]
    }
   ],
   "source": [
    "# run the MRJob class to create initial graph\n",
    "# that we'll iterate through\n",
    "!python MRgraphset.py directed_toy.txt --indx=1 --quiet > directed_ready.txt\n",
    "!cat directed_ready.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRshortest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRshortest.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import copy\n",
    "  \n",
    "class MRshortest(MRJob):\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer)] \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # performs an action based on the\n",
    "    # status of the node\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into the node\n",
    "        # and the payload that has so\n",
    "        # much information\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        payload = ast.literal_eval(line[1])\n",
    "\n",
    "        # split up the payload into its\n",
    "        # component parts\n",
    "        outlinks = payload[0]\n",
    "        dist = int(payload[1])\n",
    "        path = payload[2]\n",
    "        status = payload[3]\n",
    "        \n",
    "        # if we're not dealing with a node in\n",
    "        # status 'Q'\n",
    "        if status == 'Q':\n",
    "            \n",
    "            # the distance to the outlinks is\n",
    "            # the distance to the current node\n",
    "            # plus 1 \n",
    "            edge_dist = dist + 1\n",
    "            \n",
    "            # we don't know the outlinks for the \n",
    "            # outlinks we're about to emit\n",
    "            edge_outlinks = {}\n",
    "            \n",
    "            # we append to the existing path\n",
    "            # the node that brought us here\n",
    "            edge_path = copy.deepcopy(path)\n",
    "            edge_path.append(node)\n",
    "            \n",
    "            # the status for the next edge is\n",
    "            # Q because we will look at that\n",
    "            # next\n",
    "            edge_status = 'Q'\n",
    "            \n",
    "            # loop through the outedges\n",
    "            for edge in outlinks:\n",
    "                \n",
    "                # set the key, value pair\n",
    "                # we emit for each queued up\n",
    "                # node\n",
    "                edge_key = edge\n",
    "                edge_payload = (edge_outlinks,\\\n",
    "                                edge_dist,\\\n",
    "                                edge_path,\\\n",
    "                                edge_status)\n",
    "                \n",
    "                # emit the next in line for\n",
    "                # queueing\n",
    "                yield int(edge_key), edge_payload\n",
    "                \n",
    "            # set a new status for this node,\n",
    "            # visited\n",
    "            new_status = 'V'\n",
    "            \n",
    "            # set the key,value pair for this\n",
    "            # original node\n",
    "            new_key = node\n",
    "            new_value = (outlinks,dist,path,new_status)\n",
    "            yield int(new_key),new_value\n",
    "\n",
    "        # else if the node is not in queue\n",
    "        # status, then simply yield it as is\n",
    "        else:\n",
    "            key = node\n",
    "            value = (outlinks,dist,path,status)\n",
    "            yield int(key),value\n",
    "            \n",
    "        \n",
    "    # our reducer merges the outputs \n",
    "    # from nodes of various statuses\n",
    "    # and yields a single line for each\n",
    "    # node\n",
    "    def reducer(self, node, payloads):\n",
    "\n",
    "        # initalize storage variables\n",
    "        # that will keep track of the \n",
    "        # everything for all the nodes\n",
    "        node = node\n",
    "        new_outlinks = []\n",
    "        new_dists = []\n",
    "        new_paths = []\n",
    "        statuses = []\n",
    "\n",
    "        # initalize the final variables\n",
    "        # that we'll use to output\n",
    "        final_outlinks = {}\n",
    "        final_dist = None\n",
    "        final_path = None\n",
    "        final_status = None\n",
    "\n",
    "        # loop through the payloads\n",
    "        for payload in payloads:\n",
    "\n",
    "            # gather all the information\n",
    "            # from the payload\n",
    "            outlinks = payload[0]\n",
    "            dist = payload[1]\n",
    "            path = payload[2]\n",
    "            status = payload[3]\n",
    "\n",
    "            # update the variables for\n",
    "            # each node\n",
    "            new_outlinks.append(outlinks)\n",
    "            new_dists.append(dist)\n",
    "            new_paths.append(path)\n",
    "            statuses.append(status)\n",
    "\n",
    "        # look at the status to determine\n",
    "        # what to do; let's start with if\n",
    "        # we have a 'V'\n",
    "        if 'V' in statuses:\n",
    "\n",
    "            # get the index of the \"v\" status\n",
    "            index = statuses.index('V')\n",
    "\n",
    "            # set the final variables based\n",
    "            # on the node we've already \n",
    "            # visited\n",
    "            final_outlinks = new_outlinks[index]\n",
    "            final_dist = new_dists[index]\n",
    "            final_path = new_paths[index]\n",
    "            final_status = statuses[index]\n",
    "        \n",
    "        # else if we have a 'Q'\n",
    "        elif 'Q' in statuses:\n",
    "            \n",
    "            # find the unvisited and queued\n",
    "            # indices\n",
    "            q_index = statuses.index('Q')\n",
    "            \n",
    "            # only add outlinks if we have 'U'\n",
    "            # otherwise, we should just add\n",
    "            # blank outlinks as we are going\n",
    "            # outside of our graph     \n",
    "            if 'U' in statuses:\n",
    "\n",
    "                u_index = statuses.index('U')\n",
    "\n",
    "                # get the final outlinks from the \n",
    "                # unvisited node\n",
    "                final_outlinks = new_outlinks[u_index]\n",
    "\n",
    "            # else put up blank outlinks\n",
    "            else: \n",
    "                final_outlinks = {}\n",
    "\n",
    "            \n",
    "\n",
    "            # get the distance and the path and\n",
    "            # the status from 'queued' node\n",
    "            final_dist = new_dists[q_index]\n",
    "            final_path = new_paths[q_index]\n",
    "            final_status = statuses[q_index]\n",
    "            \n",
    "        # else, we must have only an unvisited\n",
    "        # node, and we'll take all the information\n",
    "        # from the unvisited node\n",
    "        else:\n",
    "            \n",
    "            # set the index based on the\n",
    "            # unvisited node\n",
    "            index = statuses.index('U')\n",
    "            \n",
    "            # get the final variables all from\n",
    "            # the unvisited node\n",
    "            final_outlinks = new_outlinks[index]\n",
    "            final_dist = new_dists[index]\n",
    "            final_path = new_paths[index]\n",
    "            final_status = statuses[index]\n",
    "            \n",
    "            \n",
    "        # set the key value that we'll be\n",
    "        # outputting for each node, only one for\n",
    "        # each node\n",
    "        key = node\n",
    "        value = (final_outlinks,\\\n",
    "                 final_dist,\\\n",
    "                 final_path,\\\n",
    "                 final_status)\n",
    "        \n",
    "        # yield the key-value pair\n",
    "        yield key,value\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRshortest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"2\": 1, \"5\": 1}, 0, [], \"V\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"5\": 1, \"4\": 1}, 1, [1], \"Q\"]\r\n",
      "3\t[{\"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "4\t[{\"3\": 1, \"2\": 1, \"5\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 1, [1], \"Q\"]\r\n"
     ]
    }
   ],
   "source": [
    "# unit test for a single go\n",
    "!python MRshortest.py undirected_ready.txt --quiet > temp.txt\n",
    "!cat temp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"2\": 1, \"5\": 1}, 0, [], \"V\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"5\": 1, \"4\": 1}, 1, [1], \"V\"]\r\n",
      "3\t[{\"2\": 1, \"4\": 1}, 2, [1, 2], \"Q\"]\r\n",
      "4\t[{\"3\": 1, \"2\": 1, \"5\": 1}, 2, [1, 2], \"Q\"]\r\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 1, [1], \"V\"]\r\n"
     ]
    }
   ],
   "source": [
    "# keep unit testing one iteration at a time\n",
    "!python MRshortest.py temp.txt --quiet > temp2.txt\n",
    "!cat temp2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"2\": 1, \"5\": 1}, 0, [], \"V\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"5\": 1, \"4\": 1}, 1, [1], \"V\"]\r\n",
      "3\t[{\"2\": 1, \"4\": 1}, 2, [1, 2], \"V\"]\r\n",
      "4\t[{\"3\": 1, \"2\": 1, \"5\": 1}, 2, [1, 2], \"V\"]\r\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 1, [1], \"V\"]\r\n"
     ]
    }
   ],
   "source": [
    "# keep unit testing one iteration at a time\n",
    "!python MRshortest.py temp2.txt --quiet > temp3.txt\n",
    "!cat temp3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a driver and run on both the directed and undirected toy files\n",
    "We've now shown how we can iterate through the toy graphs. We'll now do so with a driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undirected Ready\n",
      "1\t[{'2': 1, '5': 1}, 0, [], 'Q']\n",
      "2\t[{'1': 1, '3': 1, '5': 1, '4': 1}, 9223372036854775807, [], 'U']\n",
      "3\t[{'2': 1, '4': 1}, 9223372036854775807, [], 'U']\n",
      "4\t[{'3': 1, '2': 1, '5': 1}, 9223372036854775807, [], 'U']\n",
      "5\t[{'1': 1, '2': 1, '4': 1}, 9223372036854775807, [], 'U']\n",
      "\n",
      "Directed Ready\n",
      "1\t[{'2': 1, '6': 1}, 0, [], 'Q']\n",
      "2\t[{'1': 1, '3': 1, '4': 1}, 9223372036854775807, [], 'U']\n",
      "3\t[{'2': 1, '4': 1}, 9223372036854775807, [], 'U']\n",
      "4\t[{'2': 1, '5': 1}, 9223372036854775807, [], 'U']\n",
      "5\t[{'1': 1, '2': 1, '4': 1}, 9223372036854775807, [], 'U']\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRgraphset import MRgraphset\n",
    "\n",
    "###\n",
    "# prepare the undirected data\n",
    "###\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job1 = MRgraphset(args=['undirected_toy.txt',\\\n",
    "                           '--indx=1']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job1.make_runner() as runner:\n",
    "    runner.run()\n",
    "    \n",
    "    # create the file we will output to\n",
    "    with open('undirected_ready.txt','w') as myfile:\n",
    "        \n",
    "        # run the runner and send each\n",
    "        # line to the file\n",
    "        for line in runner.stream_output():\n",
    "            \n",
    "            # grab the key,value\n",
    "            key,value =  mr_job1.parse_output_line(line)\n",
    "            \n",
    "            # write to the file\n",
    "            info = str(key) + \"\\t\" + str(value) + \"\\n\"\n",
    "            myfile.write(info)\n",
    "\n",
    "# show the output\n",
    "!echo Undirected Ready\n",
    "!cat undirected_ready.txt\n",
    "            \n",
    "###\n",
    "# prepare the directed data\n",
    "###\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job1 = MRgraphset(args=['directed_toy.txt',\\\n",
    "                           '--indx=1']) \n",
    "\n",
    "# create the runner and run it\n",
    "with mr_job1.make_runner() as runner:\n",
    "    runner.run()\n",
    "    \n",
    "    # create the file we will output to\n",
    "    with open('directed_ready.txt','w') as myfile:\n",
    "        \n",
    "        # run the runner and send each\n",
    "        # line to the file\n",
    "        for line in runner.stream_output():\n",
    "            \n",
    "            # grab the key,value\n",
    "            key,value =  mr_job1.parse_output_line(line)\n",
    "            \n",
    "            # write to the file\n",
    "            info = str(key) + \"\\t\" + str(value) + \"\\n\"\n",
    "            myfile.write(info)\n",
    "\n",
    "# show the output            \n",
    "!echo\n",
    "!echo Directed Ready\n",
    "!cat directed_ready.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step through the UNDIRECTED prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{'2': 1, '5': 1}, 0, [], 'V']\r\n",
      "2\t[{'1': 1, '3': 1, '5': 1, '4': 1}, 1, [1], 'V']\r\n",
      "3\t[{'2': 1, '4': 1}, 2, [1, 2], 'V']\r\n",
      "4\t[{'3': 1, '2': 1, '5': 1}, 2, [1, 2], 'V']\r\n",
      "5\t[{'1': 1, '2': 1, '4': 1}, 1, [1], 'V']\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRshortest import MRshortest\n",
    "\n",
    "# copy the prepared file into a \n",
    "# temporary file that we'll keep\n",
    "# overwriting\n",
    "!cp undirected_ready.txt undirected_temp.txt\n",
    "\n",
    "###\n",
    "# prepare the undirected data\n",
    "###\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRshortest(args=['undirected_temp.txt']) \n",
    "\n",
    "# set the q_count equal to 1 and \n",
    "# keep running the runner until\n",
    "# we have nothing in queue\n",
    "q_count = 1\n",
    "while (q_count > 0):\n",
    "    \n",
    "    # set the q_count to 0\n",
    "    q_count = 0\n",
    "    \n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        # create the file we will output to\n",
    "        with open('undirected_temp.txt','w') as myfile:\n",
    "\n",
    "            # run the runner and send each\n",
    "            # line to the file\n",
    "            for line in runner.stream_output():\n",
    "\n",
    "                # grab the key,value\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                \n",
    "                # grab the status\n",
    "                status = value[3]\n",
    "                \n",
    "                # if it's a q, increment the \n",
    "                # q_count\n",
    "                if status == 'Q':\n",
    "                    q_count = q_count + 1\n",
    "                \n",
    "                # write to the file\n",
    "                info = str(key) + \"\\t\" + str(value) + \"\\n\"\n",
    "                myfile.write(info)\n",
    "\n",
    "# check out the output file, everything\n",
    "# should be visited\n",
    "!cat undirected_temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step through the DIRECTED preprared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{'2': 1, '6': 1}, 0, [], 'V']\r\n",
      "2\t[{'1': 1, '3': 1, '4': 1}, 1, [1], 'V']\r\n",
      "3\t[{'2': 1, '4': 1}, 2, [1, 2], 'V']\r\n",
      "4\t[{'2': 1, '5': 1}, 2, [1, 2], 'V']\r\n",
      "5\t[{'1': 1, '2': 1, '4': 1}, 3, [1, 2, 4], 'V']\r\n",
      "6\t[{}, 1, [1], 'V']\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRshortest import MRshortest\n",
    "\n",
    "# copy the prepared file into a \n",
    "# temporary file that we'll keep\n",
    "# overwriting\n",
    "!cp directed_ready.txt directed_temp.txt\n",
    "\n",
    "###\n",
    "# prepare the undirected data\n",
    "###\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRshortest(args=['directed_temp.txt']) \n",
    "\n",
    "# set the q_count equal to 1 and \n",
    "# keep running the runner until\n",
    "# we have nothing in queue\n",
    "q_count = 1\n",
    "while (q_count > 0):\n",
    "    \n",
    "    # set the q_count to 0\n",
    "    q_count = 0\n",
    "    \n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        # create the file we will output to\n",
    "        with open('directed_temp.txt','w') as myfile:\n",
    "\n",
    "            # run the runner and send each\n",
    "            # line to the file\n",
    "            for line in runner.stream_output():\n",
    "\n",
    "                # grab the key,value\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                \n",
    "                # grab the status\n",
    "                status = value[3]\n",
    "                \n",
    "                # if it's a q, increment the \n",
    "                # q_count\n",
    "                if status == 'Q':\n",
    "                    q_count = q_count + 1\n",
    "                \n",
    "                # write to the file\n",
    "                info = str(key) + \"\\t\" + str(value) + \"\\n\"\n",
    "                myfile.write(info)\n",
    "\n",
    "# check out the output file, everything\n",
    "# should be visited\n",
    "!cat directed_temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 7.1: Exploratory data analysis (NLTK synonyms)\n",
    "\n",
    "Using MRJob, explore the synonyms network data.\n",
    "Consider plotting the degree distribution (does it follow a power law?),\n",
    "and determine some of the key features, like:\n",
    "\n",
    "- number of nodes, \n",
    "- number links,\n",
    "- or the average degree (i.e., the average number of links per node),\n",
    "- etc...\n",
    "\n",
    "As you develop your code, please be sure to run it locally first (though not on the whole dataset). \n",
    "Once you have gotten you code to run locally, deploy it on AWS as a systems test\n",
    "in preparation for our next dataset (which will require AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL: Number of nodes, links, and average degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRexplore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRexplore.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRexplore(MRJob):\n",
    "    \n",
    "    # set some global variables\n",
    "    total_nodes = 0\n",
    "    total_edges = 0\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we set a single reducer so that we\n",
    "        # can take the total edges over the\n",
    "        # total nodes\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       reducer=self.reducer,\\\n",
    "                       reducer_final=self.reducer_final)]        \n",
    "    \n",
    "    \n",
    "    # the mapper outputs for each node\n",
    "    # a node count and the number of links\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        num_edges = len(edges)\n",
    "        \n",
    "        # yield the node with count 1\n",
    "        # and the number of edges\n",
    "        yield 'Node',1\n",
    "        yield 'Edges',num_edges\n",
    "        \n",
    "    \n",
    "    # the reducer sums across the nodes and edges \n",
    "    # and updates the global variables\n",
    "    def reducer(self, label, counts):\n",
    "        \n",
    "        # sum the counts\n",
    "        sum_counts = sum(counts)\n",
    "        \n",
    "        # if we have the nodes, set the\n",
    "        # total nodes\n",
    "        if label == 'Node':\n",
    "            self.total_nodes = sum_counts\n",
    "            \n",
    "        # else set the total edges\n",
    "        else:\n",
    "            self.total_edges = sum_counts\n",
    "            \n",
    "    \n",
    "    # the reducer final takes the total nodes and \n",
    "    # edges and divides to get the average\n",
    "    def reducer_final(self):\n",
    "        \n",
    "        # yield the edges and nodes\n",
    "        yield \"Total Nodes:\", self.total_nodes\n",
    "        yield \"Total Edges:\", self.total_edges\n",
    "        \n",
    "        # calculate the average\n",
    "        avg_edges = float(self.total_edges) / float(self.total_nodes)\n",
    "        yield \"Average Degrees:\", avg_edges\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRexplore.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our exploratory analysis yields:\n",
      "\"Total Nodes:\"\t8271\n",
      "\"Total Edges:\"\t61134\n",
      "\"Average Degrees:\"\t7.391367428364164\n"
     ]
    }
   ],
   "source": [
    "# run the exploratory data anlaysis on\n",
    "# our data\n",
    "!echo Our exploratory analysis yields:\n",
    "!python MRexplore.py synNet/synNet.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Number of nodes, links, and average degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/no_script.Alex.20160709.230736.149005\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/no_script.Alex.20160709.230736.149005/files/...\n",
      "j-2CVJI0SC449S9\n"
     ]
    }
   ],
   "source": [
    "# create the cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-1 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: synNet/synNet.txt to s3://aks-w261-hw7/synNet.txt\r\n"
     ]
    }
   ],
   "source": [
    "# upload the NLTK file to S3\n",
    "!aws s3 cp synNet/synNet.txt s3://aks-w261-hw7/synNet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our exploratory analysis yields:\n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRexplore.Alex.20160709.232520.874079\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/MRexplore.Alex.20160709.232520.874079/files/...\n",
      "Adding our job to existing cluster j-2CVJI0SC449S9\n",
      "Waiting for step 1 of 1 (s-3SC4NKHAOBYXB) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40770/cluster\n",
      "  RUNNING for 4.7s\n",
      "  RUNNING for 37.4s\n",
      "     5.0% complete\n",
      "  RUNNING for 68.6s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3SC4NKHAOBYXB on ec2-54-153-88-129.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-153-88-129.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3SC4NKHAOBYXB/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1001102\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=78\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26252\n",
      "\t\tFILE: Number of bytes written=3471387\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2560\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=1001102\n",
      "\t\tS3: Number of bytes written=78\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=32\n",
      "\t\tLaunched map tasks=32\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1129819680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=32333760\n",
      "\t\tTotal time spent by all map tasks (ms)=784597\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35306865\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11227\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1010430\n",
      "\t\tTotal vcore-seconds taken by all map tasks=784597\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11227\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=38750\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=9690\n",
      "\t\tInput split bytes=2560\n",
      "\t\tMap input records=8271\n",
      "\t\tMap output bytes=159033\n",
      "\t\tMap output materialized bytes=28741\n",
      "\t\tMap output records=16542\n",
      "\t\tMerged Map outputs=32\n",
      "\t\tPhysical memory (bytes) snapshot=15886327808\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce input records=16542\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=28741\n",
      "\t\tShuffled Maps =32\n",
      "\t\tSpilled Records=33084\n",
      "\t\tTotal committed heap usage (bytes)=17710972928\n",
      "\t\tVirtual memory (bytes) snapshot=66250878976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from s3://aks-w261-hw7/out_7-1/...\n",
      "\"Total Nodes:\"\t8271\n",
      "\"Total Edges:\"\t61134\n",
      "\"Average Degrees:\"\t7.391367428364164\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/MRexplore.Alex.20160709.232520.874079/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRexplore.Alex.20160709.232520.874079...\n",
      "Killing our SSH tunnel (pid 20988)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!echo Our exploratory analysis yields:\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-1\n",
    "!python MRexplore.py -r emr s3://aks-w261-hw7/synNet.txt \\\n",
    "    --cluster-id=j-2CVJI0SC449S9 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/out_7-1 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL: Degrees distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MRdegdist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRdegdist.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRdegdist(MRJob):\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we set a single reducer so that we\n",
    "        # can easily take a single file and \n",
    "        # load it into matplot lib later\n",
    "        # if we find this approach doesn't\n",
    "        # scale, we can always up this without\n",
    "        # affecting the job's functionality\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       combiner=self.reducer,\\\n",
    "                       reducer=self.reducer)]        \n",
    "    \n",
    "    \n",
    "    # the mapper outputs for each node\n",
    "    # athe number of edges and 1\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        num_edges = len(edges)\n",
    "        \n",
    "        # yield the node with count 1\n",
    "        # and the number of edges\n",
    "        yield num_edges,1\n",
    "        \n",
    "    \n",
    "    # the reducer calculates how many\n",
    "    # counts we have for each edge\n",
    "    def reducer(self, num_edges, counts):\n",
    "        \n",
    "        # yield the number of edges and\n",
    "        # the total number of nodes that\n",
    "        # have that many edges\n",
    "        yield num_edges, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRdegdist.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our degree distribution yields (sample):\n",
      "1\t1421\n",
      "10\t254\n",
      "11\t200\n",
      "110\t2\n",
      "117\t1\n",
      "12\t158\n",
      "127\t1\n",
      "13\t135\n",
      "131\t1\n",
      "14\t111\n"
     ]
    }
   ],
   "source": [
    "# run the degree distribution anlaysis on\n",
    "# our data\n",
    "!echo Our degree distribution yields \\(sample\\):\n",
    "!python MRdegdist.py synNet/synNet.txt --quiet > synNet/degDist.txt\n",
    "!head synNet/degDist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAJZCAYAAAAZJaRRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGkii0iSJEmSJEma6L+Wckga86vHqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e794b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import pandas and matplotlib to load\n",
    "# the data and plot the histogram\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# read in the data as a data frame\n",
    "data = pd.read_table('synNet/degDist.txt',header=None)\n",
    "data_count = data.iloc[:,1]\n",
    "data_lengt = data.iloc[:,0]\n",
    "\n",
    "# gives histogram bars a width\n",
    "width = 1.0 \n",
    "\n",
    "# plot the histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(data_lengt,data_count, width, color='b')\n",
    "plt.title(\"Distribution of NLTK Degrees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this does appear to follow a power law. The vast majority of data points are clustered around a single point with a very long tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Degrees distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our degree distribution on the cloud completes successfully:\n",
      "delete: s3://aks-w261-hw7/out_7-1/_SUCCESS\n",
      "delete: s3://aks-w261-hw7/out_7-1/part-00000     \n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRdegdist.Alex.20160709.233257.566325\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/MRdegdist.Alex.20160709.233257.566325/files/...\n",
      "Adding our job to existing cluster j-2CVJI0SC449S9\n",
      "Waiting for step 1 of 1 (s-1FOIGLZ70VL4I) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40770/cluster\n",
      "  RUNNING for 11.4s\n",
      "   100.0% complete\n",
      "  RUNNING for 43.7s\n",
      "     5.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1FOIGLZ70VL4I on ec2-54-153-88-129.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-153-88-129.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1FOIGLZ70VL4I/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=994902\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=470\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2653\n",
      "\t\tFILE: Number of bytes written=3436705\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2560\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=994902\n",
      "\t\tS3: Number of bytes written=470\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=32\n",
      "\t\tLaunched map tasks=32\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1060603200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=27008640\n",
      "\t\tTotal time spent by all map tasks (ms)=736530\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33143850\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9378\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=844020\n",
      "\t\tTotal vcore-seconds taken by all map tasks=736530\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9378\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=39670\n",
      "\t\tCombine input records=8271\n",
      "\t\tCombine output records=972\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=10029\n",
      "\t\tInput split bytes=2560\n",
      "\t\tMap input records=8271\n",
      "\t\tMap output bytes=34968\n",
      "\t\tMap output materialized bytes=5962\n",
      "\t\tMap output records=8271\n",
      "\t\tMerged Map outputs=32\n",
      "\t\tPhysical memory (bytes) snapshot=15863590912\n",
      "\t\tReduce input groups=83\n",
      "\t\tReduce input records=972\n",
      "\t\tReduce output records=83\n",
      "\t\tReduce shuffle bytes=5962\n",
      "\t\tShuffled Maps =32\n",
      "\t\tSpilled Records=1944\n",
      "\t\tTotal committed heap usage (bytes)=18084790272\n",
      "\t\tVirtual memory (bytes) snapshot=66213998592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/MRdegdist.Alex.20160709.233257.566325/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRdegdist.Alex.20160709.233257.566325...\n",
      "Killing our SSH tunnel (pid 21013)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!echo Our degree distribution on the cloud completes successfully:\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-1\n",
    "!python MRdegdist.py -r emr s3://aks-w261-hw7/synNet.txt \\\n",
    "    --cluster-id=j-2CVJI0SC449S9 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/out_7-1 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 7.2: Shortest path graph distances (NLTK synonyms)\n",
    "\n",
    "Write (reuse your code from 7.0) an MRJob class to find shortest path graph distances, \n",
    "and apply it to the NLTK synonyms network dataset. \n",
    "\n",
    "Prove your code's function by running the job:\n",
    "\n",
    "- shortest path starting at \"walk\" (index=7827) and ending at \"make\" (index=536),\n",
    "\n",
    "and showing you code's output. Once again, your output should include the path and the distance.\n",
    "\n",
    "As you develop your code, please be sure to run it locally first (though not on the whole dataset). \n",
    "Once you have gotten you code to run locally, deploy it on AWS as a systems test\n",
    "in preparation for our next dataset (which will require AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL: Prepare the data\n",
    "We set the source node and develop the adjacency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRgraphset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRgraphset.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRgraphset(MRJob):\n",
    "    \n",
    "    # set the source node\n",
    "    source = None\n",
    "    \n",
    "    # configure the options so that we can \n",
    "    # pass in the source node of interest\n",
    "    def configure_options(self):\n",
    "        super(MRgraphset, self).configure_options()\n",
    "        self.add_passthrough_option(\"--indx\", type='int', default=1)\n",
    "\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\\\n",
    "                        mapper=self.mapper)] \n",
    "    \n",
    "    \n",
    "    # the mapper init sets the source node\n",
    "    def mapper_init(self):\n",
    "        self.source = self.options.indx        \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # outputs the node, its path to \n",
    "    # the source, its distance to the \n",
    "    # source, it's linked nodes, and its\n",
    "    # status as visited\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        \n",
    "        # initalize distance, path, and\n",
    "        # status\n",
    "        dist = 0\n",
    "        path = []\n",
    "        status = None\n",
    "        \n",
    "        # if this is the source node\n",
    "        if node == self.source:\n",
    "            \n",
    "            # set the distance to 0\n",
    "            # the path to none and \n",
    "            # the status to 'Q' for\n",
    "            # queue\n",
    "            dist = 0\n",
    "            path = []\n",
    "            status = 'Q'\n",
    "            \n",
    "        # else if this is not the source node\n",
    "        else:\n",
    "            \n",
    "            # set the distance to a max\n",
    "            # value, the path to null, and \n",
    "            # the status to 'U' for unvisited\n",
    "            dist = sys.maxint\n",
    "            path = []\n",
    "            status = 'U'\n",
    "            \n",
    "        # for each node yield the initial\n",
    "        # graph state\n",
    "        key = node\n",
    "        value = (edges,dist,path,status)\n",
    "        yield key,value\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRgraphset.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"3\": 1, \"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"310\": 1, \"4\": 1, \"311\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "3\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "4\t[{\"1\": 1, \"3\": 1, \"2\": 1, \"311\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "5\t[{\"6\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "6\t[{\"5\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "7\t[{\"9\": 1, \"8\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "8\t[{\"9\": 1, \"7\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "9\t[{\"8\": 1, \"124\": 1, \"7\": 1, \"1316\": 1}, 9223372036854775807, [], \"U\"]\r\n",
      "10\t[{\"11\": 1, \"13\": 1, \"12\": 1, \"15\": 1, \"14\": 1, \"17\": 1, \"16\": 1}, 9223372036854775807, [], \"U\"]\r\n"
     ]
    }
   ],
   "source": [
    "# run the MRJob class to create initial graph\n",
    "# that we'll iterate through\n",
    "!python MRgraphset.py synNet/synNet.txt --indx=7827 --quiet > synNet/synNet_ready.txt\n",
    "!head synNet/synNet_ready.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL: Step through our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRshortest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRshortest.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import copy\n",
    "  \n",
    "class MRshortest(MRJob):\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer)] \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # performs an action based on the\n",
    "    # status of the node\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into the node\n",
    "        # and the payload that has so\n",
    "        # much information\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        payload = ast.literal_eval(line[1])\n",
    "\n",
    "        # split up the payload into its\n",
    "        # component parts\n",
    "        outlinks = payload[0]\n",
    "        dist = int(payload[1])\n",
    "        path = payload[2]\n",
    "        status = payload[3]\n",
    "        \n",
    "        # if we're not dealing with a node in\n",
    "        # status 'Q'\n",
    "        if status == 'Q':\n",
    "            \n",
    "            # the distance to the outlinks is\n",
    "            # the distance to the current node\n",
    "            # plus 1 \n",
    "            edge_dist = dist + 1\n",
    "            \n",
    "            # we don't know the outlinks for the \n",
    "            # outlinks we're about to emit\n",
    "            edge_outlinks = {}\n",
    "            \n",
    "            # we append to the existing path\n",
    "            # the node that brought us here\n",
    "            edge_path = copy.deepcopy(path)\n",
    "            edge_path.append(node)\n",
    "            \n",
    "            # the status for the next edge is\n",
    "            # Q because we will look at that\n",
    "            # next\n",
    "            edge_status = 'Q'\n",
    "            \n",
    "            # loop through the outedges\n",
    "            for edge in outlinks:\n",
    "                \n",
    "                # set the key, value pair\n",
    "                # we emit for each queued up\n",
    "                # node\n",
    "                edge_key = edge\n",
    "                edge_payload = (edge_outlinks,\\\n",
    "                                edge_dist,\\\n",
    "                                edge_path,\\\n",
    "                                edge_status)\n",
    "                \n",
    "                # emit the next in line for\n",
    "                # queueing\n",
    "                yield int(edge_key), edge_payload\n",
    "                \n",
    "            # set a new status for this node,\n",
    "            # visited\n",
    "            new_status = 'V'\n",
    "            \n",
    "            # set the key,value pair for this\n",
    "            # original node\n",
    "            new_key = node\n",
    "            new_value = (outlinks,dist,path,new_status)\n",
    "            yield int(new_key),new_value\n",
    "\n",
    "        # else if the node is not in queue\n",
    "        # status, then simply yield it as is\n",
    "        else:\n",
    "            key = node\n",
    "            value = (outlinks,dist,path,status)\n",
    "            yield int(key),value\n",
    "            \n",
    "        \n",
    "    # our reducer merges the outputs \n",
    "    # from nodes of various statuses\n",
    "    # and yields a single line for each\n",
    "    # node\n",
    "    def reducer(self, node, payloads):\n",
    "\n",
    "        # initalize storage variables\n",
    "        # that will keep track of the \n",
    "        # everything for all the nodes\n",
    "        node = node\n",
    "        new_outlinks = []\n",
    "        new_dists = []\n",
    "        new_paths = []\n",
    "        statuses = []\n",
    "\n",
    "        # initalize the final variables\n",
    "        # that we'll use to output\n",
    "        final_outlinks = {}\n",
    "        final_dist = None\n",
    "        final_path = None\n",
    "        final_status = None\n",
    "\n",
    "        # loop through the payloads\n",
    "        for payload in payloads:\n",
    "\n",
    "            # gather all the information\n",
    "            # from the payload\n",
    "            outlinks = payload[0]\n",
    "            dist = payload[1]\n",
    "            path = payload[2]\n",
    "            status = payload[3]\n",
    "\n",
    "            # update the variables for\n",
    "            # each node\n",
    "            new_outlinks.append(outlinks)\n",
    "            new_dists.append(dist)\n",
    "            new_paths.append(path)\n",
    "            statuses.append(status)\n",
    "\n",
    "        # look at the status to determine\n",
    "        # what to do; let's start with if\n",
    "        # we have a 'V'\n",
    "        if 'V' in statuses:\n",
    "\n",
    "            # get the index of the \"v\" status\n",
    "            index = statuses.index('V')\n",
    "\n",
    "            # set the final variables based\n",
    "            # on the node we've already \n",
    "            # visited\n",
    "            final_outlinks = new_outlinks[index]\n",
    "            final_dist = new_dists[index]\n",
    "            final_path = new_paths[index]\n",
    "            final_status = statuses[index]\n",
    "        \n",
    "        # else if we have a 'Q'\n",
    "        elif 'Q' in statuses:\n",
    "            \n",
    "            # find the unvisited and queued\n",
    "            # indices\n",
    "            q_index = statuses.index('Q')\n",
    "            \n",
    "            # only add outlinks if we have 'U'\n",
    "            # otherwise, we should just add\n",
    "            # blank outlinks as we are going\n",
    "            # outside of our graph     \n",
    "            if 'U' in statuses:\n",
    "\n",
    "                u_index = statuses.index('U')\n",
    "\n",
    "                # get the final outlinks from the \n",
    "                # unvisited node\n",
    "                final_outlinks = new_outlinks[u_index]\n",
    "\n",
    "            # else put up blank outlinks\n",
    "            else: \n",
    "                final_outlinks = {}\n",
    "\n",
    "            \n",
    "\n",
    "            # get the distance and the path and\n",
    "            # the status from 'queued' node\n",
    "            final_dist = new_dists[q_index]\n",
    "            final_path = new_paths[q_index]\n",
    "            final_status = statuses[q_index]\n",
    "            \n",
    "        # else, we must have only an unvisited\n",
    "        # node, and we'll take all the information\n",
    "        # from the unvisited node\n",
    "        else:\n",
    "            \n",
    "            # set the index based on the\n",
    "            # unvisited node\n",
    "            index = statuses.index('U')\n",
    "            \n",
    "            # get the final variables all from\n",
    "            # the unvisited node\n",
    "            final_outlinks = new_outlinks[index]\n",
    "            final_dist = new_dists[index]\n",
    "            final_path = new_paths[index]\n",
    "            final_status = statuses[index]\n",
    "            \n",
    "            \n",
    "        # set the key value that we'll be\n",
    "        # outputting for each node, only one for\n",
    "        # each node\n",
    "        key = node\n",
    "        value = (final_outlinks,\\\n",
    "                 final_dist,\\\n",
    "                 final_path,\\\n",
    "                 final_status)\n",
    "        \n",
    "        # yield the key-value pair\n",
    "        yield key,value\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRshortest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance for node of interest from source:\n",
      "3\n",
      "Path to reach node of interest from source:\n",
      "[7827, 1426, 1668]\n",
      "\n",
      "\n",
      "*~*~*~*~*\n",
      "\n",
      "\n",
      "Sample of our graph\n",
      "1\t[{'3': 1, '2': 1, '4': 1}, 7, [7827, 1426, 3480, 1030, 586, 310, 2], 'V']\n",
      "10\t[{'11': 1, '13': 1, '12': 1, '15': 1, '14': 1, '17': 1, '16': 1}, 4, [7827, 1426, 1685, 17], 'V']\n",
      "100\t[{'3827': 1, '2872': 1, '2871': 1, '2870': 1, '2873': 1, '2875': 1, '2874': 1, '3122': 1, '2443': 1, '617': 1, '4647': 1, '2441': 1, '3722': 1, '5550': 1, '5336': 1, '93': 1, '92': 1, '3720': 1, '360': 1}, 5, [7827, 1426, 1038, 3833, 93], 'V']\n",
      "1000\t[{'997': 1, '999': 1, '1004': 1, '1003': 1, '1002': 1, '1001': 1}, 9223372036854775807, [], 'U']\n",
      "1001\t[{'997': 1, '999': 1, '998': 1, '1000': 1}, 9223372036854775807, [], 'U']\n",
      "1002\t[{'997': 1, '999': 1, '998': 1, '1000': 1}, 9223372036854775807, [], 'U']\n",
      "1003\t[{'2095': 1, '997': 1, '999': 1, '998': 1, '1000': 1}, 9223372036854775807, [], 'U']\n",
      "1004\t[{'997': 1, '3352': 1, '999': 1, '998': 1, '1000': 1}, 9223372036854775807, [], 'U']\n",
      "1005\t[{'1006': 1, '948': 1}, 6, [7827, 1426, 1429, 3241, 1997, 948], 'V']\n",
      "1006\t[{'1005': 1, '948': 1}, 6, [7827, 1426, 1429, 3241, 1997, 948], 'V']\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRshortest import MRshortest\n",
    "\n",
    "# copy the prepared file into a \n",
    "# temporary file that we'll keep\n",
    "# overwriting\n",
    "!cp synNet/synNet_ready.txt synNet/synNet_temp.txt\n",
    "\n",
    "# set the data that we're going to pull\n",
    "mr_job = MRshortest(args=['synNet/synNet_temp.txt']) \n",
    "\n",
    "# set the index of interest and initalize\n",
    "# values to hold the distance and the path\n",
    "INTEREST = 536\n",
    "dist = None\n",
    "path = None\n",
    "\n",
    "# set the q_count equal to 1 and \n",
    "# keep running the runner until\n",
    "# we have nothing in queue\n",
    "q_count = 1\n",
    "while (q_count > 0):\n",
    "    \n",
    "    # set the q_count to 0\n",
    "    q_count = 0\n",
    "    \n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        # create the file we will output to\n",
    "        with open('synNet/synNet_temp.txt','w') as myfile:\n",
    "\n",
    "            # run the runner and send each\n",
    "            # line to the file\n",
    "            for line in runner.stream_output():\n",
    "\n",
    "                # grab the key,value\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                \n",
    "                # grab the status\n",
    "                status = value[3]\n",
    "                \n",
    "                # if it's a q, increment the \n",
    "                # q_count\n",
    "                if status == 'Q':\n",
    "                    q_count = q_count + 1\n",
    "                \n",
    "                # if we've got to our one of interest\n",
    "                if key == INTEREST:\n",
    "                    dist = value[1]\n",
    "                    path = value[2]\n",
    "                \n",
    "                # write to the file\n",
    "                info = str(key) + \"\\t\" + str(value) + \"\\n\"\n",
    "                myfile.write(info)\n",
    "\n",
    "# print out the path and the distance\n",
    "# to the source node\n",
    "print \"Distance for node of interest from source:\"\n",
    "print dist\n",
    "print \"Path to reach node of interest from source:\"\n",
    "print path\n",
    "print \"\\n\"\n",
    "print \"*~*~*~*~*\"\n",
    "print \"\\n\"\n",
    "\n",
    "# write the path to a file for later joining\n",
    "with open('synNet/path.txt','w') as myfile:\n",
    "    myfile.write(str(path))\n",
    "\n",
    "# check out the output file, everything\n",
    "# should be visited, unvisited nodes should\n",
    "# be unconnected to the source node\n",
    "!echo Sample of our graph\n",
    "!head synNet/synNet_temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL: Join the indices we found with names\n",
    "We perform a memory backed join where we hold the smaller dataset in memory and run the larger data set through. If we get a match, we yield that out. We don't need a reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRjoinInd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRjoinInd.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRjoinInd(MRJob):\n",
    "    \n",
    "    # store the indices of interest\n",
    "    path = []\n",
    "    path_labels = [0]*len(path)\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we set a single reducer so that we\n",
    "        # can make a single file with all the \n",
    "        # indices linked up.\n",
    "        # if we find this approach doesn't\n",
    "        # scale, we can always up this without\n",
    "        # affecting the job's functionality\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper_init=self.mapper_init,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       reducer_init=self.mapper_init,\\\n",
    "                       reducer=self.reducer,\\\n",
    "                       reducer_final=self.reducer_final)]        \n",
    "    \n",
    "    \n",
    "    # the mapper_init loads the list of indices\n",
    "    # of interest into memory\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # open the path file, that contains\n",
    "        # the list of indices of interest\n",
    "        with open('path.txt','r') as myfile:\n",
    "            \n",
    "            # read the line in the file\n",
    "            for line in myfile.readlines():\n",
    "                self.path = ast.literal_eval(line)\n",
    "                \n",
    "        # geneate an empty array for the labels\n",
    "        self.path_labels = [0]*len(self.path)\n",
    "            \n",
    "    \n",
    "    # the mapper checks each line of the total\n",
    "    # index with matching labels and emits\n",
    "    # only those that match with our stored\n",
    "    # list in the path\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        label = line[0]\n",
    "        index = int(line[1])\n",
    "\n",
    "        # check to see if this is in our\n",
    "        # path, and if so, yield it out\n",
    "        if index in self.path:\n",
    "            yield index,label\n",
    "\n",
    "    \n",
    "    # the reducer updates the path labels list\n",
    "    def reducer(self, index, labels):\n",
    "\n",
    "        # grab the index \n",
    "        path_index = self.path.index(index)\n",
    "\n",
    "        # place the label in the \n",
    "        # path labels place in the appropriate\n",
    "        # place\n",
    "        self.path_labels[path_index] = list(labels)[0]\n",
    "        \n",
    "        \n",
    "    # the reducer final yields the completed\n",
    "    # list for the path\n",
    "    def reducer_final(self):\n",
    "        \n",
    "        # loop through each element in the \n",
    "        # path and yield it with its label\n",
    "        for i,index in enumerate(self.path):\n",
    "            yield self.path[i],self.path_labels[i]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRjoinInd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest path between walk (index=7827) and make (index=536):\n",
      "7827\t\"walk\"\n",
      "1426\t\"pass\"\n",
      "1668\t\"Give\"\n"
     ]
    }
   ],
   "source": [
    "# run the MRJob class to find the path\n",
    "# between our nodes in words\n",
    "!python MRjoinInd.py synNet/indices.txt --file=synNet/path.txt --quiet > synNet/path_labelled.txt\n",
    "\n",
    "!echo The shortest path between \"walk\" \\(index=7827\\) and \"make\" \\(index=536\\):\n",
    "!cat synNet/path_labelled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Prepare our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We prepare the data in the cloud\n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRgraphset.Alex.20160710.011827.926015\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/MRgraphset.Alex.20160710.011827.926015/files/...\n",
      "Adding our job to existing cluster j-2CVJI0SC449S9\n",
      "Waiting for step 1 of 1 (s-247QQOS83QDKW) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40770/cluster\n",
      "  RUNNING for 5.7s\n",
      "   100.0% complete\n",
      "  RUNNING for 38.0s\n",
      "     5.0% complete\n",
      "  RUNNING for 69.8s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-247QQOS83QDKW on ec2-54-153-88-129.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-153-88-129.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-247QQOS83QDKW/syslog\n",
      "Counters: 35\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=990646\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=969952\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=3300315\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2560\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=990646\n",
      "\t\tS3: Number of bytes written=969952\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=32\n",
      "\t\tLaunched map tasks=32\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1021916160\n",
      "\t\tTotal time spent by all map tasks (ms)=709664\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=31934880\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=709664\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=38140\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=9180\n",
      "\t\tInput split bytes=2560\n",
      "\t\tMap input records=8271\n",
      "\t\tMap output records=8271\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=8736931840\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=10808197120\n",
      "\t\tVirtual memory (bytes) snapshot=62776127488\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/MRgraphset.Alex.20160710.011827.926015/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRgraphset.Alex.20160710.011827.926015...\n",
      "Killing our SSH tunnel (pid 21471)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!echo We prepare the data in the cloud\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/synNet_ready --quiet\n",
    "!python MRgraphset.py -r emr s3://aks-w261-hw7/synNet.txt \\\n",
    "    --cluster-id=j-2CVJI0SC449S9 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/synNet_ready \\\n",
    "    --no-output \\\n",
    "    --indx=7827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Step through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 preparation complete\n"
     ]
    }
   ],
   "source": [
    "# copy the prepared file into a \n",
    "# temporary file that we'll keep\n",
    "# overwriting\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-2/input --quiet\n",
    "!aws s3 cp --recursive s3://aks-w261-hw7/synNet_ready s3://aks-w261-hw7/out_7-2/input --quiet\n",
    "\n",
    "# remove anything in the output\n",
    "# directory\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-2/output --quiet\n",
    "    \n",
    "print \"S3 preparation complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance for node of interest from source:\n",
      "3\n",
      "Path to reach node of interest from source:\n",
      "[7827, 1426, 1685]\n",
      "\n",
      "\n",
      "*~*~*~*~*\n",
      "\n",
      "\n",
      "Sample of our graph\n",
      "1001\t[{'997': 1, '999': 1, '998': 1, '1000': 1}, 9223372036854775807, [], 'U']\n",
      "1010\t[{'6065': 1, '6066': 1, '7760': 1, '1009': 1, '1008': 1, '1007': 1}, 9223372036854775807, [], 'U']\n",
      "104\t[{'108': 1, '109': 1, '111': 1, '110': 1, '113': 1, '112': 1, '106': 1, '107': 1, '105': 1}, 6, [7827, 1426, 1706, 4439, 782, 109], 'V']\n",
      "1079\t[{'1425': 1, '1078': 1, '1075': 1, '1590': 1, '4203': 1, '2158': 1, '2163': 1}, 4, [7827, 1426, 2160, 2163], 'V']\n",
      "1088\t[{'1087': 1, '3834': 1, '1084': 1, '2884': 1, '4338': 1, '3835': 1, '4383': 1, '610': 1, '5315': 1, '5314': 1, '3336': 1, '96': 1}, 4, [7827, 4655, 1092, 3834], 'V']\n",
      "1097\t[{'3834': 1, '1094': 1, '1096': 1, '1084': 1, '1091': 1, '1095': 1, '2297': 1, '1093': 1, '1098': 1, '1099': 1, '581': 1, '2294': 1, '2406': 1, '2407': 1, '1102': 1, '2414': 1, '2400': 1, '2402': 1, '96': 1, '6164': 1, '6166': 1}, 4, [7827, 1426, 2345, 96], 'V']\n",
      "1100\t[{'3834': 1, '3835': 1, '4947': 1, '1084': 1, '2884': 1, '4936': 1, '4937': 1, '3336': 1, '3327': 1, '4950': 1, '96': 1, '6167': 1, '1103': 1}, 4, [7827, 4655, 1092, 3834], 'V']\n",
      "113\t[{'2731': 1, '104': 1, '105': 1}, 7, [7827, 1426, 1706, 4439, 782, 109, 104], 'V']\n",
      "1169\t[{'1164': 1, '1165': 1, '2659': 1, '1209': 1, '1162': 1, '2921': 1, '2657': 1, '3865': 1, '3864': 1, '3140': 1, '4556': 1, '4631': 1, '64': 1, '4433': 1, '4432': 1, '7038': 1, '5915': 1, '4473': 1, '5505': 1, '4475': 1, '4474': 1, '6620': 1, '1199': 1, '6004': 1, '4555': 1, '6002': 1, '3798': 1, '6001': 1, '5090': 1, '3797': 1, '3796': 1, '1195': 1, '7523': 1, '1094': 1, '2405': 1, '7538': 1, '6240': 1, '6241': 1, '7537': 1, '7588': 1, '753': 1, '3761': 1, '1313': 1, '636': 1, '2280': 1, '2284': 1, '1027': 1, '5913': 1, '1954': 1, '1492': 1, '5088': 1, '5089': 1, '4845': 1, '1869': 1, '3310': 1, '7589': 1, '6003': 1, '1701': 1, '4428': 1, '4054': 1, '4053': 1, '4052': 1, '4914': 1, '4427': 1, '4756': 1, '4757': 1, '6313': 1, '4755': 1, '6239': 1, '2414': 1, '4429': 1, '3760': 1, '5914': 1, '6373': 1, '2253': 1, '2252': 1, '2534': 1, '1660': 1}, 4, [7827, 1426, 3554, 1094], 'V']\n",
      "1178\t[{'3714': 1, '1493': 1, '2960': 1, '4891': 1, '7019': 1, '3031': 1, '4479': 1, '4478': 1, '6721': 1, '6720': 1, '6457': 1, '6878': 1, '7016': 1, '7017': 1, '1489': 1, '4475': 1, '4474': 1, '4477': 1, '958': 1, '5988': 1, '974': 1, '5439': 1, '405': 1, '4607': 1, '657': 1, '1500': 1, '4487': 1, '1398': 1, '1400': 1, '6367': 1, '1177': 1, '1176': 1, '4467': 1, '5383': 1, '3742': 1, '6879': 1, '461': 1, '3743': 1, '3740': 1, '3918': 1, '1490': 1, '1491': 1, '7021': 1, '7020': 1, '1494': 1, '1495': 1, '4468': 1, '1497': 1, '1498': 1, '1499': 1, '3741': 1, '1496': 1, '6097': 1, '2212': 1, '1647': 1, '967': 1, '7018': 1, '5443': 1, '5442': 1, '1492': 1, '2955': 1, '5444': 1, '4484': 1, '4473': 1, '6197': 1, '4488': 1, '4481': 1, '4482': 1, '4483': 1, '3151': 1, '6198': 1}, 3, [7827, 4655, 1495], 'V']\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRshortest import MRshortest\n",
    "\n",
    "# clear the output to make space for the \n",
    "# next iteration\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-2/output --quiet\n",
    "        \n",
    "# set the data that we're going to pull\n",
    "mr_job = MRshortest(args=['s3://aks-w261-hw7/out_7-2/input', \\\n",
    "                          '-r','emr',\\\n",
    "                          '--cluster-id=j-2CVJI0SC449S9', \\\n",
    "                          '--aws-region=us-west-1', \\\n",
    "                          '--output-dir=s3://aks-w261-hw7/out_7-2/output', \\\n",
    "                          '--no-output'\\\n",
    "                         ]) \n",
    "\n",
    "# set the index of interest and initalize\n",
    "# values to hold the distance and the path\n",
    "INTEREST = 536\n",
    "dist = None\n",
    "path = None\n",
    "\n",
    "# set the q_count equal to 1 and \n",
    "# keep running the runner until\n",
    "# we have nothing in queue\n",
    "q_count = 1\n",
    "while (q_count > 0):\n",
    "    \n",
    "    # set the q_count to 0\n",
    "    q_count = 0\n",
    "    \n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        # create the file we will output to\n",
    "        with open('synNet/synNet_temp.txt','w') as myfile:\n",
    "\n",
    "            # run the runner and send each\n",
    "            # line to the file\n",
    "            for line in runner.stream_output():\n",
    "\n",
    "                # grab the key,value\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                \n",
    "                # grab the status\n",
    "                status = value[3]\n",
    "                \n",
    "                # if it's a q, increment the \n",
    "                # q_count\n",
    "                if status == 'Q':\n",
    "                    q_count = q_count + 1\n",
    "                \n",
    "                # if we've got to our one of interest\n",
    "                if key == INTEREST:\n",
    "                    dist = value[1]\n",
    "                    path = value[2]\n",
    "                \n",
    "                # write to the file\n",
    "                info = str(key) + \"\\t\" + str(value) + \"\\n\"\n",
    "                myfile.write(info)\n",
    "                \n",
    "        # remove the existing files from aws and \n",
    "        # upload this temporary file\n",
    "        !aws s3 rm --recursive s3://aks-w261-hw7/out_7-2/input --quiet\n",
    "        !aws s3 cp synNet/synNet_temp.txt s3://aks-w261-hw7/out_7-2/input --quiet\n",
    "        \n",
    "        # clear the output to make space for the \n",
    "        # next iteration\n",
    "        !aws s3 rm --recursive s3://aks-w261-hw7/out_7-2/output --quiet\n",
    "\n",
    "# print out the path and the distance\n",
    "# to the source node\n",
    "print \"Distance for node of interest from source:\"\n",
    "print dist\n",
    "print \"Path to reach node of interest from source:\"\n",
    "print path\n",
    "print \"\\n\"\n",
    "print \"*~*~*~*~*\"\n",
    "print \"\\n\"\n",
    "\n",
    "# write the path to a file for later joining\n",
    "with open('synNet/path.txt','w') as myfile:\n",
    "    myfile.write(str(path))\n",
    "\n",
    "# check out the output file, everything\n",
    "# should be visited, unvisited nodes should\n",
    "# be unconnected to the source node\n",
    "!echo Sample of our graph\n",
    "!head synNet/synNet_temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Join the indices we found with names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully uploaded\n"
     ]
    }
   ],
   "source": [
    "# upload the path file to S3 and the indices file\n",
    "!aws s3 cp synNet/path.txt s3://aks-w261-hw7/nltk/path.txt --quiet\n",
    "!aws s3 cp synNet/indices.txt s3://aks-w261-hw7/nltk/indices.txt --quiet\n",
    "print \"Files successfully uploaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We join our findings with the labels\n",
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRjoinInd.Alex.20160710.023114.065927\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/MRjoinInd.Alex.20160710.023114.065927/files/...\n",
      "Adding our job to existing cluster j-2CVJI0SC449S9\n",
      "Waiting for step 1 of 1 (s-T2MYBYP4IQ4Y) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40770/cluster\n",
      "  RUNNING for 11.9s\n",
      "   100.0% complete\n",
      "  RUNNING for 44.3s\n",
      "     7.8% complete\n",
      "  RUNNING for 75.3s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-T2MYBYP4IQ4Y on ec2-54-153-88-129.us-west-1.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-153-88-129.us-west-1.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-T2MYBYP4IQ4Y/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=429812\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=63\n",
      "\t\tFILE: Number of bytes written=3427089\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2752\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=429812\n",
      "\t\tS3: Number of bytes written=37\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=32\n",
      "\t\tLaunched map tasks=32\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1025743680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=27800640\n",
      "\t\tTotal time spent by all map tasks (ms)=712322\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32054490\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9653\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=868770\n",
      "\t\tTotal vcore-seconds taken by all map tasks=712322\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9653\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=36000\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=9722\n",
      "\t\tInput split bytes=2752\n",
      "\t\tMap input records=8271\n",
      "\t\tMap output bytes=37\n",
      "\t\tMap output materialized bytes=555\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=32\n",
      "\t\tPhysical memory (bytes) snapshot=15743078400\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=555\n",
      "\t\tShuffled Maps =32\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=17834704896\n",
      "\t\tVirtual memory (bytes) snapshot=66231300096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-f8c316b67324528f/tmp/MRjoinInd.Alex.20160710.023114.065927/...\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRjoinInd.Alex.20160710.023114.065927...\n",
      "Killing our SSH tunnel (pid 21917)\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!echo We join our findings with the labels\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/joins_7-2\n",
    "!python MRjoinInd.py -r emr s3://aks-w261-hw7/nltk/indices.txt \\\n",
    "    --file=s3://aks-w261-hw7/nltk/path.txt \\\n",
    "    --cluster-id=j-2CVJI0SC449S9 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/joins_7-2 \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our shortest path is:\n",
      "download: s3://aks-w261-hw7/joins_7-2/part-00000 to synNet/aws_path_labelled.txt\n",
      "7827\t\"walk\"\n",
      "1426\t\"pass\"\n",
      "1685\t\"Given\"\n"
     ]
    }
   ],
   "source": [
    "# bring the file done from the cloud and \n",
    "# show our results\n",
    "!echo Our shortest path is:\n",
    "!aws s3 cp s3://aks-w261-hw7/joins_7-2/part-00000 synNet/aws_path_labelled.txt\n",
    "!cat synNet/aws_path_labelled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.3: Exploratory data analysis (Wikipedia)\n",
    "\n",
    "Using MRJob, explore the Wikipedia network data on the AWS cloud. Reuse your code from HW 7.1---does is scale well? \n",
    "Be cautioned that Wikipedia is a directed network, where links are not symmetric. \n",
    "So, even though a node may be linked to, it will not appear as a primary record itself if it has no out-links. \n",
    "This means that you may have to ADJUST your code (depending on its design). \n",
    "To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systems test on directed_toy.txt completed above in 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the number of nodes, number of edges, and average degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRexplore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRexplore.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRexplore(MRJob):\n",
    "    \n",
    "    # set some global variables\n",
    "    total_nodes = 0\n",
    "    total_edges = 0\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we set a single reducer so that we\n",
    "        # can take the total edges over the\n",
    "        # total nodes\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       reducer=self.reducer,\\\n",
    "                       reducer_final=self.reducer_final)]        \n",
    "    \n",
    "    \n",
    "    # the mapper outputs for each node\n",
    "    # a node count and the number of links\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        num_edges = len(edges)\n",
    "        \n",
    "        # yield the node with count 1\n",
    "        # and the number of edges\n",
    "        yield 'Node',1\n",
    "        yield 'Edges',num_edges\n",
    "        \n",
    "    \n",
    "    # the reducer sums across the nodes and edges \n",
    "    # and updates the global variables\n",
    "    def reducer(self, label, counts):\n",
    "        \n",
    "        # sum the counts\n",
    "        sum_counts = sum(counts)\n",
    "        \n",
    "        # if we have the nodes, set the\n",
    "        # total nodes\n",
    "        if label == 'Node':\n",
    "            self.total_nodes = sum_counts\n",
    "            \n",
    "        # else set the total edges\n",
    "        else:\n",
    "            self.total_edges = sum_counts\n",
    "            \n",
    "    \n",
    "    # the reducer final takes the total nodes and \n",
    "    # edges and divides to get the average\n",
    "    def reducer_final(self):\n",
    "        \n",
    "        # yield the edges and nodes\n",
    "        yield \"Total Nodes:\", self.total_nodes\n",
    "        yield \"Total Edges:\", self.total_edges\n",
    "        \n",
    "        # calculate the average\n",
    "        avg_edges = float(self.total_edges) / float(self.total_nodes)\n",
    "        yield \"Average Degrees:\", avg_edges\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRexplore.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our exploratory analysis in the cloud is complete\r\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!echo Our exploratory analysis in the cloud is complete\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-3\n",
    "!python MRexplore.py -r emr s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt \\\n",
    "    --cluster-id=j-2CVJI0SC449S9 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/out_7-3 \\\n",
    "    --no-output \\\n",
    "    --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aks-w261-hw7/out_7-3/part-00000 to wiki/eda.txt\n",
      "EDA for Wikipedia\n",
      "\"Total Nodes:\"\t5781290\n",
      "\"Total Edges:\"\t142114057\n",
      "\"Average Degrees:\"\t24.58172086160701\n"
     ]
    }
   ],
   "source": [
    "# store the file locally\n",
    "!aws s3 cp s3://aks-w261-hw7/out_7-3/part-00000 wiki/eda.txt\n",
    "!echo EDA for Wikipedia\n",
    "!cat wiki/eda.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the degree distribution for the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRdegdist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRdegdist.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRdegdist(MRJob):\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we set a single reducer so that we\n",
    "        # can easily take a single file and \n",
    "        # load it into matplot lib later\n",
    "        # if we find this approach doesn't\n",
    "        # scale, we can always up this without\n",
    "        # affecting the job's functionality\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       combiner=self.reducer,\\\n",
    "                       reducer=self.reducer)]        \n",
    "    \n",
    "    \n",
    "    # the mapper outputs for each node\n",
    "    # athe number of edges and 1\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        num_edges = len(edges)\n",
    "        \n",
    "        # yield the node with count 1\n",
    "        # and the number of edges\n",
    "        yield num_edges,1\n",
    "        \n",
    "    \n",
    "    # the reducer calculates how many\n",
    "    # counts we have for each edge\n",
    "    def reducer(self, num_edges, counts):\n",
    "        \n",
    "        # yield the number of edges and\n",
    "        # the total number of nodes that\n",
    "        # have that many edges\n",
    "        yield num_edges, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRdegdist.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our degree distribution on the cloud completes successfully:\n",
      "delete: s3://aks-w261-hw7/out_7-3/_SUCCESS\n",
      "delete: s3://aks-w261-hw7/out_7-3/part-00000     \n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!echo Our degree distribution on the cloud completes successfully:\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-3\n",
    "!python MRdegdist.py -r emr s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt \\\n",
    "    --cluster-id=j-2CVJI0SC449S9 \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/out_7-3 \\\n",
    "    --no-output \\\n",
    "    --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aks-w261-hw7/out_7-3/part-00000 to wiki/degreeDist.txt\r\n"
     ]
    }
   ],
   "source": [
    "# store the file locally\n",
    "!aws s3 cp s3://aks-w261-hw7/out_7-3/part-00000 wiki/degreeDist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKEAAAJZCAYAAABx1iZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X/U5VV9H/r3BwgSK+DQRIwDaNIAiqaNQ4TceKNPTa6A\nyUKStYLT3ASMtKvt2OJNmruA/IDxGhux6Q3l3sBabY0MlIQQY9pYuTBQeWqaig5qYxIITJuAMMgQ\nGRg15nqBfO4f5ztyeJxn5hl5NsPg67XWrPmez9l7n/09c/6Y9V577291dwAAAABgpIP29wQAAAAA\neO4TQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDghFAAAAADDCaEAgFTVlVX186s01rFV9YWq\nqun1rVX1ttUYexrvhqr6ydUabx8+95eq6i+q6oFVGOvHq+rGudd/XVXfsUzbL1bVy57uZ+5m3HOr\n6vdHfw4AwC5CKAB4jquqe6rqy1W1s6p2VNV/qap/uCskSpLu/sfd/e4VjPXnVfWGPbXp7vu6+4ju\n7lWY+yVVdfWS8d/U3dc83bH3cR7HJvmZJC/v7pfs5v0/raofm3v9fVOwNF977RTOHdTdv9Hdp88N\nsex31d2Hd/c9q3MnXzv80/2cqnp9VT0x3dsXquqzVfVbVfU9qzpTAOCAJ4QCgOe+TvJD3X1kkpcm\neU+SC5K8b7U/qKoOXu0xnyVemuTz3f3wMu9/NMnr5l6/LsmdS2rfn+S/dvdf76Z/7aZ2INk2BY9H\nJPneJH+a5Per6u+u9gc9h39jAPCcJ4QCgG8MlSTd/cXu/o9J3pLk3Ko6KUmq6v1V9X9M13+zqj5U\nVY9U1cNV9Z+n+tVJjkvyoWnFy89W1UunFT9vq6p7k/ynudr8/zO+s6o+Pq3G+t2qeuE05uur6r6n\nTHRabVVVpyX5uSRvmbaKfXp6/6vb+2rmF6bVXg9W1VVVdcT03q55nFNV91bVQ1X1c8t+QVVHVNXV\nU7s/37U9sap+IMnmJC+Z7vvXd9N9aQj1/Uku3U3to9OYT9kKt2Qe//O0muh10+uvbtWb/p2urKrN\n01xurarj5vq+fHrv4aq6c8lKrKOq6vemf4PbkvytJZ87/zlvqqpPTW3vrapLlvveluruB7r7kiT/\ndvoOVjq3D02f9/Gqelc9davgX1fVhqq6O8ndKxjv0Kr6lWnun6uqK6rqedN7u/19AwDjCaEA4BtQ\nd29Jcn9mwchS/yzJfUn+ZpIXZRYEpbvPSfLZJD88rXr5lbk+r0vy8iSn7fqIJWP+ZJK3JnlxkieS\n/F/z01lmjjcl+edJfmvaKvbq3TT7qSTnJHl9ku9IcniS/3tJm9cmOT7JDya5uKpO3N3nTf0OT/Ky\nJAtJzqmqn+ru/5TkjCQPTPe9u/OtPprklVX1wqqqJCcn+a0ka+Zqr00yH3h8zX1X1elJrk3yI939\n0WXa/XiSd2b27/OHU/tU1fMzC8v+XZJvSbI+yRVV9fKp3xVJvpzk6CTnJVl6H/Of86UkPzmtnvuh\nJP+oqs7czX3vyQeTrKuqb17h3L6Y2e/trUnO3c19vznJKUlOWma8X5sb79Ik35nkb09/r01y8fTe\nbn/fAMB4QigA+Mb1QJKjdlN/LMm3Jfn27n6iu/9gyftLt451kku6+6+6+yvLfNY13X1nd/9Vkl9M\n8mNTMPN0/XiS/7O77+3uLye5KMn6uVVYnWRjd/9/3f2ZzEKbv7N0kKn9W5Jc2N1f7u57k/zLzMKz\nveruz2YW0H3/NP7W6bv4g7naNyX5xB6GOTvJlUlO7+5Pzk9vSbsPd/cfdPdjSX4+yfdW1dokP5zk\nz7v76p75wyS/k9l3fVCSH03yi939/3b3nyTZtPRrmLufj05t0t1/nOS6zIK+ffHANOYLVzi3i7v7\nK919527mliT/vLsfnb7X3Y33wSS7VkP9gyQ/3d07u/svM9uC+vem9/b2+wYABhFCAcA3rrVJduym\n/i+S/I8km6vqv1fVBSsY6/69vD+/5e7ezAKZb1nRLPfsJdN482Mfktlqn122z11/OckLdjPOt0z9\nPrtkrLX7MJffz2xF2Oum6yT5L5mFN69L8okpOFrOO5JcP4Uwe/LV73IKWB7J7Ht4aWaB1I7pzyOZ\nhXRHJ/nWzO5v/t9p/nt7iqo6tao+Mm1NfDTJP8y+/3utzSwEfPTrmNt9Swdb8v6y41XVtyZ5fpJP\n7no/yf+T2cqn5Ov7fQMAq+CQ/T0BAOCZV1WvySy4+Jpzibr7S0l+NsnP1uzMqFur6hPdfWuWf4rb\n3p6Ed+zc9UszW43y+SR/mVlgsGteB2cWSqx03Aem8ZaOvX3JZ+7N56d+L83sUO1dY23bhzE+mllY\nc0+S90+1389sa9m90/vL6cxW8fx6VW3r7sv30Par91VVL0iyJrPv4b4ki9192tIO02qjx6a+d0/l\n45a2m3NtksuTnNbdj1XVr+bJEGelfjTJp7r7r2p27tfe5nZMkv8+lXf3bzf/W9jTeJVZ2PjK7v7c\n1wyy5983ADCQlVAA8A2kqg6vqh9O8puZbZG7Yzdtfqiqdh1a/cUkj2d2jlMyC3e+Y2mX3X3Uktc/\nMR0k/fzMzjP67e7uzAKRw6rqjKo6JMkvJDl0rt/2JC/bw9a930zy01X1simQeXeS6+aeQLeiLX9T\n++uTvLuqXlBVL03y00muWUn/yUeTvDqzVU+7tnj9UZJvz+yMqT2FUJVZkPQDSc6vqn+0h7Zvqqrv\nq6pDk7wryW3dvS3Jf0xyQlX9RFUdUlXfVFXfU1UnTvf3wSQbpzOaTsosHFvOC5I8MgVQp2S2ymhP\nvvo9V9VLpoPM35bZ9sjs49xentk5X3uyp/E6yb9Jctm0KipVtbaq3jhd7+73vbsnFgIAq0wIBQDf\nGD5UVTsz2252UZJfydceTL3L8UluqaovZham/NrcIdm/nOQXp21OPzPVdrdaqZdcX5PZOT8PZBYy\nvSNJuvsLSTYkeV9m262+mKduu/rtzAKOh6vq9t2M/evT2B/NbIvVl5Ocv8w8lpvrLudP/f9sGu/f\ndff799D+qQN3b03yUJLPTfeVKRD5RGYHnv/XPXWf2t+X2QHqF9T0BMDdzPk3kmxM8nBmoddPTH2/\nlOSNmR3S/cD05z1Jnjf1+6fTPD6X2fe29Cl/85+zIcm7pt/ML2R2yPqefFvNntb3xel+X5nk9dOh\n7iud2wunuW2a7nH+fLGnfAcrGO/CzFZV3TZtJ9yc5ITpvd39vj0hDwCeATX7v9FeGlW9I8nfn17+\nm+6+vKrWZPYfkpdmtuz87O7eObW/KLP/2D6e5B3dvXmqr0tyVZLDktzQ3f/bVD80ydWZPUnm80ne\nMh3wmao6N7NDNzvJu7v76qd/2wAAB56qen+S+7r74r02PoBV1XuSHN3dP7W/5wIArJ69roSqqldm\n9hjf70ny3Ul+eFrCfGGSW7r7xCQfybTcelrefXaSV2T2OOMr5pbQX5nkvO4+IbMl1Lv28Z+XZEd3\nH5/ksiTvncZak9njdF+T5NQkl1TVkU/7rgEAeNaoqhOr6rum61My+7/hB/fvrACA1baS7XivSPLx\n6ZG5T2S2PP1Hk5yZJx+fuynJWdP1mZmdxfB4d9+TZGuSU6rqxUkO7+4tU7ur5/q8eW6sDyR5w3R9\nWpLN0+N1dy2lPn3fbxMA4Dlh70vYD0yHJ/lgVX0ps3O+/kV3f2g/zwkAWGUreTreHyf5pWlV0leS\nvCnJ7Zktkd6eJN39YFW9aGq/NsnH5vpvm2qP56lnPNyfJx97vDbTo3i7+4mq2llVR83Xl4wFAPAN\np7uXO8frgNbdt2d2VhMA8By21xCqu/+0qi5NcnOSLyX5dJ58Qs5Tmq7ivFb0JBsAAAAADgwrWQmV\n6ckw70+Sqnp3ZquTtlfV0d29fdpq99DUfFuSY+e6HzPVlqvP93mgqg5OckR376iqbZk90ni+z61L\n51dVz9Wl6QAAAAD7TXev2kKhFYVQVfWt3f0XVXVckh9J8r1Jvj3JW5NcmuTcJP9hav57Sa6tql/N\nbOvcdyb5RHf3tM3ulCRbkpyT5PK5Pucm+XiSH8vsoPMkuSnJu6fDyA9K8r9kdiD611jJU/4gSTZu\n3JiNGzfu72lwAPBbYV/4vbBSfivsC78XVspvhX3h98JKPfmcudWxohAqye9MZzQ9lmRDd39h2qJ3\nfVW9Lcm9mT0RL919R1Vdn+SOufa7EqK3J7kqyWFJbujuG6f6+5JcU1VbkzycZP001iNV9a7MzqDq\nJO+cDigHAAAA4ACy0u14r9tNbUeSH1ym/S8n+eXd1D+Z5Lt2U/9KphBrN+9dlVlwBQAAAMAB6qD9\nPQF4pi0sLOzvKXCA8FthX/i9sFJ+K+wLvxdWym+FfeH3wv5Sz4WzlKqqnwv3AQAAAPBsUVWrejC5\nlVAAAAAADCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAA\ngOGEUAAAAAAMJ4QCAAAAYDghFAAAAADDCaEAAAAAGO45E0Jt3Lhxf08BAAAAgGVUd+/vOTxtVdVJ\n8ly4FwAAAIBng6pKd9dqjfecWQkFAAAAwLOXEAoAAACA4YRQAAAAAAwnhAIAAABgOCEUAAAAAMMJ\noQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4IRQAAAAA\nwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDghFAAA\nAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABgOCEU\nAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4\nIRQAAAAAw60ohKqqn66qP66qz1TVtVV1aFWtqarNVXVXVd1UVUfOtb+oqrZW1Z1V9ca5+rppjLur\n6rK5+qFVdd3U52NVddzce+dO7e+qqnNW68YBAAAAeObsNYSqqpck+adJ1nX3305ySJK/l+TCJLd0\n94lJPpLkoqn9SUnOTvKKJGckuaKqahruyiTndfcJSU6oqtOm+nlJdnT38UkuS/Leaaw1SS5O8pok\npya5ZD7sAgAAAODAsNLteAcn+RtVdUiSb06yLcmbk2ya3t+U5Kzp+swk13X34919T5KtSU6pqhcn\nOby7t0ztrp7rMz/WB5K8Ybo+Lcnm7t7Z3Y8m2Zzk9H27RQAAAAD2t72GUN39QJJ/meSzmYVPO7v7\nliRHd/f2qc2DSV40dVmb5L65IbZNtbVJ7p+r3z/VntKnu59IsrOqjtrDWAAAAAAcQFayHe+Fma1U\nemmSl2S2Iup/TdJLmi59/XTU3psAAAAAcKA4ZAVtfjDJn3X3jiSpqt9N8n1JtlfV0d29fdpq99DU\nfluSY+f6HzPVlqvP93mgqg5OckR376iqbUkWlvS5dbmJbty4MUmysLCQhYWF5ZoBAAAAsMTi4mIW\nFxeHjV/de17AVFWnJHlfZoeDfyXJ+5NsSXJcZoeJX1pVFyRZ090XTgeTX5vZQeJrk9yc5Pju7qq6\nLcn5U/8PJ7m8u2+sqg1JXtXdG6pqfZKzunv9dDD57UnWZbZq6/YkJ0/nQ83PsZNkb/cCAAAAwMpU\nVbp71Xar7XUlVHd/oqo+kOTTSR6b/v7XSQ5Pcn1VvS3JvZk9ES/dfUdVXZ/kjqn9hn4yHXp7kquS\nHJbkhu6+caq/L8k1VbU1ycNJ1k9jPVJV78osfOok71waQAEAAADw7LfXlVAHAiuhAAAAAFbXaq+E\n2uvB5AAAAADwdAmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QC\nAAAAYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwn\nhAIAAABgOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAA\nDCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAA\nAAAMJ4QCAAAAYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQ\nAAAAAAwnhAIAAABgOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDh\nhFAAAAAADCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAw3F5DqKo6\noao+XVWfmv7eWVXnV9WaqtpcVXdV1U1VdeRcn4uqamtV3VlVb5yrr6uqz1TV3VV12Vz90Kq6burz\nsao6bu69c6f2d1XVOat58wAAAAA8M6q7V9646qAk9yc5Nck/SfJwd7+3qi5Isqa7L6yqk5Jcm+Q1\nSY5JckuS47u7q+rjSf5Jd2+pqhuS/Kvuvqmq/nGS7+ruDVX1liQ/0t3rq2pNktuTrEtSST6ZZF13\n71wyr06SfbkXAAAAAJZXVenuWq3x9nU73g8m+R/dfV+SNyfZNNU3JTlruj4zyXXd/Xh335Nka5JT\nqurFSQ7v7i1Tu6vn+syP9YEkb5iuT0uyubt3dvejSTYnOX0f5wwAAADAfravIdRbkvzGdH10d29P\nku5+MMmLpvraJPfN9dk21dZmtopql/un2lP6dPcTSXZW1VF7GAsAAACAA8iKQ6iq+qbMVjn99lRa\nuvdtNffCrdpSLwAAAAD2v0P2oe0ZST7Z3Z+fXm+vqqO7e/u01e6hqb4tybFz/Y6ZasvV5/s8UFUH\nJzmiu3dU1bYkC0v63LrcBDdu3JgkWVhYyMLCwnLNAAAAAFhicXExi4uLw8Zf8cHkVfWbSW7s7k3T\n60uT7OjuS5c5mPzUzLbO3ZwnDya/Lcn5SbYk+XCSy7v7xqrakORV08Hk65OctZuDyQ+ark+ezoea\nn5uDyQEAAABW0WofTL6iEKqqnp/k3iTf0d1fnGpHJbk+sxVM9yY5e1c4VFUXJTkvyWNJ3tHdm6f6\nyUmuSnJYkhu6+x1T/XlJrkny6iQPJ1k/HWqeqnprkp/PbLvfL3X31buZnxAKAAAAYBXtlxDq2U4I\nBQAAALC6VjuE2ten4wEAAADAPhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABgOCEUAAAAAMMJ\noQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4IRQAAAAA\nwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDghFAAA\nAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABgOCEU\nAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4\nIRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAA\nYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIA\nAABguBWFUFV1ZFX9dlXdWVV/UlWnVtWaqtpcVXdV1U1VdeRc+4uqauvU/o1z9XVV9ZmquruqLpur\nH1pV1019PlZVx829d+7U/q6qOme1bhwAAACAZ85KV0L9qyQ3dPcrkvydJH+a5MIkt3T3iUk+kuSi\nJKmqk5KcneQVSc5IckVV1TTOlUnO6+4TkpxQVadN9fOS7Oju45NcluS901hrklyc5DVJTk1yyXzY\nBQAAAMCBYa8hVFUdkeT7u/v9SdLdj3f3ziRvTrJparYpyVnT9ZlJrpva3ZNka5JTqurFSQ7v7i1T\nu6vn+syP9YEkb5iuT0uyubt3dvejSTYnOf3rulMAAAAA9puVrIT69iSfr6r3V9WnqupfV9Xzkxzd\n3duTpLsfTPKiqf3aJPfN9d821dYmuX+ufv9Ue0qf7n4iyc6qOmoPYwEAAABwAFlJCHVIknVJfq27\n1yX5y8y24vWSdktfPx219yYAAAAAHCgOWUGb+5Pc1923T69/J7MQantVHd3d26etdg9N729Lcuxc\n/2Om2nL1+T4PVNXBSY7o7h1VtS3JwpI+ty430Y0bNyZJFhYWsrCwsFwzAAAAAJZYXFzM4uLisPGr\ne+8LmKrqPyf5B919d1VdkuT501s7uvvSqrogyZruvnA6mPzazA4SX5vk5iTHd3dX1W1Jzk+yJcmH\nk1ze3TdW1YYkr+ruDVW1PslZ3b1+Opj89sxWYh00XZ88nQ81P79OkpXcCwAAAAB7V1Xp7lXbrbaS\nlVDJLDi6tqq+KcmfJfmpJAcnub6q3pbk3syeiJfuvqOqrk9yR5LHkmzoJ9Ohtye5KslhmT1t78ap\n/r4k11TV1iQPJ1k/jfVIVb0rs/Cpk7xzaQAFAAAAwLPfilZCPdtZCQUAAACwulZ7JdRKDiYHAAAA\ngKdFCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDghFAAAAADDCaEA\nAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABgOCEUAAAAAMMJ\noQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4IRQAAAAA\nwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDghFAAA\nAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABgOCEU\nAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4\nIRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOFWFEJV1T1V9YdV9emq\n+sRUW1NVm6vqrqq6qaqOnGt/UVVtrao7q+qNc/V1VfWZqrq7qi6bqx9aVddNfT5WVcfNvXfu1P6u\nqjpndW4bAAAAgGfSSldC/XWShe5+dXefMtUuTHJLd5+Y5CNJLkqSqjopydlJXpHkjCRXVFVNfa5M\ncl53n5DkhKo6baqfl2RHdx+f5LIk753GWpPk4iSvSXJqkkvmwy4AAAAADgwrDaFqN23fnGTTdL0p\nyVnT9ZlJruvux7v7niRbk5xSVS9Ocnh3b5naXT3XZ36sDyR5w3R9WpLN3b2zux9NsjnJ6SucMwAA\nAADPEisNoTrJzVW1par+/lQ7uru3J0l3P5jkRVN9bZL75vpum2prk9w/V79/qj2lT3c/kWRnVR21\nh7EAAAAAOIAcssJ2r+3uz1XVtybZXFV3ZRZMzVv6+umovTcBAAAA4ECxohCquz83/f0XVfXvk5yS\nZHtVHd3d26etdg9NzbclOXau+zFTbbn6fJ8HqurgJEd0946q2pZkYUmfW5eb58aNG5MkCwsLWVhY\nWK4ZAAAAAEssLi5mcXFx2PjVvecFTFX1/CQHdfeXqupvZHYu0zuT/EBmh4lfWlUXJFnT3RdOB5Nf\nm9lB4muT3Jzk+O7uqrotyflJtiT5cJLLu/vGqtqQ5FXdvaGq1ic5q7vXTweT355kXWZbB29PcvJ0\nPtT8HDtJ9nYvAAAAAKxMVaW7V2232kpWQh2d5HenoOeQJNd29+aquj3J9VX1tiT3ZvZEvHT3HVV1\nfZI7kjyWZEM/mQ69PclVSQ5LckN33zjV35fkmqramuThJOunsR6pqndlFj51kncuDaAAAAAAePbb\n60qoA4GVUAAAAACra7VXQq306XgAAAAA8HUTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDgh\nFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABg\nOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAA\nAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QC\nAAAAYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwn\nhAIAAABgOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAA\nDCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAA\nAAAMJ4QCAAAAYLgVh1BVdVBVfaqqfm96vaaqNlfVXVV1U1UdOdf2oqraWlV3VtUb5+rrquozVXV3\nVV02Vz+0qq6b+nysqo6be+/cqf1dVXXO079lAAAAAJ5p+7IS6h1J7ph7fWGSW7r7xCQfSXJRklTV\nSUnOTvKKJGckuaKqaupzZZLzuvuEJCdU1WlT/bwkO7r7+CSXJXnvNNaaJBcneU2SU5NcMh92AQAA\nAHBgWFEIVVXHJHlTkn87V35zkk3T9aYkZ03XZya5rrsf7+57kmxNckpVvTjJ4d29ZWp39Vyf+bE+\nkOQN0/VpSTZ3987ufjTJ5iSnr/z2AAAAAHg2WOlKqF9N8r8n6bna0d29PUm6+8EkL5rqa5PcN9du\n21Rbm+T+ufr9U+0pfbr7iSQ7q+qoPYwFAAAAwAFkryFUVf1Qku3d/d+S1B6a9h7e21d7+hwAAAAA\nDjCHrKDwv2CEAAAWy0lEQVTNa5OcWVVvSvLNSQ6vqmuSPFhVR3f39mmr3UNT+21Jjp3rf8xUW64+\n3+eBqjo4yRHdvaOqtiVZWNLn1uUmunHjxiTJwsJCFhYWlmsGAAAAwBKLi4tZXFwcNn51r3wBU1W9\nPsk/6+4zq+q9SR7u7kur6oIka7r7wulg8mszO0h8bZKbkxzf3V1VtyU5P8mWJB9Ocnl331hVG5K8\nqrs3VNX6JGd19/rpYPLbk6zLbNXW7UlOns6Hmp9XJ8m+3AsAAAAAy6uqdPeq7VZbyUqo5bwnyfVV\n9bYk92b2RLx09x1VdX1mT9J7LMmGfjIdenuSq5IcluSG7r5xqr8vyTVVtTXJw0nWT2M9UlXvyix8\n6iTvXBpAAQAAAPDst08roZ6trIQCAAAAWF2rvRJqpU/HAwAAAICvmxAKAAAAgOGEUAAAAAAMJ4QC\nAAAAYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwn\nhAIAAABgOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAA\nDCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAA\nAAAMJ4QCAAAAYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQ\nAAAAAAwnhAIAAABgOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDh\nhFAAAAAADCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAA\ngOGEUAAAAAAMJ4QCAAAAYDghFAAAAADD7TWEqqrnVdXHq+rTVfVHVXXJVF9TVZur6q6quqmqjpzr\nc1FVba2qO6vqjXP1dVX1maq6u6oum6sfWlXXTX0+VlXHzb137tT+rqo6Z/VuHQAAAIBnyl5DqO7+\nSpK/292vTvLdSc6oqlOSXJjklu4+MclHklyUJFV1UpKzk7wiyRlJrqiqmoa7Msl53X1CkhOq6rSp\nfl6SHd19fJLLkrx3GmtNkouTvCbJqUkumQ+7AAAAADgwrGg7Xnd/ebp8XpJDknSSNyfZNNU3JTlr\nuj4zyXXd/Xh335Nka5JTqurFSQ7v7i1Tu6vn+syP9YEkb5iuT0uyubt3dvejSTYnOX2f7hAAAACA\n/W5FIVRVHVRVn07yYJKbpyDp6O7eniTd/WCSF03N1ya5b677tqm2Nsn9c/X7p9pT+nT3E0l2VtVR\nexgLAAAAgAPISldC/fW0He+YzFY1vTKz1VBPabaK86q9NwEAAADgQHHIvjTu7i9U1WJmW+K2V9XR\n3b192mr30NRsW5Jj57odM9WWq8/3eaCqDk5yRHfvqKptSRaW9Ll1uflt3LgxSbKwsJCFhYXlmgEA\nAACwxOLiYhYXF4eNX917XsBUVd+S5LHu3llV35zkpiTvSfL6zA4Tv7SqLkiyprsvnA4mvzazg8TX\nJrk5yfHd3VV1W5Lzk2xJ8uEkl3f3jVW1IcmruntDVa1PclZ3r58OJr89ybrMVm3dnuTk6Xyo+Tl2\nkuztXgAAAABYmapKd6/abrWVrIT6tiSbquqgzIKg3+ruG6ZA6fqqeluSezN7Il66+46quj7JHUke\nS7Khn0yH3p7kqiSHJbmhu2+c6u9Lck1VbU3ycJL101iPVNW7MgufOsk7lwZQAAAAADz77XUl1IHA\nSigAAACA1bXaK6FWdDA5AAAAADwdQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwnhAIAAABg\nOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAA\nAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYTQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QC\nAAAAYDghFAAAAADDCaEAAAAAGE4IBQAAAMBwQigAAAAAhhNCAQAAADCcEAoAAACA4YRQAAAAAAwn\nhAIAAABgOCEUAAAAAMMJoQAAAAAYTggFAAAAwHBCKAAAAACGE0IBAAAAMJwQCgAAAIDhhFAAAAAA\nDPecCqE2bty4v6cAAAAAwG5Ud+/vOTxtVfXVm3gu3A8AAADA/lZV6e5arfGeUyuhAAAAAHh2EkIB\nAAAAMJwQCgAAAIDhhFAAAAAADCeEAgAAAGA4IRQAAAAAwwmhAAAAABhOCAUAAADAcEIoAAAAAIYT\nQgEAAAAwnBAKAAAAgOGEUAAAAAAMJ4QCAAAAYDghFAAAAADD7TWEqqpjquojVfUnVfVHVXX+VF9T\nVZur6q6quqmqjpzrc1FVba2qO6vqjXP1dVX1maq6u6oum6sfWlXXTX0+VlXHzb137tT+rqo6Z/Vu\nHQAAAIBnykpWQj2e5Ge6+5VJ/qckb6+qlye5MMkt3X1iko8kuShJquqkJGcneUWSM5JcUVU1jXVl\nkvO6+4QkJ1TVaVP9vCQ7uvv4JJclee801pokFyd5TZJTk1wyH3YBAAAAcGDYawjV3Q9293+brr+U\n5M4kxyR5c5JNU7NNSc6ars9Mcl13P97d9yTZmuSUqnpxksO7e8vU7uq5PvNjfSDJG6br05Js7u6d\n3f1oks1JTv96bhQAAACA/WefzoSqqpcl+e4ktyU5uru3J7OgKsmLpmZrk9w3123bVFub5P65+v1T\n7Sl9uvuJJDur6qg9jAUAAADAAWTFIVRVvSCzVUrvmFZE9ZImS18/HbX3Jks9bxU/HgAAAIDVdMhK\nGlXVIZkFUNd093+Yytur6uju3j5ttXtoqm9Lcuxc92Om2nL1+T4PVNXBSY7o7h1VtS3JwpI+t+5+\nll9JkmzcuDELCwtZWFjYfTMAAAAAvsbi4mIWFxeHjV/de1/AVFVXJ/l8d//MXO3SzA4Tv7SqLkiy\nprsvnA4mvzazg8TXJrk5yfHd3VV1W5Lzk2xJ8uEkl3f3jVW1IcmruntDVa1PclZ3r58OJr89ybrM\nVm3dnuTk6Xyo+fl99SZWcj8AAAAA7FlVpbu/jt1qy4y3t9Cmql6b5KNJ/iizLXed5OeSfCLJ9Zmt\nYLo3ydm7wqGquiizJ949ltn2vc1T/eQkVyU5LMkN3f2Oqf68JNckeXWSh5Osnw41T1W9NcnPT5/7\nS9199W7mKIQCAAAAWEXPeAh1IBBCAQAAAKyu1Q6h9unpeAAAAADw9RBCAQAAADCcEAoAAACA4YRQ\nAAAAAAwnhAL+//buLtay8qwD+P+BkZm2UIo1MJYpnZqWluJXaZxWSZOxVD5qAlxVSGOrxXgBlUYT\nA/RCMN6AiZGa2iaNSCkBETC1NBI6EDwXTUTAoiCfEwkwTMvUCo6xJgTo48Ve0+45c4aZgbPOnn3m\n90tOZu3nrLXyruxn9jn5n3e9CwAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIA\nAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYn\nhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAA\nAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEa36kKo9es3znoIAAAAACxS3T3rMbxuVbXb\nRayGawIAAACYpapKd9dynW/VzYQCAAAA4OAjhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYn\nhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAA\nAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIoAAAAAEYnhAIAAABgdEIo\nAAAAAEYnhAIAAABgdPsMoarqmqraUVUPTtWOqaotVfV4VX2zqo6e+t5lVbW1qh6tqtOn6qdU1YNV\n9URVXT1VP6KqbhqO+aeqOmHqe58a9n+8qj65PJcMAAAAwErbn5lQ1yY5Y1Ht0iR3dfd7ktyd5LIk\nqar3Jfl4kpOSnJXki1VVwzFfSnJBd5+Y5MSq2nXOC5I8393vTnJ1kj8dznVMkj9K8ktJPpjk8umw\nCwAAAID5sc8Qqru/leSFReVzklw3bF+X5Nxh++wkN3X3y939VJKtSTZV1fokR3X3fcN+X506Zvpc\ntyb5yLB9RpIt3b2zu/87yZYkZx7AtQEAAABwkHita0Id2907kqS7n0ty7FA/Psm2qf22D7Xjkzw7\nVX92qO12THe/kmRnVf3kq5wLAAAAgDmzXAuT9zKdJ0lq37sAAAAAME/WvMbjdlTVcd29Y7jV7ntD\nfXuSt0/tt2Go7a0+fcx3qurwJG/u7ueranuSzYuO+cf9GdzCwkI2b968z/0AAAAAmFhYWMjCwsJo\n56/ufU9iqqqNSb7R3T83vL4qk8XEr6qqS5Ic092XDguT35DJQuLHJ7kzybu7u6vqniQXJ7kvyT8k\n+YvuvqOqLkzys919YVWdl+Tc7j5vWJj8/iSnZDJj6/4kHxjWh1o8vt0uYn+uCQAAAIC9q6p097Ld\nsbbPmVBVdWMmM5LeWlXPJLk8yZVJbqmqTyd5OpMn4qW7H6mqm5M8kuSlJBf2jxOhi5J8Jcm6JLd3\n9x1D/Zok11fV1iT/leS84VwvVNWfZBI+dZI/XiqAAgAAAODgt18zoQ52ZkIBAAAALK/lngm1XAuT\nAwAAAMBeCaEAAAAAGJ0QCgAAAIDRCaEAAAAAGN0qDKHWZv36jbMeBAAAAABTVuXT8RJPyAMAAAB4\nPTwdDwAAAIC5I4QCAAAAYHRCKAAAAABGJ4QCAAAAYHRCKAAAAABGJ4QCAAAAYHRCKAAAAABGJ4QC\nAAAAYHRCKAAAAABGJ4QCAAAAYHRCKAAAAABGJ4QCAAAAYHRCKAAAAABGJ4QCAAAAYHRCKAAAAABG\nJ4QCAAAAYHRCKAAAAABGJ4QCAAAAYHRCKAAAAABGJ4QCAAAAYHSrNIRam/XrN856EAAAAAAMqrtn\nPYbXraqWvIjVcG0AAAAAs1BV6e5arvOt0plQAAAAABxMhFAAAAAAjE4IBQAAAMDohFAAAAAAjE4I\nBQAAAMDohFAAAAAAjE4IBQAAAMDohFAAAAAAjE4IBQAAAMDoVnEItTbr12+c9SAAAAAASFLdPesx\nvG5VtdeLWA3XBwAAALDSqirdXct1vlU8EwoAAACAg4UQCgAAAIDRCaEAAAAAGJ0QCgAAAIDRrfIQ\nyhPyAAAAAA4Gq/7peIkn5AEAAAAcKE/HAwAAAGDuCKEAAAAAGN0hEEKtzZFHvmXWgwAAAAA4pB0S\na0Il1oUCAAAAOBDWhAIAAABg7hwiIdTaHHHEulkPAgAAAOCQdYiEUC/mpZeS9es3znogAAAAAIek\nQ2ZNqF1Ww/UCAAAAjO2QXBOqqs6sqseq6omquuS1n8lteQAAAACzcNCHUFV1WJIvJDkjyclJzq+q\n9762s01uy5uckkPVwsLCrIfAnNArHAj9wv7SKxwI/cL+0iscCP3CrMxDGrMpydbufrq7X0pyU5Jz\nXvvpXkxyRKrWpWrZZpQxR3zgsr/0CgdCv7C/9AoHQr+wv/QKB0K/MCvzEEIdn2Tb1Otnh9rr8OLw\nlSGMmny5VQ8AAABgHPMQQo3sxR99TW7VW3dAX4cfviZXXHHFjK8BAAAA4OB20D8dr6o+lOSK7j5z\neH1pku7uq6b2ObgvAgAAAGAOLefT8eYhhDo8yeNJTkvy3ST3Jjm/ux+d6cAAAAAA2G9rZj2Afenu\nV6rqM0m2ZHL74DUCKAAAAID5ctDPhAIAAABg/s39wuRVdWZVPVZVT1TVJbMeDyuvqq6pqh1V9eBU\n7Ziq2lJVj1fVN6vq6KnvXVZVW6vq0ao6fap+SlU9OPTS1St9HYyvqjZU1d1V9XBVPVRVFw91/cIe\nqmptVf1zVT0w9MvlQ12/sKSqOqyqvl1Vtw2v9QpLqqqnqurfhs+Xe4eafmEPVXV0Vd0yvPcPV9UH\n9QpLqaoTh8+Ubw//7qyqi/ULS6mq36+qfx/e5xuq6oiV6pW5DqGq6rAkX0hyRpKTk5xfVe+d7aiY\ngWsz6YFplya5q7vfk+TuJJclSVW9L8nHk5yU5KwkX6yqXYusfSnJBd19YpITq2rxOZl/Lyf5g+4+\nOckvJ7lo+MzQL+yhu19M8qvd/f4kv5jkrKraFP3C3n02ySNTr/UKe/PDJJu7+/3dvWmo6ReW8vkk\nt3f3SUl+Iclj0SssobufGD5TTknygSQ/SPK16BcWqaq3Jfm9JKd0989nskzT+VmhXpnrECrJpiRb\nu/vp7n4pyU1JzpnxmFhh3f2tJC8sKp+T5Lph+7ok5w7bZye5qbtf7u6nkmxNsqmq1ic5qrvvG/b7\n6tQxrBLd/Vx3/+uw/b9JHk2yIfqFveju/xs212byA7qjX1hCVW1I8rEkfzVV1ivsTWXP38P1C7up\nqjcn+XB3X5skQw/sjF5h3z6a5D+6e1v0C0s7PMmbqmpNkjck2Z4V6pV5D6GOT7Jt6vWzQw2O7e4d\nySR4SHLsUF/cM9uH2vGZ9M8uemmVq6qNmcxuuSfJcfqFpdTk9qoHkjyX5M7hh6x+YSl/nuQPMwkq\nd9Er7E0nubOq7quq3xlq+oXF3pnk+1V17XCL1Zer6o3RK+zbbyS5cdjWL+ymu7+T5M+SPJPJ+76z\nu+/KCvXKvIdQsL+swM+PVNWRSW5N8tlhRtTi/tAvJEm6+4fD7XgbMvmLz8nRLyxSVb+eZMcw07Je\nZVe9wi6nDrfMfCyTW8M/HJ8t7GlNklOS/OXQLz/I5HYZvcJeVdVPZDJz5ZahpF/YTVW9JZNZT+9I\n8rZMZkR9IivUK/MeQm1PcsLU6w1DDXZU1XFJMkwT/N5Q357k7VP77eqZvdVZZYYpp7cmub67vz6U\n9Quvqrv/J8lCkjOjX9jTqUnOrqonk/xNko9U1fVJntMrLKW7vzv8+59J/j6TJSZ8trDYs0m2dff9\nw+u/yySU0iu8mrOS/Et3f394rV9Y7KNJnuzu57v7lUzWDvuVrFCvzHsIdV+Sd1XVO6rqiCTnJblt\nxmNiNiq7//X5tiS/NWx/KsnXp+rnDav/vzPJu5LcO0w33FlVm4ZF1j45dQyry18neaS7Pz9V0y/s\noap+atdTQarqDUl+LZN1xPQLu+nuz3X3Cd39M5n8LnJ3d/9mkm9Er7BIVb1xmJGbqnpTktOTPBSf\nLSwy3BazrapOHEqnJXk4eoVXd34mfxDZRb+w2DNJPlRV64b3+LRMHqyyIr2yZlkvZYV19ytV9Zkk\nWzIJ1K7p7kdnPCxWWFXdmGRzkrdW1TNJLk9yZZJbqurTSZ7OZDX/dPcjVXVzJv/JXkpyYXfvmmZ4\nUZKvJFmXyVNI7ljJ62B8VXVqkk8keWhY56eTfC7JVUlu1i8s8tNJrqvJk1gPS/K33X17Vd0T/cL+\nuTJ6hT0dl+RrVdWZ/C5+Q3dvqar7o1/Y08VJbhhusXoyyW9nsqCwXmEPw5phH03yu1Nlv+eym+6+\nt6puTfJAJu/9A0m+nOSorECv1I+PBQAAAIBxzPvteAAAAADMASEUAAAAAKMTQgEAAAAwOiEUAAAA\nAKMTQgEAAAAwOiEUAAAAAKMTQgEAAAAwOiEUAAAAAKP7f+q6SnILXRYAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1036b6d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import pandas and matplotlib to load\n",
    "# the data and plot the histogram\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# read in the data as a data frame\n",
    "data = pd.read_table('wiki/degreeDist.txt',header=None)\n",
    "data_count = data.iloc[:,1]\n",
    "data_lengt = data.iloc[:,0]\n",
    "\n",
    "# gives histogram bars a width\n",
    "width = 1.0 \n",
    "\n",
    "# plot the histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(data_lengt,data_count, width, color='b')\n",
    "plt.title(\"Distribution of Wikipedia Degrees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 7.4: Shortest path graph distances (Wikipedia)\n",
    "\n",
    "Using MRJob, find shortest path graph distances in the Wikipedia network on the AWS cloud.\n",
    "Reuse your code from 7.2, but once again be warned of Wikipedia being a directed network.\n",
    "To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "\n",
    "When running your code on the Wikipedia network, proof its function by running the job:\n",
    "\n",
    "- shortest path from \"Ireland\" (index=6176135) to \"University of California, Berkeley\" (index=13466359),\n",
    "\n",
    "and show your code's output. Show the shortest path in terms of just page IDS but also in terms of the name of page (show of your MapReduce join skills!!)\n",
    "\n",
    "Once your code is running, find some other shortest paths and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:violet\">SCALABILITY!!!</span>\n",
    "So, our previous approaches worked for the NLTK data because it wasn't that large. However, copying data back and forth from local disk to cloud is highly inefficient. We also don't want to see each line to check for a stopping condition. Instead, let's use MRJob's counters. To test this out, we rewrite our program to use counters and use the counters to tell us when to stop. Instead of using runners, we wrap magic commands within python while loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRJob revised class to step through graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRshortCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRshortCount.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import copy\n",
    "  \n",
    "class MRshortCount(MRJob):\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\\\n",
    "                       reducer=self.reducer)] \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # performs an action based on the\n",
    "    # status of the node\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into the node\n",
    "        # and the payload that has so\n",
    "        # much information\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        payload = ast.literal_eval(line[1])\n",
    "\n",
    "        # split up the payload into its\n",
    "        # component parts\n",
    "        outlinks = payload[0]\n",
    "        dist = int(payload[1])\n",
    "        path = payload[2]\n",
    "        status = payload[3]\n",
    "        \n",
    "        # if we're not dealing with a node in\n",
    "        # status 'Q'\n",
    "        if status == 'Q':\n",
    "            \n",
    "            # the distance to the outlinks is\n",
    "            # the distance to the current node\n",
    "            # plus 1 \n",
    "            edge_dist = dist + 1\n",
    "            \n",
    "            # we don't know the outlinks for the \n",
    "            # outlinks we're about to emit\n",
    "            edge_outlinks = {}\n",
    "            \n",
    "            # we append to the existing path\n",
    "            # the node that brought us here\n",
    "            edge_path = copy.deepcopy(path)\n",
    "            edge_path.append(node)\n",
    "            \n",
    "            # the status for the next edge is\n",
    "            # Q because we will look at that\n",
    "            # next\n",
    "            edge_status = 'Q'\n",
    "            \n",
    "            # loop through the outedges\n",
    "            for edge in outlinks:\n",
    "                \n",
    "                # set the key, value pair\n",
    "                # we emit for each queued up\n",
    "                # node\n",
    "                edge_key = edge\n",
    "                edge_payload = (edge_outlinks,\\\n",
    "                                edge_dist,\\\n",
    "                                edge_path,\\\n",
    "                                edge_status)\n",
    "                \n",
    "                # emit the next in line for\n",
    "                # queueing\n",
    "                yield int(edge_key), edge_payload\n",
    "                \n",
    "            # set a new status for this node,\n",
    "            # visited\n",
    "            new_status = 'V'\n",
    "            \n",
    "            # set the key,value pair for this\n",
    "            # original node\n",
    "            new_key = node\n",
    "            new_value = (outlinks,dist,path,new_status)\n",
    "            yield int(new_key),new_value\n",
    "\n",
    "        # else if the node is not in queue\n",
    "        # status, then simply yield it as is\n",
    "        else:\n",
    "            key = node\n",
    "            value = (outlinks,dist,path,status)\n",
    "            yield int(key),value\n",
    "            \n",
    "        \n",
    "    # our reducer merges the outputs \n",
    "    # from nodes of various statuses\n",
    "    # and yields a single line for each\n",
    "    # node\n",
    "    def reducer(self, node, payloads):\n",
    "\n",
    "        # initalize storage variables\n",
    "        # that will keep track of the \n",
    "        # everything for all the nodes\n",
    "        node = node\n",
    "        new_outlinks = []\n",
    "        new_dists = []\n",
    "        new_paths = []\n",
    "        statuses = []\n",
    "\n",
    "        # initalize the final variables\n",
    "        # that we'll use to output\n",
    "        final_outlinks = {}\n",
    "        final_dist = None\n",
    "        final_path = None\n",
    "        final_status = None\n",
    "\n",
    "        # loop through the payloads\n",
    "        for payload in payloads:\n",
    "\n",
    "            # gather all the information\n",
    "            # from the payload\n",
    "            outlinks = payload[0]\n",
    "            dist = payload[1]\n",
    "            path = payload[2]\n",
    "            status = payload[3]\n",
    "\n",
    "            # update the variables for\n",
    "            # each node\n",
    "            new_outlinks.append(outlinks)\n",
    "            new_dists.append(dist)\n",
    "            new_paths.append(path)\n",
    "            statuses.append(status)\n",
    "\n",
    "        # look at the status to determine\n",
    "        # what to do; let's start with if\n",
    "        # we have a 'V'\n",
    "        if 'V' in statuses:\n",
    "\n",
    "            # get the index of the \"v\" status\n",
    "            index = statuses.index('V')\n",
    "\n",
    "            # set the final variables based\n",
    "            # on the node we've already \n",
    "            # visited\n",
    "            final_outlinks = new_outlinks[index]\n",
    "            final_dist = new_dists[index]\n",
    "            final_path = new_paths[index]\n",
    "            final_status = statuses[index]\n",
    "        \n",
    "        # else if we have a 'Q'\n",
    "        elif 'Q' in statuses:\n",
    "            \n",
    "            # increment the counter for the number\n",
    "            # of q's we've got\n",
    "            self.increment_counter('Nodes', 'enqueue', 1)\n",
    "            \n",
    "            # find the unvisited and queued\n",
    "            # indices\n",
    "            q_index = statuses.index('Q')\n",
    "            \n",
    "            # only add outlinks if we have 'U'\n",
    "            # otherwise, we should just add\n",
    "            # blank outlinks as we are going\n",
    "            # outside of our graph     \n",
    "            if 'U' in statuses:\n",
    "\n",
    "                u_index = statuses.index('U')\n",
    "\n",
    "                # get the final outlinks from the \n",
    "                # unvisited node\n",
    "                final_outlinks = new_outlinks[u_index]\n",
    "\n",
    "            # else put up blank outlinks\n",
    "            else: \n",
    "                final_outlinks = {}\n",
    "\n",
    "            \n",
    "\n",
    "            # get the distance and the path and\n",
    "            # the status from 'queued' node\n",
    "            final_dist = new_dists[q_index]\n",
    "            final_path = new_paths[q_index]\n",
    "            final_status = statuses[q_index]\n",
    "            \n",
    "        # else, we must have only an unvisited\n",
    "        # node, and we'll take all the information\n",
    "        # from the unvisited node\n",
    "        else:\n",
    "            \n",
    "            # set the index based on the\n",
    "            # unvisited node\n",
    "            index = statuses.index('U')\n",
    "            \n",
    "            # get the final variables all from\n",
    "            # the unvisited node\n",
    "            final_outlinks = new_outlinks[index]\n",
    "            final_dist = new_dists[index]\n",
    "            final_path = new_paths[index]\n",
    "            final_status = statuses[index]\n",
    "            \n",
    "            \n",
    "        # set the key value that we'll be\n",
    "        # outputting for each node, only one for\n",
    "        # each node\n",
    "        key = node\n",
    "        value = (final_outlinks,\\\n",
    "                 final_dist,\\\n",
    "                 final_path,\\\n",
    "                 final_status)\n",
    "        \n",
    "        # yield the key-value pair\n",
    "        yield key,value\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRshortCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test locally\n",
    "We figure out this freaky counter things by unit testing on our directed toy network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/Alex/.mrjob.conf\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRshortCount.Alex.20160710.144059.553049\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\tNodes\n",
      "\t\tenqueue=2\n",
      "Streaming final output from /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRshortCount.Alex.20160710.144059.553049/output...\n",
      "1\t[{\"2\": 1, \"6\": 1}, 0, [], \"V\"]\n",
      "2\t[{\"1\": 1, \"3\": 1, \"4\": 1}, 1, [1], \"Q\"]\n",
      "3\t[{\"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\n",
      "4\t[{\"2\": 1, \"5\": 1}, 9223372036854775807, [], \"U\"]\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 9223372036854775807, [], \"U\"]\n",
      "6\t[{}, 1, [1], \"Q\"]\n",
      "Removing temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/MRshortCount.Alex.20160710.144059.553049...\n"
     ]
    }
   ],
   "source": [
    "!python MRshortCount.py directed_ready.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it works locally for one iteration. That's good. Now let's see if we can mix and match magic with python and how well it works! It's time for some exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# call the function, direct the stdout to a \n",
    "# file, but hold/store the standard error\n",
    "stderr = !python MRshortCount.py directed_ready.txt > directed_temp.txt\n",
    "\n",
    "# set a flag for continuing to iterate\n",
    "flag = False\n",
    "\n",
    "# check to see if we have any nodes that\n",
    "# were put in queue, and if so, set the \n",
    "# flag to true to continue a while loop\n",
    "if \"\\tNodes\" in stderr:\n",
    "    flag = True\n",
    "    \n",
    "print flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t[{\"2\": 1, \"6\": 1}, 0, [], \"V\"]\r\n",
      "2\t[{\"1\": 1, \"3\": 1, \"4\": 1}, 1, [1], \"V\"]\r\n",
      "3\t[{\"2\": 1, \"4\": 1}, 2, [1, 2], \"V\"]\r\n",
      "4\t[{\"2\": 1, \"5\": 1}, 2, [1, 2], \"V\"]\r\n",
      "5\t[{\"1\": 1, \"2\": 1, \"4\": 1}, 3, [1, 2, 4], \"V\"]\r\n",
      "6\t[{}, 1, [1], \"V\"]\r\n"
     ]
    }
   ],
   "source": [
    "# okay, so let's bring it all together with\n",
    "# a nice while loop\n",
    "\n",
    "# set a flag to tell us whether to keep going\n",
    "flag = True\n",
    "\n",
    "# seta variable to keep track of our iterations\n",
    "iteration = 0\n",
    "\n",
    "# copy the input file to the steps folder\n",
    "input_file = 'steps/directed_temp_' + str(iteration) + '.txt'\n",
    "!cp directed_ready.txt $input_file\n",
    "\n",
    "# set a while loop that we keep iterating until\n",
    "# we finish\n",
    "while flag:\n",
    "    \n",
    "    # set the flag to false to stop iterating\n",
    "    flag = False\n",
    "    \n",
    "    # iterate our iteration counter\n",
    "    iteration = iteration + 1\n",
    "    \n",
    "    # set the file name for the output and input\n",
    "    input_file = 'steps/directed_temp_' + str(iteration-1) + '.txt'\n",
    "    output_file = 'steps/directed_temp_' + str(iteration) + '.txt'\n",
    "    \n",
    "    # call the function, direct the stdout to a \n",
    "    # file, but hold/store the standard error\n",
    "    stderr = !python MRshortCount.py $input_file > $output_file\n",
    "    \n",
    "    # if we've found a counter then \n",
    "    # go ahead and keep going\n",
    "    if \"\\tNodes\" in stderr:\n",
    "        flag = True\n",
    "    \n",
    "# print out the final graph\n",
    "filename = 'steps/directed_temp_' + str(iteration) + '.txt'\n",
    "!cat $filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move to the cloud\n",
    "<img src=\"https://dl.dropboxusercontent.com/u/37624818/W261_Week7/buzzlightyear.jpg\" alt=\"buzz\" style=\"width: 200px;\"/>\n",
    "*[Source](http://feelgrafix.com/data_images/out/28/987639-buzz-lightyear.jpg)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /Users/Alex/.mrjob.conf\n",
      "Using s3://mrjob-f8c316b67324528f/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/k8/fy2j66nj4xsczx6cbcxhjlvm0000gn/T/no_script.Alex.20160710.150924.445730\n",
      "Copying local files to s3://mrjob-f8c316b67324528f/tmp/no_script.Alex.20160710.150924.445730/files/...\n",
      "j-2YV9UWK5JVJFL\n"
     ]
    }
   ],
   "source": [
    "# create the cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-1 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job complete with 4 iterations.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRshortCount import MRshortCount\n",
    "\n",
    "# clear the output to make space for the \n",
    "# next iteration\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/test --quiet\n",
    "\n",
    "# seta variable to keep track of our iterations\n",
    "iteration = 0\n",
    "\n",
    "# set the flag to true that we will keep iterating\n",
    "flag = True\n",
    "\n",
    "# copy the input file to the steps folder on S3\n",
    "input_file = 's3://aks-w261-hw7/test/iter_' + str(iteration)\n",
    "!aws s3 cp directed_ready.txt $input_file --quiet\n",
    "\n",
    "# while we are continuing to iterate\n",
    "while flag: \n",
    "\n",
    "    # set the flag to false, that we'll stop\n",
    "    # unless we count any nodes enqueue\n",
    "    flag = False\n",
    "    \n",
    "    # increment the iteration counters\n",
    "    iteration = iteration + 1\n",
    "\n",
    "    # set the input and output file names\n",
    "    input_file = 's3://aks-w261-hw7/test/iter_' + str(iteration-1)\n",
    "    output_file = '--output-dir=s3://aks-w261-hw7/test/iter_' + str(iteration)\n",
    "\n",
    "    # set the data that we're going to pull\n",
    "    mr_job = MRshortCount(args=[input_file, \\\n",
    "                              '-r','emr',\\\n",
    "                              '--cluster-id=j-2YV9UWK5JVJFL', \\\n",
    "                              '--aws-region=us-west-1', \\\n",
    "                              output_file, \\\n",
    "                              '--no-output'\\\n",
    "                             ]) \n",
    "\n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        # grab all the counters\n",
    "        all_counters = runner.counters()[0]\n",
    "\n",
    "        # if we encounter a node enqueue\n",
    "        # set the flag to true to run \n",
    "        # another iteration\n",
    "        if 'Nodes' in all_counters:\n",
    "            flag = True\n",
    "            \n",
    "# let us know that we've completed the job\n",
    "print \"Job complete with\", iteration, \"iterations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Prepare the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRgraphset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRgraphset.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRgraphset(MRJob):\n",
    "    \n",
    "    # set the source node\n",
    "    source = None\n",
    "    \n",
    "    # configure the options so that we can \n",
    "    # pass in the source node of interest\n",
    "    def configure_options(self):\n",
    "        super(MRgraphset, self).configure_options()\n",
    "        self.add_passthrough_option(\"--indx\", type='int', default=1)\n",
    "\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\\\n",
    "                        mapper=self.mapper)] \n",
    "    \n",
    "    \n",
    "    # the mapper init sets the source node\n",
    "    def mapper_init(self):\n",
    "        self.source = self.options.indx        \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # outputs the node, its path to \n",
    "    # the source, its distance to the \n",
    "    # source, it's linked nodes, and its\n",
    "    # status as visited\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        edges = ast.literal_eval(line[1])\n",
    "        \n",
    "        # initalize distance, path, and\n",
    "        # status\n",
    "        dist = 0\n",
    "        path = []\n",
    "        status = None\n",
    "        \n",
    "        # if this is the source node\n",
    "        if node == self.source:\n",
    "            \n",
    "            # set the distance to 0\n",
    "            # the path to none and \n",
    "            # the status to 'Q' for\n",
    "            # queue\n",
    "            dist = 0\n",
    "            path = []\n",
    "            status = 'Q'\n",
    "            \n",
    "        # else if this is not the source node\n",
    "        else:\n",
    "            \n",
    "            # set the distance to a max\n",
    "            # value, the path to null, and \n",
    "            # the status to 'U' for unvisited\n",
    "            dist = sys.maxint\n",
    "            path = []\n",
    "            status = 'U'\n",
    "            \n",
    "        # for each node yield the initial\n",
    "        # graph state\n",
    "        key = node\n",
    "        value = (edges,dist,path,status)\n",
    "        yield key,value\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRgraphset.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have completed preparing the data on the cloud\r\n"
     ]
    }
   ],
   "source": [
    "# run the program with the cluster we \n",
    "# just spun up\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/wiki_ready --quiet\n",
    "!python MRgraphset.py -r emr s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt \\\n",
    "    --indx=6176135 \\\n",
    "    --cluster-id=j-2YV9UWK5JVJFL \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/wiki_ready \\\n",
    "    --no-output \\\n",
    "    --quiet\n",
    "!echo We have completed preparing the data on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLOUD: Step through the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import the MRJobs that we created\n",
    "from MRshortCount import MRshortCount\n",
    "\n",
    "# clear the output to make space for the \n",
    "# next iteration\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/out_7-4 --quiet\n",
    "\n",
    "# seta variable to keep track of our iterations\n",
    "iteration = 0\n",
    "\n",
    "# set the flag to true that we will keep iterating\n",
    "flag = True\n",
    "\n",
    "# copy the input file to the steps folder on S3\n",
    "input_file = 's3://aks-w261-hw7/out_7-4/iter_' + str(iteration)\n",
    "!aws s3 cp s3://aks-w261-hw7/wiki_ready $input_file --quiet --recursive\n",
    "\n",
    "# while we are continuing to iterate\n",
    "while flag: \n",
    "\n",
    "    # set the flag to false, that we'll stop\n",
    "    # unless we count any nodes enqueue\n",
    "    flag = False\n",
    "    \n",
    "    # increment the iteration counters\n",
    "    iteration = iteration + 1\n",
    "\n",
    "    # set the input and output file names\n",
    "    input_file = 's3://aks-w261-hw7/out_7-4/iter_' + str(iteration-1)\n",
    "    output_file = '--output-dir=s3://aks-w261-hw7/out_7-4/iter_' + str(iteration)\n",
    "\n",
    "    # set the data that we're going to pull\n",
    "    mr_job = MRshortCount(args=[input_file, \\\n",
    "                              '-r','emr',\\\n",
    "                              '--cluster-id=j-2YV9UWK5JVJFL', \\\n",
    "                              '--aws-region=us-west-1', \\\n",
    "                              output_file, \\\n",
    "                              '--no-output'\\\n",
    "                             ]) \n",
    "\n",
    "    # create the runner and run it\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        # grab all the counters\n",
    "        all_counters = runner.counters()[0]\n",
    "\n",
    "        # if we encounter a node enqueue\n",
    "        # set the flag to true to run \n",
    "        # another iteration\n",
    "        if 'Nodes' in all_counters:\n",
    "            flag = True\n",
    "            \n",
    "# let us know that we've completed the job\n",
    "print \"Job complete with\", iteration, \"iterations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aks-w261-hw7/out_7-4/iter_59/_SUCCESS to wiki/final/_SUCCESS\r\n",
      "Completed 1 of 346 part(s) with 15 file(s) remaining\r"
     ]
    }
   ],
   "source": [
    "# save the completed file locally\n",
    "iteration = 59\n",
    "filename = 's3://aks-w261-hw7/out_7-4/iter_' + str(iteration)\n",
    "!aws s3 cp $filename wiki/final/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find our destination node\n",
    "We write an MRJob class to find our destination node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile MRgraphfind.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRgraphfind(MRJob):\n",
    "    \n",
    "    # set the destination node\n",
    "    destination = None\n",
    "    \n",
    "    # configure the options so that we can \n",
    "    # pass in the source node of interest\n",
    "    def configure_options(self):\n",
    "        super(MRgraphset, self).configure_options()\n",
    "        self.add_passthrough_option(\"--indx\", type='int', default=1)\n",
    "\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we use a single reducer because the \n",
    "        # output file is tiny and we want only \n",
    "        # a single file to deal with it since the\n",
    "        # file only has 1 line\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF, \\\n",
    "                       mapper_init = self.mapper_init,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       reducer=self.reducer\\\n",
    "                      )] \n",
    "    \n",
    "    \n",
    "    # the mapper init sets the source node\n",
    "    def mapper_init(self):\n",
    "        self.destination = int(self.options.indx)        \n",
    "    \n",
    "    \n",
    "    # the mapper takes each line and \n",
    "    # determines if its the destination node\n",
    "    # we're interested in\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        node = int(line[0])\n",
    "        \n",
    "        # if this is the node we're interested\n",
    "        # in \n",
    "        if node == destination:\n",
    "        \n",
    "            # grab the payload and the path\n",
    "            payload = ast.literal_eval(line[1])\n",
    "            path = payload[2]\n",
    "            \n",
    "            # yield the node and the path\n",
    "            yield node,path\n",
    "            \n",
    "    # the reducer yields the node and \n",
    "    # the path \n",
    "    def reducer(self,node,paths):\n",
    "        for path in paths:\n",
    "            yield node,paths\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRgraphfind.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the program to find the index of \n",
    "# interest\n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/wiki_path --quiet\n",
    "!python MRgraphfind.py -r emr $filename \\\n",
    "    --indx=13466359 \\\n",
    "    --cluster-id=j-2YV9UWK5JVJFL \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/wiki_path \\\n",
    "    --no-output \\\n",
    "    --quiet\n",
    "\n",
    "!echo Path has been found\n",
    "\n",
    "# bring the file down locally\n",
    "!aws s3 cp s3://aks-w261-hw7/wiki_path/part-00000 wiki/prelim_path.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rewrite the file we just created it\n",
    "# to make it suitable for our MRjob \n",
    "# join function\n",
    "\n",
    "# open the originally created path document\n",
    "with open('wiki/prelim_path.txt','r') as myfile:\n",
    "    \n",
    "    # create a new file\n",
    "    with open('wiki/path.txt','w') as mynewfile:\n",
    "        \n",
    "        # loop through the lines in the\n",
    "        # original file, writing just the path \n",
    "        # to the new file\n",
    "        for line in myfile.readlines():\n",
    "            line = line.split('\\t')\n",
    "            mynewfile.write(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the path with its labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile MRjoinInd.py\n",
    "\n",
    "# import the MRJob class\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# import libraries to help us\n",
    "# get our job done\n",
    "import ast\n",
    "import sys\n",
    "  \n",
    "class MRjoinInd(MRJob):\n",
    "    \n",
    "    # store the indices of interest\n",
    "    path = []\n",
    "    path_labels = [0]*len(path)\n",
    "    \n",
    "    # define the steps for the MapReduce job\n",
    "    def steps(self):\n",
    "        \n",
    "        # we set a single reducer so that we\n",
    "        # can make a single file with all the \n",
    "        # indices linked up.\n",
    "        # if we find this approach doesn't\n",
    "        # scale, we can always up this without\n",
    "        # affecting the job's functionality\n",
    "        JOBCONF = {        \n",
    "            'mapreduce.job.reduces': 1,\n",
    "        }\n",
    "        \n",
    "        return [MRStep(jobconf = JOBCONF,\\\n",
    "                       mapper_init=self.mapper_init,\\\n",
    "                       mapper=self.mapper,\\\n",
    "                       reducer_init=self.mapper_init,\\\n",
    "                       reducer=self.reducer,\\\n",
    "                       reducer_final=self.reducer_final)]        \n",
    "    \n",
    "    \n",
    "    # the mapper_init loads the list of indices\n",
    "    # of interest into memory\n",
    "    def mapper_init(self):\n",
    "        \n",
    "        # open the path file, that contains\n",
    "        # the list of indices of interest\n",
    "        with open('path.txt','r') as myfile:\n",
    "            \n",
    "            # read the line in the file\n",
    "            for line in myfile.readlines():\n",
    "                self.path = ast.literal_eval(line)\n",
    "                \n",
    "        # geneate an empty array for the labels\n",
    "        self.path_labels = [0]*len(self.path)\n",
    "            \n",
    "    \n",
    "    # the mapper checks each line of the total\n",
    "    # index with matching labels and emits\n",
    "    # only those that match with our stored\n",
    "    # list in the path\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # split the line into its length\n",
    "        # and the path\n",
    "        line = line.strip().split('\\t')\n",
    "        label = line[0]\n",
    "        index = int(line[1])\n",
    "\n",
    "        # check to see if this is in our\n",
    "        # path, and if so, yield it out\n",
    "        if index in self.path:\n",
    "            yield index,label\n",
    "\n",
    "    \n",
    "    # the reducer updates the path labels list\n",
    "    def reducer(self, index, labels):\n",
    "\n",
    "        # grab the index \n",
    "        path_index = self.path.index(index)\n",
    "\n",
    "        # place the label in the \n",
    "        # path labels place in the appropriate\n",
    "        # place\n",
    "        self.path_labels[path_index] = list(labels)[0]\n",
    "        \n",
    "        \n",
    "    # the reducer final yields the completed\n",
    "    # list for the path\n",
    "    def reducer_final(self):\n",
    "        \n",
    "        # loop through each element in the \n",
    "        # path and yield it with its label\n",
    "        for i,index in enumerate(self.path):\n",
    "            yield self.path[i],self.path_labels[i]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRjoinInd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy the final path up to the cloud\n",
    "!aws s3 cp wiki/path.txt s3://aks-w261-hw7/wiki_path.txt --quiet\n",
    "\n",
    "# perform the join \n",
    "!aws s3 rm --recursive s3://aks-w261-hw7/wiki_joins\n",
    "!python MRjoinInd.py -r emr s3://ucb-mids-mls-networks/wikipedia/indices.txt \\\n",
    "    --file=s3://aks-w261-hw7/wiki_path.txt \\\n",
    "    --cluster-id=j-2YV9UWK5JVJFL \\\n",
    "    --aws-region=us-west-1 \\\n",
    "    --output-dir=s3://aks-w261-hw7/wiki_joins \\\n",
    "    --no-output\n",
    "\n",
    "# copy the joined path back to local directory\n",
    "!aws cp s3 s3://aks-w261-hw7/wiki_joins/part-00000 wiki/path_labelled.txt\n",
    "\n",
    "!echo We have joined our findings with the labels\n",
    "!echo Shortest path is:\n",
    "!cat wiki/path_labelled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 7.5: Conceptual exercise: Largest single-source network distances\n",
    "\n",
    "Suppose you wanted to find the largest network distance from a single source,\n",
    "i.e., a node that is the furthest (but still reachable) from a single source.\n",
    "\n",
    "How would you implement this task? \n",
    "How is this different from finding the shortest path graph distances?\n",
    "\n",
    "Is this task more difficult to implement than the shortest path distance?\n",
    "\n",
    "As you respond, please comment on program structure, runtimes, iterations, general system requirements, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finding the longest path in a graph is [NP hard problem](https://en.wikipedia.org/wiki/Longest_path_problem). It cannot be solved in polynomial time. Finding the longest path requires us checking every path and comparing it. We can try thinking about it intuitively. In the single shortest path, we can check for the most direct path by going breadth first and then only re-looking if this other path is shorter than the existing shortest. However, with longest path, we won't be able to tell until we loop through all possible combinations. <br>\n",
    "<br>\n",
    "I would implement this task by modifying my reducer. My mapper would be substantially the same. In my reducer, currently, a node stays as visited once it's visited. If even another part of the graph attempts to queue it up next. However, to find the longest path, I would have to put the node back into 'queue' status. I would keep all my distances and all my paths as I moved along. At the end of it, I could take the longest path for each node. This approach would take many more iterations and would require significantly longer run times.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## HW 7.5.1: \n",
    "Can we utilize combiners in the HW 7 to perform the shortest path implementation?\n",
    "Does order inversion help with the HW 7 shortest path implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We could use combiners to perform the shortest path implementation. We would need to complexify our reducer just a bit. For example, we could use combiners to combine U's and Q's nodes. We could do the same thing with Q's and V's nodes. We would then to modify our reducer to simply pass the node if it was already combined. This would definitely be helpful. <br>\n",
    "<br>\n",
    "Order inversion would definitley be helpful if we're attempting to find the shortest path to a particular end node. We could pass that end node first, and once we see the end node and calculate the distance and path, we could pre-emptively end the program. This would potentially save time.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
