{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \\#2\n",
    "**Student: Alex Smith** <br>\n",
    "Course: W261 - Machine Learning at Scale <br>\n",
    "Professor: Jimi Shanahan <br>\n",
    "Due Date: May 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Useful resources\n",
    "When completing this notebook, I found the following resources particularly useful:\n",
    "- [Race conditions, mutual exclusion, synchronization, and parallel slowdown](https://en.wikipedia.org/wiki/Parallel_computing#Race_conditions.2C_mutual_exclusion.2C_synchronization.2C_and_parallel_slowdown)\n",
    "- [Data-Intensive Text Processing with MapReduce](https://lintool.github.io/MapReduceAlgorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "The following libraries must be installed before running the below code. They can all be installed through [Pip](https://github.com/pypa/pip).\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "- [Regular Expression](https://docs.python.org/2/library/re.html)\n",
    "- [Pretty Table](https://pypi.python.org/pypi/PrettyTable)\n",
    "- [Random](https://docs.python.org/2/library/random.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Environment: Cloudera\n",
    "This notebook is designed to run in a Cloudera virtual box. To set up a virtual box like this one, follow the instructions [here](https://docs.google.com/presentation/d/1qCQM-2U2C6e584uM9kqTGr675K3_a8M1mEZaiT4Wmi8/edit#slide=id.p). Before beginning, make sure that you have started (in the following order) from the Cloudera manager:\n",
    "1. Zookeeper\n",
    "1. Yarn\n",
    "1. HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we create some generic folders to make running\n",
    "# the hadoop commands simpler\n",
    "# you should have already made the w261 directory\n",
    "# when you followed the cloudera installation\n",
    "# instructions\n",
    "!mkdir ~/w261/Outputs\n",
    "!mkdir ~/w261/data\n",
    "!hdfs dfs -mkdir -p /user/cloudera/w261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.0.  \n",
    "*What is a race condition in the context of parallel computation? Give an example. <br>\n",
    "What is MapReduce? <br>\n",
    "How does it differ from Hadoop?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of parallel computation, the **race condition** is when two parallel processes attempt to access and update some variable. If the sequence of access and update can change the variable outcome, we have a potential race condition. Let's consider an example inspired by Async 2.4. We have two parallel programs, A and B.\n",
    "- Program A: \n",
    "    - Read variable V\n",
    "    - Assign to V: V • 2    \n",
    "- Program B:\n",
    "    - Read variable V\n",
    "    - Assign to V: V • V\n",
    "\n",
    "We have different conditions based on the sequence of events:\n",
    "- Case 1: Both programs could read V at the same time, Program A updates V last\n",
    "    - Both programs could read V at the same time\n",
    "    - Program B updates V to V<sup>2</sup>\n",
    "    - Program A updates V to 2V\n",
    "    - **Final result: 2V**\n",
    "- Case 2: Both programs could read V at the same time, Program B updates V last \n",
    "    - Both programs could read V at the same time\n",
    "    - Program A updates V to 2V\n",
    "    - Program B updates V to V<sup>2</sup>\n",
    "    - **Final result: V<sup>2</sup>**\n",
    "- Case 3: Program A reads V first, then Program B\n",
    "    - Program A updates V to 2V\n",
    "    - Program B reads the updated value of V\n",
    "    - Program B updates V to V<sup>2</sup>\n",
    "    - **Final result: 4V<sup>2</sup>**\n",
    "- Case 4: Program B reads V first, then Program A\n",
    "    - Program B updates V to V<sup>2</sup>\n",
    "    - Program A reads the updated value of V\n",
    "    - Program A updates V to 2V\n",
    "    - **Final result: 2V<sup>2</sup>**\n",
    "\n",
    "**MapReduce** is a framework that allows for parallelization of computation across many nodes. It is an abstraction that handles most of the details of the parallelization behind the scenes while allowing the programmer to focus on writing parallel computations. One of the major advantages of MapReduce is that it moves the code to the data rather than the data to the code. This is helpful because moving data to code is time intensive and risky. MapReduce is designed to handle large data sets and distribute the computation over them. The name \"MapReduce\" comes from the mappers and reducers that make up the programming. We can think of the mapper by comparing it to the mapper in *functional programming* that applies a function to every value of its input. We can think of the reducer by comparing it othe fold function in functional programming that combines the results of multiple outputs from the mappers. **Hadoop** is a specific open source implementation of MapReduce. It differs slightly from other implementations of MapReduce, such as Google's proprietary implementation. In addition to a MapReduce framework, Hadoop also includes a distributed file system for holding large amounts of data. Hadoop is based on the **functional programming paradigm** of MapReduce. It is built on top of Java, a non-functional programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.0.1  \n",
    "*Which programming paradigm is Hadoop based on? Explain and give a simple example of functional programming in raw python code and show the code running. E.g., in raw python find the average length of a string in and of strings using a python \"map-reduce\" (functional programming) job (similar in style to the above). Alternatively, you can do this in python Hadoop Streaming.<br>\n",
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]*<br>\n",
    "**We answer this last part of the question in the last paragraph of the part above because it flows with that information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the strings is 8\n"
     ]
    }
   ],
   "source": [
    "# import our libraries for functional programming\n",
    "import functools\n",
    "\n",
    "# set our strings\n",
    "strings = \\\n",
    "[\"str1\",\"string2\",\"w261\",\"MAchine learning at SCALE\"]\n",
    "\n",
    "# add a length of zero to front of the string\n",
    "strings.insert(0,0)\n",
    "\n",
    "# calculate the average\n",
    "average = \\\n",
    "functools.reduce(lambda x,y:x+len(y),strings)/len(strings)\n",
    "\n",
    "print \"The average length of the strings is\",average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.1. Sort in Hadoop MapReduce\n",
    "*Given as input: Records of the form (integer, “NA”), where integer is any integer, and “NA” is just the empty string.\n",
    "<br>Output: sorted key value pairs of the form (integer, “NA”) in decreasing order.\n",
    "<br>What happens if you have multiple reducers? Do you need additional steps? Explain.<br>\n",
    "Write code to generate N  random records of the form (integer, “NA”). Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Multiple reducers\n",
    "If we have multiple reducers, we need to make sure that we sort the inputs before they enter the reducers. Each reducer should get all the mapper outputs for a given key. This ensures that the operations done on a given key are complete by the time of the reducer's output. If we're sorting the outputs from all the reducers, we might need a final reducer to complete that sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate 10,000 random numbers\n",
    "We write a program to generate 10,000 random integers between 1 and 10,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the library random to help us generate\n",
    "# the random numbers\n",
    "import random\n",
    "\n",
    "def generateN(N=10000,lower=1,upper=10000,\n",
    "             filename=\"data/random_input.txt\"):\n",
    "    \"\"\"generate N random integers between\n",
    "    the upper and lower limits and print\n",
    "    these numbers out to a file in the form:\n",
    "    integer <tab> empty string <new line>\"\"\"\n",
    "    \n",
    "    # create a new file \n",
    "    with open(filename,\"w\") as myfile:\n",
    "        \n",
    "        # create a new line for each random\n",
    "        # integer specified\n",
    "        for n in range(N):\n",
    "            \n",
    "            # get the line we are writing\n",
    "            info = str(random.randint(lower,upper)) + \\\n",
    "            \"\\t\" + \"\" + \"\\n\"\n",
    "            \n",
    "            # write the information to the file\n",
    "            myfile.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the file of random integers\n",
    "generateN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper\n",
    "We write a mapper function to take as input a file with a list of integers and sort it from smallest to largest and output to another file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW2.1\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# create a list and read each line into it\n",
    "numbers = []\n",
    "\n",
    "# loop through each line\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split off any extra spaces\n",
    "    number = line.strip()\n",
    "\n",
    "    # add the number to the list as an\n",
    "    # integer\n",
    "    numbers.append(int(number))\n",
    "\n",
    "# loop through each number and print it out\n",
    "# for number in numbers_nodupes:\n",
    "for number in sorted(numbers):\n",
    "    print number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the mapper on the command-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\r\n",
      "6\r\n",
      "8\r\n",
      "9\r\n",
      "9\r\n",
      "11\r\n",
      "14\r\n",
      "15\r\n",
      "16\r\n",
      "18\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py\n",
    "!cat data/random_input.txt | ~/w261/mapper.py  > testingM.txt\n",
    "!head testingM.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "We write a reducer function that takes the outputs from the multiple mapper functions and sorts each list by comparing the first elements from each list. Our reducer function does not have to do much work because Hadoop already sorts the outputs from the mappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW2.1\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# create an array to hold all the numbers\n",
    "numbers = []\n",
    "\n",
    "# loop through each number and append it to\n",
    "# the list\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # strip away random line breaks\n",
    "    number = line.strip()\n",
    "    \n",
    "    # append the number to our list of numbers\n",
    "    numbers.append(int(number))\n",
    "\n",
    "# print out the sorted list as our output\n",
    "for number in numbers:\n",
    "    print number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing our reducer on the commandline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\r\n",
      "6\r\n",
      "8\r\n",
      "9\r\n",
      "9\r\n",
      "11\r\n",
      "14\r\n",
      "15\r\n",
      "16\r\n",
      "18\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x reducer.py\n",
    "!cat testingM.txt | ~/w261/reducer.py  > testingR.txt\n",
    "!head testingR.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Command line code to run our program in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/25 19:36:18 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/random_input.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/random_input.txt1464230178920\n",
      "16/05/25 19:36:29 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-2-1' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-2-11464230189333\n",
      "16/05/25 19:36:30 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob896569251327698838.jar tmpDir=null\n",
      "16/05/25 19:36:33 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/25 19:36:33 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/25 19:36:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/25 19:36:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/25 19:36:35 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/25 19:36:35 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/25 19:36:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464226260520_0003\n",
      "16/05/25 19:36:36 INFO impl.YarnClientImpl: Submitted application application_1464226260520_0003\n",
      "16/05/25 19:36:36 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464226260520_0003/\n",
      "16/05/25 19:36:36 INFO mapreduce.Job: Running job: job_1464226260520_0003\n",
      "16/05/25 19:36:51 INFO mapreduce.Job: Job job_1464226260520_0003 running in uber mode : false\n",
      "16/05/25 19:36:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/25 19:37:28 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/25 19:37:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/25 19:37:41 INFO mapreduce.Job: Job job_1464226260520_0003 completed successfully\n",
      "16/05/25 19:37:41 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26688\n",
      "\t\tFILE: Number of bytes written=423642\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=88546\n",
      "\t\tHDFS: Number of bytes written=58872\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8621440\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1173248\n",
      "\t\tTotal time spent by all map tasks (ms)=67355\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9166\n",
      "\t\tTotal vcore-seconds taken by all map tasks=67355\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9166\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8621440\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1173248\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=58872\n",
      "\t\tMap output materialized bytes=34378\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6350\n",
      "\t\tReduce shuffle bytes=34378\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=234\n",
      "\t\tCPU time spent (ms)=5320\n",
      "\t\tPhysical memory (bytes) snapshot=380956672\n",
      "\t\tVirtual memory (bytes) snapshot=2181541888\n",
      "\t\tTotal committed heap usage (bytes)=139460608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=88308\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=58872\n",
      "16/05/25 19:37:41 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-2-1\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/random_input.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-2-1\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_2_1\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-2-1\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-2-1/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Display the highest and lowest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest ten numbers are: [9991, 9991, 9992, 9996, 9996, 9997, 9998, 9999, 9999, 10000]\n",
      "The lowest ten numbers are: [1, 2, 3, 3, 3, 3, 4, 6, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "# set a list to hold our numbers\n",
    "numbers = []\n",
    "\n",
    "# open the file as readable\n",
    "with open(\"Outputs/Out_2_1\",\"r\") as myfile:\n",
    "    \n",
    "    # read in all the lines and loop through them\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # add each line to the list\n",
    "        numbers.append(int(line))\n",
    "    \n",
    "# print out the highest and lowest ten numbers\n",
    "print \"The highest ten numbers are:\", numbers[-10:]\n",
    "print \"The lowest ten numbers are:\", numbers[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.2.  WORDCOUNT\n",
    "*Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "<br>CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "<br>8\n",
    "<br>#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "We write a mapper function that takes a chunk of text and outputs a tab delimited list with each word and its associated count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments; initalize the count\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# create a dictionary that stores the word and its\n",
    "# associated count\n",
    "wordcounts = {}\n",
    "    \n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # convert the line into a list of words without punctuation\n",
    "    # also lowercase all the words\n",
    "    words = wordify.findall(line.lower())\n",
    "\n",
    "    # loop through each word in each line\n",
    "    for word in words:\n",
    "\n",
    "        # if the word is not in the dictionary, \n",
    "        # initalize the dictionary entry with a \n",
    "        # count of zero\n",
    "        if word not in wordcounts.keys():\n",
    "            wordcounts[word] = 1\n",
    "\n",
    "        # otherwise, incremement the count by 1\n",
    "        else:\n",
    "            wordcounts[word] = wordcounts[word]+1\n",
    "\n",
    "# loop through each word and its associated word count\n",
    "# print it out to the system\n",
    "for word in wordcounts.keys():\n",
    "    print word,\"\\t\",wordcounts[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the mapper in the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow \t1\r\n",
      "four \t8\r\n",
      "prefix \t1\r\n",
      "railing \t1\r\n",
      "looking \t4\r\n",
      "granting \t1\r\n",
      "electricity \t1\r\n",
      "originality \t1\r\n",
      "homemakers \t1\r\n",
      "hormone \t2\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x mapper.py\n",
    "!cat data/enronemail_1h.txt | ~/w261/mapper.py > testingM.txt\n",
    "!head testingM.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "The reducer function takes the outputs from the many mappers and combines them to find the word count for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "# import the system libraries to read from the input\n",
    "import sys\n",
    "\n",
    "# initalize a dictionary to hold each word and\n",
    "# its associated count\n",
    "wordcounts = {}\n",
    "\n",
    "# loop through each line in the input\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.split(\"\\t\")\n",
    "    word = line[0].strip()\n",
    "    count = int(line[1])\n",
    "    \n",
    "    # if the word is not in the dictionary, \n",
    "    # initalize the dictionary entry\n",
    "    if word not in wordcounts.keys():\n",
    "        wordcounts[word] = count\n",
    "\n",
    "    # otherwise, incremement add to the count\n",
    "    else:\n",
    "        wordcounts[word] = wordcounts[word]+count\n",
    "    \n",
    "# loop through each word in the dictionary and \n",
    "# print the word with its count\n",
    "for word in wordcounts.keys():\n",
    "    print word,\"\\t\",wordcounts[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the reducer on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow \t1\r\n",
      "four \t8\r\n",
      "prefix \t1\r\n",
      "railing \t1\r\n",
      "looking \t4\r\n",
      "granting \t1\r\n",
      "electricity \t1\r\n",
      "originality \t1\r\n",
      "homemakers \t1\r\n",
      "hormone \t2\r\n"
     ]
    }
   ],
   "source": [
    "# set permissions and execute\n",
    "!chmod +x reducer.py\n",
    "!cat testingM.txt | ~/w261/reducer.py > testingR.txt\n",
    "!head testingR.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Set the right permissions and execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/25 19:53:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/random_input.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/random_input.txt1464231220472\n",
      "rm: `/user/cloudera/w261-output-2-2': No such file or directory\n",
      "16/05/25 19:53:49 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2359727465864169885.jar tmpDir=null\n",
      "16/05/25 19:53:51 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/25 19:53:52 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/25 19:53:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/25 19:53:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/25 19:53:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464226260520_0004\n",
      "16/05/25 19:53:54 INFO impl.YarnClientImpl: Submitted application application_1464226260520_0004\n",
      "16/05/25 19:53:54 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464226260520_0004/\n",
      "16/05/25 19:53:54 INFO mapreduce.Job: Running job: job_1464226260520_0004\n",
      "16/05/25 19:54:05 INFO mapreduce.Job: Job job_1464226260520_0004 running in uber mode : false\n",
      "16/05/25 19:54:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/25 19:54:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/25 19:54:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/25 19:54:48 INFO mapreduce.Job: Job job_1464226260520_0004 completed successfully\n",
      "16/05/25 19:54:48 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=49339\n",
      "\t\tFILE: Number of bytes written=466879\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=233302\n",
      "\t\tHDFS: Number of bytes written=64666\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7705728\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=973184\n",
      "\t\tTotal time spent by all map tasks (ms)=60201\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7603\n",
      "\t\tTotal vcore-seconds taken by all map tasks=60201\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7603\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7705728\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=973184\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=7091\n",
      "\t\tMap output bytes=74643\n",
      "\t\tMap output materialized bytes=55924\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5511\n",
      "\t\tReduce shuffle bytes=55924\n",
      "\t\tReduce input records=7091\n",
      "\t\tReduce output records=5511\n",
      "\t\tSpilled Records=14182\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=608\n",
      "\t\tCPU time spent (ms)=4070\n",
      "\t\tPhysical memory (bytes) snapshot=378404864\n",
      "\t\tVirtual memory (bytes) snapshot=2207944704\n",
      "\t\tTotal committed heap usage (bytes)=142606336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=233062\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64666\n",
      "16/05/25 19:54:48 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-2-2\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/enronemail_1h.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-2-2\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_2_2\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-2-2\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-2-2/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check how many times 'assistance' appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word assistance appears 10 times.\n"
     ]
    }
   ],
   "source": [
    "# set the word we're looking for\n",
    "FIND = 'assistance'\n",
    "\n",
    "# open the output file\n",
    "with open('Outputs/Out_2_2', 'r') as myfile:\n",
    "    \n",
    "    # loop through each line\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # separate each line by the tab\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the value\n",
    "        word = line[0].strip()\n",
    "        count = int(line[1])\n",
    "        \n",
    "        # if we've found the word assistance,\n",
    "        # break out of the loop\n",
    "        if word == FIND: break\n",
    "\n",
    "# print out that we found the word and its count\n",
    "print \"The word\", word, \"appears\", count, \"times.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.2.1  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Mapper function\n",
    "We write a simple mapper function that takes the output of the previous reducer and makes the count the key and the word the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW2.2.1\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments; initalize the count\n",
    "import sys\n",
    "    \n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line based on the tab\n",
    "    line = line.split(\"\\t\")\n",
    "    \n",
    "    # set the new key value pair\n",
    "    info = str(line[1].strip()) + \"\\t\" + str(line[0].strip())\n",
    "\n",
    "    # print the new key value pair out\n",
    "    print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the mapper on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tconsidered\r\n",
      "1\tscreaming\r\n",
      "1\tsending\r\n",
      "2\taudio\r\n",
      "1\tlinda's\r\n",
      "1\twood\r\n",
      "4\tadvice\r\n",
      "7\twriting\r\n",
      "2\tdiscontinue\r\n",
      "1\treclaimers\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x mapper.py\n",
    "!cat Outputs/Out_2_2 | ~/w261/mapper.py > testingM.txt\n",
    "!head testingM.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "We write a simple reducer function that simply yields the sorted list from the mapper. This is a good reducer to build because then we can use it to test other mappers since it only yields what the mapper gives it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW2.2.1\n",
    "\n",
    "# import the system libraries to read from the input\n",
    "import sys\n",
    "\n",
    "# loop through each line in the input and print\n",
    "# it out\n",
    "for line in sys.stdin:\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the reducer on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247\tthe\r\n",
      "963\tto\r\n",
      "668\tand\r\n",
      "566\tof\r\n",
      "542\ta\r\n",
      "432\tyou\r\n",
      "417\tin\r\n",
      "394\tyour\r\n",
      "382\tect\r\n",
      "373\tfor\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x reducer.py\n",
    "!cat testingM.txt |sort -k1nr | ~/w261/reducer.py > testingR.txt\n",
    "!head testingR.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the files executable and run the hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 18:12:51 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/Out_2_2' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/Out_2_21464311571945\n",
      "16/05/26 18:12:59 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-2-2-1' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-2-2-11464311579169\n",
      "16/05/26 18:13:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3428225655655091828.jar tmpDir=null\n",
      "16/05/26 18:13:04 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 18:13:04 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 18:13:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/26 18:13:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/26 18:13:06 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/26 18:13:06 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/26 18:13:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464226260520_0007\n",
      "16/05/26 18:13:06 INFO impl.YarnClientImpl: Submitted application application_1464226260520_0007\n",
      "16/05/26 18:13:06 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464226260520_0007/\n",
      "16/05/26 18:13:06 INFO mapreduce.Job: Running job: job_1464226260520_0007\n",
      "16/05/26 18:13:15 INFO mapreduce.Job: Job job_1464226260520_0007 running in uber mode : false\n",
      "16/05/26 18:13:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/26 18:13:39 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/05/26 18:13:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/26 18:13:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/26 18:13:50 INFO mapreduce.Job: Job job_1464226260520_0007 completed successfully\n",
      "16/05/26 18:13:50 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=45116\n",
      "\t\tFILE: Number of bytes written=454110\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=97219\n",
      "\t\tHDFS: Number of bytes written=53644\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5650176\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=944896\n",
      "\t\tTotal time spent by all map tasks (ms)=44142\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7382\n",
      "\t\tTotal vcore-seconds taken by all map tasks=44142\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7382\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5650176\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=944896\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5511\n",
      "\t\tMap output records=5511\n",
      "\t\tMap output bytes=53644\n",
      "\t\tMap output materialized bytes=46409\n",
      "\t\tInput split bytes=220\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=109\n",
      "\t\tReduce shuffle bytes=46409\n",
      "\t\tReduce input records=5511\n",
      "\t\tReduce output records=5511\n",
      "\t\tSpilled Records=11022\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=298\n",
      "\t\tCPU time spent (ms)=3960\n",
      "\t\tPhysical memory (bytes) snapshot=370036736\n",
      "\t\tVirtual memory (bytes) snapshot=2190852096\n",
      "\t\tTotal committed heap usage (bytes)=141557760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=96999\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53644\n",
      "16/05/26 18:13:50 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-2-2-1\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/Outputs/Out_2_2 /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-2-2-1\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_2_2_1\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-2-2-1\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-2-2-1/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_2_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top ten\n",
    "We create a function to read in the output of the MapReduce and print out the top 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequently occuring words are:\n",
      "\tfor\t1247\n",
      "\tect\t963\n",
      "\tyour\t668\n",
      "\tin\t566\n",
      "\tyou\t542\n",
      "\ta\t432\n",
      "\tof\t417\n",
      "\tand\t394\n",
      "\tto\t382\n",
      "\tthe\t373\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold the top words\n",
    "# and the counts for each word\n",
    "words = []\n",
    "counts = []\n",
    "\n",
    "# open the file\n",
    "with open(\"Outputs/Out_2_2_1\",\"r\") as myfile:\n",
    "    \n",
    "    # read the lines and save them to the lists\n",
    "    for line in myfile.readlines():\n",
    "        # split the line into a list by tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # set the words and counts\n",
    "        word = str(line[1].strip())\n",
    "        count = int(line[0].strip())\n",
    "        \n",
    "        # append the word and counts to our\n",
    "        # master lists\n",
    "        words.append(word)\n",
    "        counts.append(count)\n",
    "\n",
    "# print the most frequently occuring words\n",
    "print \"The most frequently occuring words are:\"\n",
    "for index,word in enumerate(words[-10:]):\n",
    "    print \"\\t\" + word + \"\\t\" + str(counts[-index-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.3. Multinomial Naive Bayes with no Smoothing\n",
    "*Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "<br>the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "<br>E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "<br>Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see. \n",
    "<br>Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "<br>Let DF represent the evalution set in the following:\n",
    "<br>Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "<br>Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "The goal of this function is to spit out the occurences for each word. The input will be a chunk of email messages. The output will be a tab delimited file:\n",
    "- email id\n",
    "- spam indicator\n",
    "- word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# initalize the outputs that we'll be using\n",
    "email_id = None\n",
    "word_act = None\n",
    "spam_ind = None\n",
    "    \n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # separate out each line based on the tabs\n",
    "    line = line.split(\"\\t\")\n",
    "    \n",
    "    # set the email id and spam indicator fields\n",
    "    email_id = line[0]\n",
    "    spam_ind = line[1]\n",
    "\n",
    "    # grab the text from the body and \n",
    "    # subject and concatenate it, if both exist\n",
    "    if len(line)==4:\n",
    "        text = line[2] + \" \" + line[3]\n",
    "    else:\n",
    "        text = line[2]\n",
    "\n",
    "    # convert the text into a list of \n",
    "    # words without punctuation\n",
    "    # also lowercase all the words\n",
    "    words = wordify.findall(text.lower())\n",
    "        \n",
    "    # loop through each word\n",
    "    for word in words:\n",
    "\n",
    "        # collect the line that we want to print out\n",
    "        info = email_id + \"\\t\" + word + \"\\t\" + str(spam_ind)\n",
    "\n",
    "        # let's print each email id, word, \n",
    "        # spam indicator\n",
    "        # we'll separate each value with a tab character\n",
    "        print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the mapper function on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\tchristmas\t0\r\n",
      "0001.1999-12-10.farmer\ttree\t0\r\n",
      "0001.1999-12-10.farmer\tfarm\t0\r\n",
      "0001.1999-12-10.farmer\tpictures\t0\r\n",
      "0001.1999-12-10.farmer\tna\t0\r\n",
      "0001.1999-12-10.kaminski\tre\t0\r\n",
      "0001.1999-12-10.kaminski\trankings\t0\r\n",
      "0001.1999-12-10.kaminski\tthank\t0\r\n",
      "0001.1999-12-10.kaminski\tyou\t0\r\n",
      "0001.2000-01-17.beck\tleadership\t0\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x mapper.py\n",
    "!cat data/enronemail_1h.txt | ~/w261/mapper.py  > testingM.txt\n",
    "!head testingM.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "This function inputs the outputs from the multiple mapper functions. It uses this information to first develop a Naive Bayes classifier based on each email word. Second, it uses this Naive Bayes classifier to classify each email as spam or not spam. <br>\n",
    "In addition, it completes a number of other tasks:\n",
    "- writes a file storing the count of zero probabilities for each class\n",
    "- writes a file storing the posterior probability, P(spam | words) and P(nspam | words) for each document\n",
    "\n",
    "The reducer itself will output a tab delimited file with the email id, the true spam indicator, and the spam prediction indicator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "# import the system libraries to read \n",
    "# from the input, from the \n",
    "# math library import the log function\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "# create a dictionary to store each email id, \n",
    "# spam indciator, list of words of \n",
    "# interest, and spam prediction\n",
    "emails = {}\n",
    "    \n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "spam_ewords = 0 # number of words in spam emails\n",
    "\n",
    "# let's initalize a dictionary that will \n",
    "# hold the probabilities for \n",
    "# each and every word in the corpus\n",
    "words_probs = {}\n",
    "\n",
    "##\n",
    "##\n",
    "## gather summary statistics and words for each email\n",
    "##\n",
    "##\n",
    "        \n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by tabs\n",
    "    line = line.split(\"\\t\")\n",
    "\n",
    "    # pull out my values for each part of the line\n",
    "    email_id = line[0]\n",
    "    word_act = line[1]\n",
    "    spam_ind = int(line[2])\n",
    "\n",
    "    # let's update the number of words and \n",
    "    # the spam words (if it's spam)\n",
    "    all_words = all_words + 1\n",
    "    if spam_ind == 1:\n",
    "        spam_ewords = spam_ewords + 1\n",
    "\n",
    "    # if we don't already have the word in \n",
    "    # our dictionary of words, let's\n",
    "    # add it and initalize counts of zero\n",
    "    if word_act not in words_probs:\n",
    "        words_probs[word_act] = \\\n",
    "        {\"spam_count\":0,\"not_spam_count\":0}\n",
    "\n",
    "    # let's grab the dictionary for the word\n",
    "    word_dict = words_probs[word_act]\n",
    "\n",
    "    # let's increment the word counts \n",
    "    # for the word\n",
    "    if spam_ind == 1:\n",
    "        word_dict['spam_count'] = \\\n",
    "        word_dict['spam_count'] + 1\n",
    "    else:\n",
    "        word_dict['not_spam_count'] = \\\n",
    "        word_dict['not_spam_count'] + 1\n",
    "\n",
    "    # check to see if this email is already \n",
    "    # in the dictionary, and if\n",
    "    # its not already there, initalize it\n",
    "    if email_id not in emails:\n",
    "\n",
    "        # create a sub-dictionary within \n",
    "        # the email dictionary for each email\n",
    "        emails[email_id] = \\\n",
    "        {\"spam\":spam_ind, \"words\":[]}\n",
    "\n",
    "        # if it's not already there, \n",
    "        # let's also increment the email counter\n",
    "        # and the spam counter if it's spam\n",
    "        all_emails = all_emails + 1\n",
    "        if spam_ind == 1:\n",
    "            spam_emails = spam_emails + 1\n",
    "\n",
    "    # let's add the word to our list \n",
    "    # of words for this email\n",
    "    emails[email_id][\"words\"].append(word_act)\n",
    "\n",
    "##\n",
    "##\n",
    "## build the naive bayes classifier\n",
    "##\n",
    "##\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# for each word, let's calculate the \n",
    "# conditional probability of spam given the word\n",
    "# and not spam given the word\n",
    "\n",
    "# let's define our LaPlace smoother \n",
    "# (in this case, 0)\n",
    "SMOOTHER = 0\n",
    "VOCAB = 0\n",
    "\n",
    "# look at each word\n",
    "for word in words_probs.keys():\n",
    "    \n",
    "    # set the find word that we'll be \n",
    "    # calculating the probabilities for\n",
    "    word = words_probs[word]\n",
    "    \n",
    "    # calculate the probability of the word\n",
    "    word['total'] = word['spam_count'] + \\\n",
    "    word['not_spam_count']\n",
    "    word['probs'] = float(word['total']) / \\\n",
    "    float(all_words)\n",
    "    \n",
    "    # calculate the probability of word \n",
    "    # given spam and add the smoother\n",
    "    word['wordGIVspam'] = \\\n",
    "    float(word['spam_count'] + SMOOTHER) / \\\n",
    "    float(spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of the word \n",
    "    # given not spam and add the smoother\n",
    "    word['wordGIVnspam'] = \\\n",
    "    float(word['not_spam_count'] + SMOOTHER) / \\\n",
    "    float(all_words-spam_ewords + VOCAB)\n",
    "\n",
    "# now let's print our model out to the file\n",
    "# this will be useful because it will make it\n",
    "# easier to load it in future functions\n",
    "\n",
    "# set the header\n",
    "print \"Word \\tCount \\tP(word|Spam) \\tP(word|Not Spam)\"\n",
    "\n",
    "# loop through each word\n",
    "for word in words_probs.keys():\n",
    "\n",
    "    # set the word name\n",
    "    word_name = word\n",
    "\n",
    "    # set the word that we'll be \n",
    "    # printing the probabilities for\n",
    "    word = words_probs[word]\n",
    "\n",
    "    # set each line as tab delimited\n",
    "    info = str(word_name) + \"\\t\" +\\\n",
    "    str(word['total']) + \"\\t\" +\\\n",
    "    str(word['wordGIVspam']) + \"\\t\" +\\\n",
    "    str(word['wordGIVnspam'])\n",
    "\n",
    "    # print each line\n",
    "    print info   \n",
    "\n",
    "# print a separating line to split each section\n",
    "print \"*~*~*~*~*\"\n",
    "\n",
    "##\n",
    "##\n",
    "## use the classifier to classify each email\n",
    "##\n",
    "##      \n",
    "\n",
    "# initalizate variables to store the counts of zero\n",
    "# posterior probabilities for each class\n",
    "spam_zeros = 0\n",
    "nspam_zeros = 0\n",
    "\n",
    "# create a dictionary that will store the posterior\n",
    "# probabilities for each email\n",
    "email_probs = {}\n",
    "\n",
    "# now let's loop through each email in \n",
    "# the dictionary \n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no words, then set the \n",
    "    # prediction based on the posterior\n",
    "    # probabilities\n",
    "    if len(emails[email][\"words\"]) == 0:\n",
    "        spam_prob = log(prob_spam)\n",
    "        nspam_prob = log(prob_nspam)\n",
    "    \n",
    "    # else if the email has a word, then set \n",
    "    # the prediction based on the\n",
    "    # conditional probability\n",
    "    else:\n",
    "        \n",
    "        # initalize a set of spam and not spam \n",
    "        # probabilities that start with the \n",
    "        # priors\n",
    "        spam_prob = log(prob_spam)\n",
    "        nspam_prob = log(prob_nspam)\n",
    "        \n",
    "        # loop through each of the words in \n",
    "        # the list of words and \n",
    "        # update the spam and not spam probabilities\n",
    "        for word in emails[email]['words']:\n",
    "            \n",
    "            # let's grab the probabilities for spam\n",
    "            # and not spam\n",
    "            word_spam_prob = words_probs[word]['wordGIVspam']\n",
    "            word_nspam_prob = words_probs[word]['wordGIVnspam']\n",
    "            \n",
    "            # first check and make sure that the\n",
    "            # probability is greater than zero\n",
    "            # if it's not, we'll want to go with\n",
    "            # the other class since we're not doing\n",
    "            # any smoothing\n",
    "            \n",
    "            # we use negative 1 and zero because that \n",
    "            # will help the data show up clearly in our\n",
    "            # future histogram\n",
    "            if word_spam_prob == 0:\n",
    "                spam_zeros = spam_zeros + 1\n",
    "                spam_prob = -1\n",
    "                nspam_prob = 0\n",
    "                break\n",
    "            if word_nspam_prob == 0:\n",
    "                nspam_zeros = nspam_zeros + 1\n",
    "                nspam_prob = -1\n",
    "                spam_prob = 0\n",
    "                break\n",
    "            \n",
    "            # calculate the probability for each\n",
    "            # document by adding the log \n",
    "            # probabilities for each word\n",
    "            spam_prob = spam_prob + \\\n",
    "            log(word_spam_prob)\n",
    "            nspam_prob = nspam_prob + \\\n",
    "            log(word_nspam_prob)\n",
    "        \n",
    "        # add the email to the dictionary with its\n",
    "        # probabilities\n",
    "        email_probs[email] = [spam_prob,nspam_prob]\n",
    "        \n",
    "        # choose the prediction based on the \n",
    "        # probabilities\n",
    "        if(spam_prob > nspam_prob):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, the \n",
    "    # actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + \\\n",
    "    str(_prediction)\n",
    "    print info\n",
    "\n",
    "# print a separating line to split each section\n",
    "print \"*~*~*~*~*\"\n",
    "    \n",
    "##\n",
    "##\n",
    "## write a file that has the counts for zero \n",
    "## probabilities\n",
    "##\n",
    "##\n",
    "\n",
    "# create the strings that contain the counts\n",
    "# of zero probabilities\n",
    "info_spam = \"The spam class had to deal with \" +\\\n",
    "str(spam_zeros) + \" instances of zero probabilities.\"\n",
    "info_nspam = \"The not spam class had to deal \\\n",
    "with \" + str(nspam_zeros) + \" instances of zero \\\n",
    "probabilities.\"\n",
    "    \n",
    "# write each of these two lines to the file\n",
    "print info_spam + \"\\n\" + info_nspam\n",
    "\n",
    "# print a separating line to split each section\n",
    "print \"*~*~*~*~*\"\n",
    "\n",
    "##\n",
    "##\n",
    "## write a file that has the posterior\n",
    "## probabilities for each word\n",
    "##\n",
    "##\n",
    "\n",
    "# create the string for the first line\n",
    "first_line = \"ID\\tP(spam|doc)\\tP(notspam|doc)\\n\"\n",
    "print first_line\n",
    "\n",
    "# loop through each email\n",
    "for email in email_probs.keys():\n",
    "\n",
    "    # create the string of probabilities\n",
    "    info = email + \"\\t\" + \\\n",
    "    str(email_probs[email][0]) + \"\\t\" + \\\n",
    "    str(email_probs[email][1])\n",
    "\n",
    "    # write the probability to the file\n",
    "    print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the reducer on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \tCount \tP(word|Spam) \tP(word|Not Spam)\r\n",
      "yellow\t1\t5.35045478866e-05\t0.0\r\n",
      "four\t8\t0.000267522739433\t0.000210955628999\r\n",
      "prefix\t1\t5.35045478866e-05\t0.0\r\n",
      "railing\t1\t5.35045478866e-05\t0.0\r\n",
      "looking\t4\t0.000214018191546\t0.0\r\n",
      "granting\t1\t5.35045478866e-05\t0.0\r\n",
      "electricity\t1\t0.0\t7.03185429998e-05\r\n",
      "originality\t1\t5.35045478866e-05\t0.0\r\n",
      "homemakers\t1\t5.35045478866e-05\t0.0\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x reducer.py\n",
    "!cat testingM.txt | sort -k1,1 | ~/w261/reducer.py > testingR.txt\n",
    "!head testingR.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Set the right permissions and execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 22:05:26 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/enronemail_1h.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/enronemail_1h.txt1464325526357\n",
      "16/05/26 22:05:33 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261-output-2-3' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261-output-2-31464325533839\n",
      "16/05/26 22:05:35 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2771636755664094365.jar tmpDir=null\n",
      "16/05/26 22:05:37 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 22:05:37 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 22:05:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/26 22:05:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/26 22:05:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464226260520_0035\n",
      "16/05/26 22:05:40 INFO impl.YarnClientImpl: Submitted application application_1464226260520_0035\n",
      "16/05/26 22:05:40 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464226260520_0035/\n",
      "16/05/26 22:05:40 INFO mapreduce.Job: Running job: job_1464226260520_0035\n",
      "16/05/26 22:05:49 INFO mapreduce.Job: Job job_1464226260520_0035 running in uber mode : false\n",
      "16/05/26 22:05:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/26 22:06:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/26 22:06:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/26 22:06:22 INFO mapreduce.Job: Job job_1464226260520_0035 completed successfully\n",
      "16/05/26 22:06:23 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=228066\n",
      "\t\tFILE: Number of bytes written=817248\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=233302\n",
      "\t\tHDFS: Number of bytes written=192675\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5013888\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1043968\n",
      "\t\tTotal time spent by all map tasks (ms)=39171\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8156\n",
      "\t\tTotal vcore-seconds taken by all map tasks=39171\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8156\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5013888\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1043968\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32911\n",
      "\t\tMap output bytes=1032050\n",
      "\t\tMap output materialized bytes=227563\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=227563\n",
      "\t\tReduce input records=32911\n",
      "\t\tReduce output records=5698\n",
      "\t\tSpilled Records=65822\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1610\n",
      "\t\tCPU time spent (ms)=4620\n",
      "\t\tPhysical memory (bytes) snapshot=397651968\n",
      "\t\tVirtual memory (bytes) snapshot=2201845760\n",
      "\t\tTotal committed heap usage (bytes)=143130624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=233062\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192675\n",
      "16/05/26 22:06:23 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-2-3\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/enronemail_1h.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-2-3\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_2_3\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-2-3\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-2-3/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle our output file\n",
    "Our output file is complete but contains disparate data. We need to split it into appropriate files for further analysis. We split it into:\n",
    "- Wordpreds: a file that shows the probability of spam and not spam for each word\n",
    "- Predictions: a file that records our spam predicition for each document\n",
    "- Zeroprobs: a file that tells us how many instances of zero probabilities we saw\n",
    "- Docpreds: a file that shows the posterior for spam and not spam for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initalize the arrays to hold the information\n",
    "# for each document\n",
    "wordpreds = []\n",
    "predictions = []\n",
    "zeroprobs = []\n",
    "docpreds = []\n",
    "\n",
    "# open the output file\n",
    "with open (\"Outputs/Out_2_3\",\"r\") as myfile:\n",
    "    \n",
    "    # initalize a counter to keep track of the \n",
    "    # file divisions\n",
    "    count = 0\n",
    "    \n",
    "    # loop through every line in the file\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # strip the leading and trailing white space\n",
    "        line = line.strip()\n",
    "        \n",
    "        # we use the counter to keep track of\n",
    "        # file we write by incrementing it with\n",
    "        # every file divider\n",
    "        if line == \"*~*~*~*~*\":\n",
    "            count = count + 1\n",
    "        else:\n",
    "            \n",
    "            # add to wordpreds array\n",
    "            if count == 0:\n",
    "                wordpreds.append(line)\n",
    "            \n",
    "            # add to predicitons array\n",
    "            if count == 1:\n",
    "                predictions.append(line)\n",
    "                \n",
    "            # add to zeroprobs array\n",
    "            if count == 2:\n",
    "                zeroprobs.append(line)\n",
    "            \n",
    "            # add to docpreds array\n",
    "            if count == 3:\n",
    "                docpreds.append(line)\n",
    "\n",
    "# write to the files for each array\n",
    "with open(\"Outputs/wordspreds\",'w') as myfile:\n",
    "    for line in wordpreds:\n",
    "        myfile.write(line.strip()+\"\\n\")\n",
    "with open(\"Outputs/predictions\",'w') as myfile:\n",
    "    for line in predictions:\n",
    "        myfile.write(line.strip()+\"\\n\")\n",
    "with open(\"Outputs/zeroprobs\",'w') as myfile:\n",
    "    for line in zeroprobs:\n",
    "        myfile.write(line.strip()+\"\\n\")\n",
    "with open(\"Outputs/docpreds\",'w') as myfile:\n",
    "    for line in docpreds:\n",
    "        myfile.write(line.strip()+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report on zero probabilities\n",
    "How many times did a class get selected because a word in an email never occured in the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spam class had to deal with 56 instances of zero probabilities.\n",
      "\n",
      "The not spam class had to deal with 44 instances of zero probabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open the count file\n",
    "with open(\"Outputs/zeroprobs\",\"r\") as myfile:\n",
    "    \n",
    "    # print out the lines\n",
    "    for line in myfile.readlines():\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report the misclassification rate\n",
    "The misclassification rate is considered the number of wrongly classified emails over the total number of emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainingerror(class_file):\n",
    "    \"\"\"a function that takes a tab delimited\n",
    "    classification file with 3 entries per row \n",
    "    that correspond to: record id, true class,\n",
    "    and predicted class\"\"\"\n",
    "    \n",
    "    # initalize some counters so that we can \n",
    "    # keep track of how many are wrong and\n",
    "    # and how many total records there are\n",
    "    records_wrong = 0\n",
    "    records_total = 0\n",
    "    \n",
    "    # open the file\n",
    "    with open (class_file, \"r\") as myfile:\n",
    "        \n",
    "        # read every line in the file\n",
    "        for line in myfile.readlines():\n",
    "            \n",
    "            # separate each line by the tabs\n",
    "            line = line.split(\"\\t\")\n",
    "            \n",
    "            # get the truth and predicted values\n",
    "            _truth = int(line[1].strip())\n",
    "            _predicted = int(line[2].strip())\n",
    "        \n",
    "            # add to the wrong records if \n",
    "            # the prediction is wrong\n",
    "            if _predicted != _truth:\n",
    "                records_wrong = records_wrong + 1\n",
    "                \n",
    "            # add to the total records\n",
    "            records_total = records_total + 1\n",
    "    \n",
    "    # calculate the error rate as wrong over total\n",
    "    error = float(records_wrong) / float(records_total)\n",
    "    \n",
    "    # return this error rate\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error for our model is 0.0\n"
     ]
    }
   ],
   "source": [
    "nosmooth_error = trainingerror(\"Outputs/predictions\")\n",
    "print \"The training error for our model is\", nosmooth_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of posterior probabilities\n",
    "We create a histogram of the document posterior probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFhJJREFUeJzt3X20ZXV93/H3ZxhQh4dxbGRGQUBFAbFRSYJPNU6CJIBV\ncKUh+AhaY1aXT63VOmgTMFlxiW2XmtikKzWOYytRQlXA6GKkw/hQFUVFiZARNQLOwEW9PMiYIMK3\nf+x9mcP1zr3nPp57f7xfa+015+zz23t/z95nPud3fvvsc1NVSJLasGrUBUiSFo6hLkkNMdQlqSGG\nuiQ1xFCXpIYY6pLUEENdK1qSf5Xk2lHXMVmSJyT5yqjrmCzJ4Un+cYm29Zok71iKbWkPQ32Z6UPq\n/yW5LcmPknwuya+Muq7FkOTMJJ+bzzqq6vNVdcxC1TSsJJuT3JXkjv44bU1y1ECTPwbeOdB+OR3X\n+y5OSXJOkp8lub2f/iHJnyfZsADb+Z/Ai5P80gKsS0My1JeRJAcClwDvAdYBhwBvA+4aZV2LKAwE\nzKwXTvaZ18aT+b7+z6uqg4BDgVuAD/TrfQSwEbiov7/cj+uHq2ot8DDgBcAG4KtJ1s9npVV1F/BJ\n4GXzL1HDMtSXl8cDVVUXVOeuqrqsqv4e7uvZfr7vSd2W5JokvzmxcJKz+nl3JPlOklcNPPbsJDcm\neVOSsSQ7k5ya5OQkO/re49l7K6zvmf5l3yO9I8nlSQ4bePwZSb6c5NYkVyR5+qS6vtsv990kL0xy\nNPCXwNOT/CTJeN92vyT/Ncn1SW5K8hdJHjTpOfynJDcB75+YN7Cto/vabk1ydZLnTXoOf5Hk75L8\nBNjYP/9v9bXdmOQNsz1oVfXPwPnAsf2s5wBfq6qfLffjOul53FNV1wK/B/wQ+I8D2/n9JNf16/t4\n/8Y18dix/evix/0x2zSw2s8Az53F7tR8VZXTMpmAA+n+M30AOAl46KTHzwTuBl4H7AOcDtw20Q44\nGTiiv/0sYDfw5P7+s/tl39ov+0q63uX/BtYATwB+Chy+l9o2A7cDzwT2Bd4NfK5/bB0wDryIrqNw\nRn9/Xb/u24Ej+7brgWMGns9nJ23nXcDHgbXA/nS93T+d9Bze3tfwoH7eDf3jq4HrgDf3t38DuAN4\n3MBzuBV4Wn//QcAu4Bn9/bUT+2uIY7UZ+OP+9gHAh4Dt/f13An++HI8rcDjwvYFtnwN8cIrn9zbg\ni/3t3+zrf1K/3/8M+MzAc98F/Htgv/6Y/drAep4C/GjU/7ceSNPIC3CadEDgKOD9wA3Az/pQe3j/\n2JnADya1vwJ48V7W9THgtf3tZ/dhkP7+AcC9wK8OtL8SeP5e1rUZOH/g/v59mBwCvAT40qT2X6D7\n2L2GLuBfADx4UpupQv1O4NED958+EUL9c/hnYN+BxwdD/VnArknrOx/4o4Hn8IFJj38f+H3gwFke\np83AP/XPbRfdG9Gj+8f+Cnj7cjyuDB/qfwDs6G+/D3jHpGN/F3AY3Rv4V6fZT0cCd4/6/9UDaXL4\nZZmpqh1V9YqqOgx4IvBIul7xhJ2TFrm+b0P/kfuL/cfgW+l6eIMnqX5c/f80ukCCrlfHwLwDpinv\nvmGOqtpN1+t9ZD9dP0Vdh1TVT+k+zv874KYkl0w6oXifJA+nexP4apLxfkjmU8C/GGj2w6q6ey/1\nPWKwxsE6pnoOvd+hGx64vh+2edpe1j2V/1JVD6uqR1bVaVU18a2SW+l65/dZ5sd1KofQvWHBpOPb\nH/vxvs2jgO9Os54D6T6paYkY6stYVX2b7iP7EwdmHzKp2WHAriT7ARfSffR/eFWtowvELGBJj5q4\nkeQAuuGVXf10xBR17QSoqk9X1W/RnYDbQdeThV88SfojuqGCY/uwfFhVPbS6k3jsZZlBuwZrnFzH\nVMtX1Ver6jTg4XS95wumWf+wvkk3jj6lZXhc7ydJgOcBn+1n7aLr4U88vj/dG+1OujfJx06zumOA\nbyxOpZqKob6MJDkqyRuSHNLffxTwQuCLA80OTvLaJKuT/C5wNPB3dOOZ+9GNX96b5GTgtxa4xFP6\nE6L7AX9CN+Syk+4bDo9LckaSfZL8Ht1/5k8kOTjJ85OsoRuuuZNueABgDDg0yb7QnUmk+xrcu/te\nO0kOSTLs87gC+Gl/InV1ko3Avwb+ZqrGSfZN8qIkB1XVPcBPgHsGHr83ya8Pv3vu82nguH4/rYTj\nmr6ufZIcA3yY7tzHu/rH/wZ4eZJf7k9av53u2N8AfALYkOR1/UnuA5IcP7DuZ9O9CWmJGOrLy0+A\npwJX9N/O+AJdr++NA22uAB5H16v9E+B3quq2qrqT7kTb3/bDFmfQf6VuGpN7vTN9vfB84Fzgx3Qn\nwF4CUFXjdOH5xr6uNwLP7eevAt5A16v7EfDrdEMxANuAbwE3J5kYLtgEfAf4UpLbgK1M0+u9X/Hd\nsMzzgFP6bb0XeGlVXTfN83sp8I/9tl5Fd7J3InjvAK7e2+amqeOW/rmd1s9a7sf19CR30J2c/Tjd\nSdFfqaqb++fzf4E/BD5Kdxwf3ddBX9+JwPOBm4Fv032dkyQPpjsWW2bYvhbQxMmV6Rsla+lOljyR\nrpf1CrqD9xG6j2XfB06vKsfOFlGSM4F/W1Vz6T3Od9ubgRur6o+WetujkOTFwBOq6q1zXP4YupOy\nTx2i7ZId1ySHA5dX1WOWYFuvAQ6tqk0zNtaCWT1ku/cAn6yq302ymu7s91uAy6rqnUneDJxN18uS\nVryq+tA8l7+Wrnf+gFVV7x11DQ9EMw6/JDkIeFZVbQaoqp/3PfJT2fOxagt7PmqqTf7dwzbcxv2/\ndaPGzDj8kuRJdN9WuIbu4oMr6S402NmfiZ9oN15VD1vEWiVJMxjmROlq4Djgv1fVcXQXOmxi9idj\nJEmLbJgx9R/QnSC7sr//f+hCfSzJ+qoaS/eLbrdMtXASw16S5qCqZn09wow99aoaA25MMvG1shPo\nvoZ2MXBWP+9Mpvma1agvm10u0znnnDPyGpbL5L5wX7gvpp/mathvv7wO+FB/kcj3gJfT/XjQBUle\nQXcJ8elzrkKStCCGCvWq+gbwa1M89JyFLUeSNB9eUbqENm7cOOoSlg33xR7uiz3cF/M31BWl89pA\nUou9DUlqTRJqMU6USpJWDkNdkhpiqEtSQwx1SWqIoS5JDTHUJakhhrokNWTYnwmYl7vv3tsff18a\nq1atYp999hlpDZK0FJbk4qNVq5bkvWNKVcVjH3ss113nHzSXtHLM9eKjJUnbe+8dZU99nF27jhzh\n9iVp6TimLkkNMdQlqSGGuiQ1xFCXpIYY6pLUEENdkhpiqEtSQwx1SWqIoS5JDTHUJakhhrokNcRQ\nl6SGGOqS1BBDXZIaYqhLUkMMdUlqyFB/JCPJ94HbgXuBu6vq+CTrgI8AhwPfB06vqtsXqU5J0hCG\n7anfC2ysqqdU1fH9vE3AZVV1FLANOHsxCpQkDW/YUM8UbU8FtvS3twCnLVRRkqS5GTbUC/h0kq8k\neWU/b31VjQFU1c3AwYtRoCRpeMP+4elnVtVNSR4ObE2ygy7oB02+L0laYkOFelXd1P/7wyQfB44H\nxpKsr6qxJBuAW/a+hnMHbm/sJ0nShO3bt7N9+/Z5rydV03ewk6wBVlXVnUn2B7YCbwNOAMar6rwk\nbwbWVdWmKZav0Xbix1mz5kh27x4fYQ2SNDtJqKrMdrlheurrgY914cxq4ENVtTXJlcAFSV4BXA+c\nPtuNS5IW1ow99XlvwJ66JM3aXHvqXlEqSQ0x1CWpIYa6JDXEUJekhhjqktQQQ12SGmKoS1JDDHVJ\naoihLkkNMdQlqSGGuiQ1xFCXpIYY6pLUEENdkhpiqEtSQwx1SWqIoS5JDTHUJakhw/yNUkl6wNiw\n4QjGxq4fdRlzZqhL0oAu0Ef5d5UnzPrPkwIOv0hSUwx1SWqIoS5JDTHUJakhhrokNcRQl6SGGOqS\n1BBDXZIaYqhLUkOGDvUkq5J8LcnF/f11SbYm2ZHk0iRrF69MSdIwZtNTfz1wzcD9TcBlVXUUsA04\neyELkyTN3lChnuRQ4BTgfQOzTwW29Le3AKctbGmSpNkatqf+LuBN3P9XbtZX1RhAVd0MHLzAtUmS\nZmnGX2lM8lxgrKquSrJxmqbT/KzZuQO3N/aTJGmP7f00P6ma/icmk7wdeAnwc+AhwIHAx4BfBTZW\n1ViSDcDlVXXMFMvXaH/Gcpw1a45k9+7xEdYgaaVIwnL56d2qmvXv7844/FJVb6mqw6rqMcAZwLaq\neilwCXBW3+xM4KLZblyStLDm8z31dwAnJtkBnNDflySN0IzDL/PegMMvklaQ5odfJEkrh6EuSQ0x\n1CWpIYa6JDXEUJekhhjqktQQQ12SGmKoS1JDDHVJaoihLkkNMdQlqSGGuiQ1xFCXpIYY6pLUEENd\nkhpiqEtSQwx1SWqIoS5JDTHUJakhhrokNcRQl6SGGOqS1BBDXZIaYqhLUkMMdUlqiKEuSQ0x1CWp\nIYa6JDVkxlBP8qAkVyT5epKrk5zTz1+XZGuSHUkuTbJ28cuVJE1nxlCvqruA36iqpwBPBk5Ocjyw\nCbisqo4CtgFnL2qlkqQZDTX8UlU/7W8+CFgNFHAqsKWfvwU4bcGrkyTNylChnmRVkq8DNwOfrqqv\nAOuragygqm4GDl68MiVJw1g9TKOquhd4SpKDgI8lOZaut36/Zntfw7kDtzf2kyRpj+39ND+pmiaL\np1og+UPgp8ArgY1VNZZkA3B5VR0zRfuaNu8X3Thr1hzJ7t3jI6xB0kqRhNFm1oRQVZntUsN8++WX\nJr7ZkuQhwInAtcDFwFl9szOBi2a7cUnSwhpm+OURwJYkq+jeBD5SVZ9M8iXggiSvAK4HTl/EOiVJ\nQ5j18MusN+Dwi6QVpPnhF0nSymGoS1JDDHVJaoihLkkNMdQlqSGGuiQ1xFCXpIYY6pLUEENdkhpi\nqEtSQwx1SWqIoS5JDTHUJakhhrokNcRQl6SGGOqS1BBDXZIaYqhLUkMMdUlqiKEuSQ0x1CWpIYa6\nJDXEUJekhhjqktQQQ12SGmKoS1JDDHVJaoihLkkNmTHUkxyaZFuSbyW5Osnr+vnrkmxNsiPJpUnW\nLn65kqTpDNNT/znwhqo6Fng68OokRwObgMuq6ihgG3D24pUpSRrGjKFeVTdX1VX97TuBa4FDgVOB\nLX2zLcBpi1WkJGk4sxpTT3IE8GTgS8D6qhqDLviBgxe6OEnS7KwetmGSA4ALgddX1Z1JalKTyfcH\nnDtwe2M/SZL22N5P85OqabJ4olGyGvgE8Kmqek8/71pgY1WNJdkAXF5Vx0yxbE2b94tunDVrjmT3\n7vER1iBppUjCaDNrQqiqzHapYYdf3g9cMxHovYuBs/rbZwIXzXbjkqSFNWNPPckzgc8CV9O9fRXw\nFuDLwAXAo4DrgdOr6rYplrenLmnFWOk99aGGX+bDUJe0kqz0UPeKUklqiKEuSQ0x1CWpIYa6JDXE\nUJekhhjqktQQQ12SGmKoS1JDDHVJaoihLkkNMdQlqSGGuiQ1xFCXpIYY6pLUEENdkhpiqEtSQwx1\nSWqIoS5JDTHUJakhhrokNcRQl6SGGOqS1BBDXZIaYqhLUkMMdUlqiKEuSQ0x1CWpIYa6JDVkxlBP\n8tdJxpJ8c2DeuiRbk+xIcmmStYtbpiRpGMP01DcDvz1p3ibgsqo6CtgGnL3QhUmSZm/GUK+qzwO3\nTpp9KrClv70FOG2B65IkzcFcx9QPrqoxgKq6GTh44UqSJM3VQp0orQVajyRpHlbPcbmxJOuraizJ\nBuCW6ZufO3B7Yz9JkvbY3k/zk6qZO9lJjgAuqap/2d8/DxivqvOSvBlYV1Wb9rJsjbYjP86aNUey\ne/f4CGuQtFIkYXkMPoSqymyXGuYrjecDXwAen+SGJC8H3gGcmGQHcEJ/X5I0YkP11Oe1AXvqklaQ\n5nvqkqSVw1CXpIYY6pLUEENdkhpiqEtSQwx1SWqIoS5JDTHUJakhhrokNcRQl6SGGOqS1BBDXZIa\nYqhLUkMMdUlqiKEuSQ0x1CWpIYa6JDXEUJekhhjqktQQQ12SGmKoS1JDDHVJaoihLkkNMdQlqSGG\nuiQ1xFCXpIYY6pLUEENdkhoyr1BPclKSf0jy7SRvXqiiJElzM+dQT7IKeC/w28CxwAuTHL1QhbVo\n+/btoy5h2XBf7OG+2MN9MX/z6akfD1xXVddX1d3Ah4FTF6asNvmC3cN9sYf7Yg/3xfzNJ9QPAW4c\nuP+Dfp4kaURWL8VGDjroeUuxmSlV/Yxk35FtX5KWUqpqbgsmTwPOraqT+vubgKqq8ya1m9sGJOkB\nrqoy22XmE+r7ADuAE4CbgC8DL6yqa+e0QknSvM15+KWq7knyGmAr3dj8XxvokjRac+6pS5KWnwW/\nojTJv0ny90nuSXLcNO2av3ApybokW5PsSHJpkrV7afcf+n32zSQfSrLfUte62GaxL9Ym+dsk1yb5\nVpKnLnWti23YfdG3XZXka0kuXsoal8ow+yLJoUm29a+Hq5O8bhS1LoZhcjDJnyW5LslVSZ480zoX\n42cCrgZeAHxmbw0eQBcubQIuq6qjgG3A2ZMbJHkk8FrguKr6ZbohsTOWtMqlMeO+6L0H+GRVHQM8\nCWhxSG/YfQHweuCaJalqNIbZFz8H3lBVxwJPB17dQl4Mk4NJTgYeW1WPA/4A+B8zrXfBQ72qdlTV\ndcB0Z20fKBcunQps6W9vAU7bS7t9gP2TrAbWALuWoLalNuO+SHIQ8Kyq2gxQVT+vqjuWrsQlM9Tr\nIsmhwCnA+5aorlGYcV9U1c1VdVV/+066N/oWrokZJgdPBT4IUFVXAGuTrJ9upaP6Qa8HyoVLB1fV\nGHQvTODgyQ2qahfw34AbgJ3AbVV12ZJWuTRm3BfAo4EfJdncDzn8VZKHLGmVS2OYfQHwLuBNQMsn\nvobdFwAkOQJ4MnDFole2+IbJwcltdk7R5n7m9O2XJJ8GBt8tQvfCe2tVXTKXda5U0+yL/zxF81/4\nz5nkoXTvxocDtwMXJnlRVZ2/COUuqvnuC7rX43HAq6vqyiTvpvt4fs5C17rYFuB18VxgrKquSrKR\n6T/5LmsL8LqYWM8BwIXA6/seu6Ywp1CvqhPnud2dwGED9w/t56040+2LJGNJ1lfVWJINwC1TNHsO\n8L2qGu+X+SjwDGDFhfoC7IsfADdW1ZX9/QuBFXkSfQH2xTOB5yc5BXgIcGCSD1bVyxap5EWzAPuC\nfmjyQuB/VdVFi1TqUhsmB3cCj5qhzf0s9vDL3noXXwGOTHJ4/02PM4AWz+5fDJzV3z4TmOrFeAPw\ntCQPThK6i7laPDk4477oP4bfmOTx/awTaPMk4TD74i1VdVhVPYbu/8e2lRjoQxjm/wjA+4Frquo9\nS1HUEhkmBy8GXgb3XcV/28Rw1V5V1YJOdCc6bgT+ie5K00/18x8BfGKg3Ul0V6ReB2xa6DqWwwQ8\nDLisf55bgYfuZV+cQxfk36Q7WbTvqGsf4b54Uv9ivwr4KLB21LWPal8MtH82cPGo6x7VvqD71HJP\n/5r4OvA14KRR175Az/8XcpDuWy6vGmjzXuA7wDfoviU37Tq9+EiSGuKfs5OkhhjqktQQQ12SGmKo\nS1JDDHVJaoihLkkNMdQlqSGGuiQ15P8D4nJw0TEm2UsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a5c11eed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a5c0e9190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# import the matplotlib library we need for \n",
    "# the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create array to store the probabilities for\n",
    "# both classes\n",
    "spam_probs = []\n",
    "nspam_probs = []\n",
    "\n",
    "# open the file with the posterior probabilities\n",
    "with open(\"Outputs/docpreds\",\"r\") as myfile:\n",
    "    \n",
    "    # read every line, skipping the first title line\n",
    "    for line in myfile.readlines()[2:]:\n",
    "        \n",
    "        # split the line based on the tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # append the probabilities\n",
    "        spam_probs.append(float(line[1]))\n",
    "        nspam_probs.append(float(line[2]))\n",
    "\n",
    "# plot the histograms for each class\n",
    "plt.hist(spam_probs)\n",
    "plt.title(\"Spam posteriors, P(Spam|Doc)\")\n",
    "plt.show()\n",
    "plt.hist(nspam_probs)\n",
    "plt.title(\"Not spam posteriors, P(Not spam|Doc)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.4 Laplace smoothing\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "We reuse the same mapper function from 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# initalize the outputs that we'll be using\n",
    "email_id = None\n",
    "word_act = None\n",
    "spam_ind = None\n",
    "    \n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # separate out each line based on the tabs\n",
    "    line = line.split(\"\\t\")\n",
    "    \n",
    "    # set the email id and spam indicator fields\n",
    "    email_id = line[0]\n",
    "    spam_ind = line[1]\n",
    "\n",
    "    # grab the text from the body and \n",
    "    # subject and concatenate it, if both exist\n",
    "    if len(line)==4:\n",
    "        text = line[2] + \" \" + line[3]\n",
    "    else:\n",
    "        text = line[2]\n",
    "\n",
    "    # convert the text into a list of \n",
    "    # words without punctuation\n",
    "    # also lowercase all the words\n",
    "    words = wordify.findall(text.lower())\n",
    "        \n",
    "    # loop through each word\n",
    "    for word in words:\n",
    "\n",
    "        # collect the line that we want to print out\n",
    "        info = email_id + \"\\t\" + word + \"\\t\" + str(spam_ind)\n",
    "\n",
    "        # let's print each email id, word, \n",
    "        # spam indicator\n",
    "        # we'll separate each value with a tab character\n",
    "        print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the mapper function on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\tchristmas\t0\r\n",
      "0001.1999-12-10.farmer\ttree\t0\r\n",
      "0001.1999-12-10.farmer\tfarm\t0\r\n",
      "0001.1999-12-10.farmer\tpictures\t0\r\n",
      "0001.1999-12-10.farmer\tna\t0\r\n",
      "0001.1999-12-10.kaminski\tre\t0\r\n",
      "0001.1999-12-10.kaminski\trankings\t0\r\n",
      "0001.1999-12-10.kaminski\tthank\t0\r\n",
      "0001.1999-12-10.kaminski\tyou\t0\r\n",
      "0001.2000-01-17.beck\tleadership\t0\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x mapper.py\n",
    "!cat data/enronemail_1h.txt | ~/w261/mapper.py > testingM.txt\n",
    "!head testingM.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "We re-write the reducer function to add a Laplace smoother. We take out the extra stuff for calculating the number of zero probabilities (no longer needed since we've added the smoother). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "# import the system libraries to read \n",
    "# from the input, from the \n",
    "# math library import the log function\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "# create a dictionary to store each email id, \n",
    "# spam indciator, list of words of \n",
    "# interest, and spam prediction\n",
    "emails = {}\n",
    "    \n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "spam_ewords = 0 # number of words in spam emails\n",
    "\n",
    "# let's initalize a dictionary that will \n",
    "# hold the probabilities for \n",
    "# each and every word in the corpus\n",
    "words_probs = {}\n",
    "\n",
    "##\n",
    "##\n",
    "## gather summary statistics and words for each email\n",
    "##\n",
    "##\n",
    "        \n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by tabs\n",
    "    line = line.split(\"\\t\")\n",
    "\n",
    "    # pull out my values for each part of the line\n",
    "    email_id = line[0]\n",
    "    word_act = line[1]\n",
    "    spam_ind = int(line[2])\n",
    "\n",
    "    # let's update the number of words and \n",
    "    # the spam words (if it's spam)\n",
    "    all_words = all_words + 1\n",
    "    if spam_ind == 1:\n",
    "        spam_ewords = spam_ewords + 1\n",
    "\n",
    "    # if we don't already have the word in \n",
    "    # our dictionary of words, let's\n",
    "    # add it and initalize counts of zero\n",
    "    if word_act not in words_probs:\n",
    "        words_probs[word_act] = \\\n",
    "        {\"spam_count\":0,\"not_spam_count\":0}\n",
    "\n",
    "    # let's grab the dictionary for the word\n",
    "    word_dict = words_probs[word_act]\n",
    "\n",
    "    # let's increment the word counts \n",
    "    # for the word\n",
    "    if spam_ind == 1:\n",
    "        word_dict['spam_count'] = \\\n",
    "        word_dict['spam_count'] + 1\n",
    "    else:\n",
    "        word_dict['not_spam_count'] = \\\n",
    "        word_dict['not_spam_count'] + 1\n",
    "\n",
    "    # check to see if this email is already \n",
    "    # in the dictionary, and if\n",
    "    # its not already there, initalize it\n",
    "    if email_id not in emails:\n",
    "\n",
    "        # create a sub-dictionary within \n",
    "        # the email dictionary for each email\n",
    "        emails[email_id] = \\\n",
    "        {\"spam\":spam_ind, \"words\":[]}\n",
    "\n",
    "        # if it's not already there, \n",
    "        # let's also increment the email counter\n",
    "        # and the spam counter if it's spam\n",
    "        all_emails = all_emails + 1\n",
    "        if spam_ind == 1:\n",
    "            spam_emails = spam_emails + 1\n",
    "\n",
    "    # let's add the word to our list \n",
    "    # of words for this email\n",
    "    emails[email_id][\"words\"].append(word_act)\n",
    "\n",
    "##\n",
    "##\n",
    "## build the naive bayes classifier\n",
    "##\n",
    "##\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# for each word, let's calculate the \n",
    "# conditional probability of spam given the word\n",
    "# and not spam given the word\n",
    "\n",
    "# let's define our LaPlace smoother \n",
    "SMOOTHER = 1.0\n",
    "VOCAB = float(len(words_probs))\n",
    "\n",
    "# look at each word\n",
    "for word in words_probs.keys():\n",
    "    \n",
    "    # set the find word that we'll be \n",
    "    # calculating the probabilities for\n",
    "    word = words_probs[word]\n",
    "    \n",
    "    # calculate the probability of the word\n",
    "    word['total'] = word['spam_count'] + \\\n",
    "    word['not_spam_count']\n",
    "    word['probs'] = float(word['total']) / \\\n",
    "    float(all_words)\n",
    "    \n",
    "    # calculate the probability of word \n",
    "    # given spam and add the smoother\n",
    "    word['wordGIVspam'] = \\\n",
    "    float(word['spam_count'] + SMOOTHER) / \\\n",
    "    float(spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of the word \n",
    "    # given not spam and add the smoother\n",
    "    word['wordGIVnspam'] = \\\n",
    "    float(word['not_spam_count'] + SMOOTHER) / \\\n",
    "    float(all_words-spam_ewords + VOCAB)\n",
    "\n",
    "# now let's print our model out to the file\n",
    "# this will be useful because it will make it\n",
    "# easier to load it in future functions\n",
    "\n",
    "# set the header\n",
    "print \"Word \\tCount \\tP(word|Spam) \\tP(word|Not Spam)\"\n",
    "\n",
    "# loop through each word\n",
    "for word in words_probs.keys():\n",
    "\n",
    "    # set the word name\n",
    "    word_name = word\n",
    "\n",
    "    # set the word that we'll be \n",
    "    # printing the probabilities for\n",
    "    word = words_probs[word]\n",
    "\n",
    "    # set each line as tab delimited\n",
    "    info = str(word_name) + \"\\t\" +\\\n",
    "    str(word['total']) + \"\\t\" +\\\n",
    "    str(word['wordGIVspam']) + \"\\t\" +\\\n",
    "    str(word['wordGIVnspam'])\n",
    "\n",
    "    # print each line\n",
    "    print info   \n",
    "\n",
    "# print a separating line to split each section\n",
    "print \"*~*~*~*~*\"\n",
    "\n",
    "##\n",
    "##\n",
    "## use the classifier to classify each email\n",
    "##\n",
    "##      \n",
    "\n",
    "# create a dictionary that will store the posterior\n",
    "# probabilities for each email\n",
    "email_probs = {}\n",
    "\n",
    "# now let's loop through each email in \n",
    "# the dictionary \n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no words, then set the \n",
    "    # prediction based on the posterior\n",
    "    # probabilities\n",
    "    if len(emails[email][\"words\"]) == 0:\n",
    "        spam_prob = log(prob_spam)\n",
    "        nspam_prob = log(prob_nspam)\n",
    "    \n",
    "    # else if the email has a word, then set \n",
    "    # the prediction based on the\n",
    "    # conditional probability\n",
    "    else:\n",
    "        \n",
    "        # initalize a set of spam and not spam \n",
    "        # probabilities that start with the \n",
    "        # priors\n",
    "        spam_prob = log(prob_spam)\n",
    "        nspam_prob = log(prob_nspam)\n",
    "        \n",
    "        # loop through each of the words in \n",
    "        # the list of words and \n",
    "        # update the spam and not spam probabilities\n",
    "        for word in emails[email]['words']:\n",
    "            \n",
    "            # let's grab the probabilities for spam\n",
    "            # and not spam\n",
    "            word_spam_prob = words_probs[word]['wordGIVspam']\n",
    "            word_nspam_prob = words_probs[word]['wordGIVnspam']\n",
    "            \n",
    "            # calculate the probability for each\n",
    "            # document by adding the log \n",
    "            # probabilities for each word\n",
    "            spam_prob = spam_prob + \\\n",
    "            log(word_spam_prob)\n",
    "            nspam_prob = nspam_prob + \\\n",
    "            log(word_nspam_prob)\n",
    "        \n",
    "        # add the email to the dictionary with its\n",
    "        # probabilities\n",
    "        email_probs[email] = [spam_prob,nspam_prob]\n",
    "        \n",
    "        # choose the prediction based on the \n",
    "        # probabilities\n",
    "        if(spam_prob > nspam_prob):\n",
    "            _prediction = 1\n",
    "        else:\n",
    "            _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, the \n",
    "    # actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + \\\n",
    "    str(_prediction)\n",
    "    print info\n",
    "\n",
    "# print a separating line to split each section\n",
    "print \"*~*~*~*~*\"\n",
    "    \n",
    "##\n",
    "##\n",
    "## write a file that has the posterior\n",
    "## probabilities for each word\n",
    "##\n",
    "##\n",
    "\n",
    "# create the string for the first line\n",
    "first_line = \"ID\\tP(spam|doc)\\tP(notspam|doc)\\n\"\n",
    "print first_line\n",
    "\n",
    "# loop through each email\n",
    "for email in email_probs.keys():\n",
    "\n",
    "    # create the string of probabilities\n",
    "    info = email + \"\\t\" + \\\n",
    "    str(email_probs[email][0]) + \"\\t\" + \\\n",
    "    str(email_probs[email][1])\n",
    "\n",
    "    # write the probability to the file\n",
    "    print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the reducer on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \tCount \tP(word|Spam) \tP(word|Not Spam)\r\n",
      "yellow\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "four\t8\t0.000248138957816\t0.000202932372787\r\n",
      "prefix\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "railing\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "looking\t4\t0.000206782464847\t5.07330931967e-05\r\n",
      "granting\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "electricity\t1\t4.13564929694e-05\t0.000101466186393\r\n",
      "originality\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "homemakers\t1\t8.27129859388e-05\t5.07330931967e-05\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x reducer.py\n",
    "!cat testingM.txt | sort -k1,1 | ~/w261/reducer.py > testingR.txt\n",
    "!head testingR.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Set the right permissions and execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 22:21:33 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/enronemail_1h.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/enronemail_1h.txt1464326493153\n",
      "rm: `/user/cloudera/w261-output-2-4': No such file or directory\n",
      "rm: cannot remove `/home/cloudera/w261/Outputs/Out_2_4': No such file or directory\n",
      "16/05/26 22:21:41 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob9015346952522049383.jar tmpDir=null\n",
      "16/05/26 22:21:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 22:21:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 22:21:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/26 22:21:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/26 22:21:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464226260520_0036\n",
      "16/05/26 22:21:46 INFO impl.YarnClientImpl: Submitted application application_1464226260520_0036\n",
      "16/05/26 22:21:46 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464226260520_0036/\n",
      "16/05/26 22:21:46 INFO mapreduce.Job: Running job: job_1464226260520_0036\n",
      "16/05/26 22:21:54 INFO mapreduce.Job: Job job_1464226260520_0036 running in uber mode : false\n",
      "16/05/26 22:21:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/26 22:22:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/26 22:22:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/26 22:22:31 INFO mapreduce.Job: Job job_1464226260520_0036 completed successfully\n",
      "16/05/26 22:22:31 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=228066\n",
      "\t\tFILE: Number of bytes written=817248\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=233302\n",
      "\t\tHDFS: Number of bytes written=258181\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5879680\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1051520\n",
      "\t\tTotal time spent by all map tasks (ms)=45935\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8215\n",
      "\t\tTotal vcore-seconds taken by all map tasks=45935\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8215\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5879680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1051520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32911\n",
      "\t\tMap output bytes=1032050\n",
      "\t\tMap output materialized bytes=227563\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=227563\n",
      "\t\tReduce input records=32911\n",
      "\t\tReduce output records=5695\n",
      "\t\tSpilled Records=65822\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=665\n",
      "\t\tCPU time spent (ms)=4580\n",
      "\t\tPhysical memory (bytes) snapshot=375197696\n",
      "\t\tVirtual memory (bytes) snapshot=2190225408\n",
      "\t\tTotal committed heap usage (bytes)=142606336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=233062\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=258181\n",
      "16/05/26 22:22:31 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-2-4\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/enronemail_1h.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-2-4\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_2_4\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-2-4\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-2-4/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_2_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare our output file\n",
    "We convert our single output from the reducer to a set of files:\n",
    "- Wordpreds_smooth: a file that shows the probability of spam and not spam for each word\n",
    "- Predictions_smooth: a file that records our spam predicition for each document\n",
    "- Docpreds_smooth: a file that shows the posterior for spam and not spam for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initalize the arrays to hold the information\n",
    "# for each document\n",
    "wordpreds = []\n",
    "predictions = []\n",
    "docpreds = []\n",
    "\n",
    "# open the output file\n",
    "with open (\"Outputs/Out_2_4\",\"r\") as myfile:\n",
    "    \n",
    "    # initalize a counter to keep track of the \n",
    "    # file divisions\n",
    "    count = 0\n",
    "    \n",
    "    # loop through every line in the file\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # strip the leading and trailing white space\n",
    "        line = line.strip()\n",
    "        \n",
    "        # we use the counter to keep track of\n",
    "        # file we write by incrementing it with\n",
    "        # every file divider\n",
    "        if line == \"*~*~*~*~*\":\n",
    "            count = count + 1\n",
    "        else:\n",
    "            \n",
    "            # add to wordpreds array\n",
    "            if count == 0:\n",
    "                wordpreds.append(line)\n",
    "            \n",
    "            # add to predicitons array\n",
    "            if count == 1:\n",
    "                predictions.append(line)\n",
    "                \n",
    "            # add to zeroprobs array\n",
    "            if count == 2:\n",
    "                docpreds.append(line)\n",
    "\n",
    "# write to the files for each array\n",
    "with open(\"Outputs/wordspreds_smooth\",'w') as myfile:\n",
    "    for line in wordpreds:\n",
    "        myfile.write(line.strip()+\"\\n\")\n",
    "with open(\"Outputs/predictions_smooth\",'w') as myfile:\n",
    "    for line in predictions:\n",
    "        myfile.write(line.strip()+\"\\n\")\n",
    "with open(\"Outputs/docpreds_smooth\",'w') as myfile:\n",
    "    for line in docpreds:\n",
    "        myfile.write(line.strip()+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the training error with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error for our model with smoothing is 0.0\n",
      "+-------------------------+----------------+\n",
      "|          Model          | Training Error |\n",
      "+-------------------------+----------------+\n",
      "| Model without smoothing |      0.0       |\n",
      "|   Model with smoothing  |      0.0       |\n",
      "+-------------------------+----------------+\n",
      "My misclassification rate is 0 for both models \n",
      "because the vocabularly between the spam and not spam \n",
      "emails has so little overlap.\n"
     ]
    }
   ],
   "source": [
    "# import Pretty table to help us compare errors\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# set the error for the model with smoothing\n",
    "smooth_error = trainingerror(\"Outputs/predictions_smooth\")\n",
    "print \"The training error for our model with smoothing \\\n",
    "is\", smooth_error\n",
    "\n",
    "# create a new table to compare the \n",
    "# training errors\n",
    "pretty = PrettyTable([\"Model\",\"Training Error\"])\n",
    "pretty.add_row([\"Model without smoothing\",nosmooth_error])\n",
    "pretty.add_row([\"Model with smoothing\",smooth_error])\n",
    "\n",
    "# print the table\n",
    "print pretty\n",
    "\n",
    "# print explanation\n",
    "print \"My misclassification rate is 0 for both models \\\n",
    "\\nbecause the vocabularly between the spam and not spam \\\n",
    "\\nemails has so little overlap.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of posterior probabilities\n",
    "We create a histogram of the document posterior probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a55b46610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns0w40M3MMuFANzPLxP8HMyuo++vS81EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a5c103690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# import the matplotlib library we need for \n",
    "# the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create array to store the probabilities for\n",
    "# both classes\n",
    "spam_probs = []\n",
    "nspam_probs = []\n",
    "\n",
    "# open the file with the posterior probabilities\n",
    "with open(\"Outputs/docpreds_smooth\",\"r\") as myfile:\n",
    "    \n",
    "    # read every line, skipping the first title line\n",
    "    for line in myfile.readlines()[2:]:\n",
    "        \n",
    "        # split the line based on the tabs\n",
    "        line = line.split(\"\\t\")\n",
    "        \n",
    "        # append the probabilities\n",
    "        spam_probs.append(float(line[1]))\n",
    "        nspam_probs.append(float(line[2]))\n",
    "\n",
    "# plot the histograms for each class\n",
    "plt.hist(spam_probs)\n",
    "plt.title(\"Spam posteriors, P(Spam|Doc)\")\n",
    "plt.show()\n",
    "plt.hist(nspam_probs)\n",
    "plt.title(\"Not spam posteriors, P(Not spam|Doc)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.5. Repeat HW2.4. \n",
    "This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapper function\n",
    "We repeat our mapper function from 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alex Smith\n",
    "## Description: mapper code for HW2.5\n",
    "\n",
    "# import our libraries to use regular expression and \n",
    "# read from the system arguments\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# initalize the outputs that we'll be using\n",
    "email_id = None\n",
    "word_act = None\n",
    "spam_ind = None\n",
    "    \n",
    "# create a regex that matches alphanumeric characters\n",
    "wordify = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # separate out each line based on the tabs\n",
    "    line = line.split(\"\\t\")\n",
    "    \n",
    "    # set the email id and spam indicator fields\n",
    "    email_id = line[0]\n",
    "    spam_ind = line[1]\n",
    "\n",
    "    # grab the text from the body and \n",
    "    # subject and concatenate it, if both exist\n",
    "    if len(line)==4:\n",
    "        text = line[2] + \" \" + line[3]\n",
    "    else:\n",
    "        text = line[2]\n",
    "\n",
    "    # convert the text into a list of \n",
    "    # words without punctuation\n",
    "    # also lowercase all the words\n",
    "    words = wordify.findall(text.lower())\n",
    "        \n",
    "    # loop through each word\n",
    "    for word in words:\n",
    "\n",
    "        # collect the line that we want to print out\n",
    "        info = email_id + \"\\t\" + word + \"\\t\" + str(spam_ind)\n",
    "\n",
    "        # let's print each email id, word, \n",
    "        # spam indicator\n",
    "        # we'll separate each value with a tab character\n",
    "        print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the mapper on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\tchristmas\t0\r\n",
      "0001.1999-12-10.farmer\ttree\t0\r\n",
      "0001.1999-12-10.farmer\tfarm\t0\r\n",
      "0001.1999-12-10.farmer\tpictures\t0\r\n",
      "0001.1999-12-10.farmer\tna\t0\r\n",
      "0001.1999-12-10.kaminski\tre\t0\r\n",
      "0001.1999-12-10.kaminski\trankings\t0\r\n",
      "0001.1999-12-10.kaminski\tthank\t0\r\n",
      "0001.1999-12-10.kaminski\tyou\t0\r\n",
      "0001.2000-01-17.beck\tleadership\t0\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x mapper.py\n",
    "!cat data/enronemail_1h.txt | ~/w261/mapper.py > testingM.txt\n",
    "!head testingM.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer function\n",
    "We modify the reducer function to ignore words that occur less than 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alex Smith\n",
    "## Description: reducer code for HW2.5\n",
    "\n",
    "# import the system libraries to read \n",
    "# from the input, from the \n",
    "# math library import the log function\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "# create a dictionary to store each email id, \n",
    "# spam indciator, list of words of \n",
    "# interest, and spam prediction\n",
    "emails = {}\n",
    "    \n",
    "# initalize the summary statistic counts\n",
    "all_words = 0\n",
    "all_emails = 0\n",
    "spam_emails = 0\n",
    "spam_ewords = 0 # number of words in spam emails\n",
    "\n",
    "# let's initalize a dictionary that will \n",
    "# hold the probabilities for \n",
    "# each and every word in the corpus\n",
    "words_probs = {}\n",
    "\n",
    "##\n",
    "##\n",
    "## gather summary statistics and words for each email\n",
    "##\n",
    "##\n",
    "        \n",
    "# loop through each line in the file\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by tabs\n",
    "    line = line.split(\"\\t\")\n",
    "\n",
    "    # pull out my values for each part of the line\n",
    "    email_id = line[0]\n",
    "    word_act = line[1]\n",
    "    spam_ind = int(line[2])\n",
    "\n",
    "    # let's update the number of words and \n",
    "    # the spam words (if it's spam)\n",
    "    all_words = all_words + 1\n",
    "    if spam_ind == 1:\n",
    "        spam_ewords = spam_ewords + 1\n",
    "\n",
    "    # if we don't already have the word in \n",
    "    # our dictionary of words, let's\n",
    "    # add it and initalize counts of zero\n",
    "    if word_act not in words_probs:\n",
    "        words_probs[word_act] = \\\n",
    "        {\"spam_count\":0,\"not_spam_count\":0}\n",
    "\n",
    "    # let's grab the dictionary for the word\n",
    "    word_dict = words_probs[word_act]\n",
    "\n",
    "    # let's increment the word counts \n",
    "    # for the word\n",
    "    if spam_ind == 1:\n",
    "        word_dict['spam_count'] = \\\n",
    "        word_dict['spam_count'] + 1\n",
    "    else:\n",
    "        word_dict['not_spam_count'] = \\\n",
    "        word_dict['not_spam_count'] + 1\n",
    "\n",
    "    # check to see if this email is already \n",
    "    # in the dictionary, and if\n",
    "    # its not already there, initalize it\n",
    "    if email_id not in emails:\n",
    "\n",
    "        # create a sub-dictionary within \n",
    "        # the email dictionary for each email\n",
    "        emails[email_id] = \\\n",
    "        {\"spam\":spam_ind, \"words\":[]}\n",
    "\n",
    "        # if it's not already there, \n",
    "        # let's also increment the email counter\n",
    "        # and the spam counter if it's spam\n",
    "        all_emails = all_emails + 1\n",
    "        if spam_ind == 1:\n",
    "            spam_emails = spam_emails + 1\n",
    "\n",
    "##\n",
    "##\n",
    "## gather the information for each email\n",
    "##\n",
    "##\n",
    "\n",
    "# set the minimum count for classification\n",
    "COUNT = 3\n",
    "\n",
    "# loop through each file in the list of count files\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # split the line by tabs\n",
    "    line = line.split(\"\\t\")\n",
    "\n",
    "    # pull out my values for each part \n",
    "    # of the line\n",
    "    email_id = line[0]\n",
    "    word_act = line[1]\n",
    "\n",
    "    # check to make sure the word occurs\n",
    "    # at least 3 times\n",
    "    word_count = \\\n",
    "    words_probs[word_act]['spam_count']+\\\n",
    "    words_probs[word_act]['nspam_count']\n",
    "\n",
    "    # if the word appears at least as many\n",
    "    # times as we have requested\n",
    "    if word_count >= COUNT:\n",
    "\n",
    "        # let's add the word to our list \n",
    "        # of words for this email\n",
    "        emails[email_id][\"words\"].append(word_act)    \n",
    "\n",
    "##\n",
    "##\n",
    "## build the naive bayes classifier\n",
    "##\n",
    "##\n",
    "\n",
    "# posterior probabilities\n",
    "prob_spam = float(spam_emails) / float(all_emails)\n",
    "prob_nspam = 1.0 - prob_spam\n",
    "\n",
    "# for each word, let's calculate the \n",
    "# conditional probability of spam given the word\n",
    "# and not spam given the word\n",
    "\n",
    "# let's define our LaPlace smoother \n",
    "SMOOTHER = 1.0\n",
    "VOCAB = float(len(words_probs))\n",
    "\n",
    "# look at each word\n",
    "for word in words_probs.keys():\n",
    "    \n",
    "    # set the find word that we'll be \n",
    "    # calculating the probabilities for\n",
    "    word = words_probs[word]\n",
    "    \n",
    "    # calculate the probability of the word\n",
    "    word['total'] = word['spam_count'] + \\\n",
    "    word['not_spam_count']\n",
    "    word['probs'] = float(word['total']) / \\\n",
    "    float(all_words)\n",
    "    \n",
    "    # calculate the probability of word \n",
    "    # given spam and add the smoother\n",
    "    word['wordGIVspam'] = \\\n",
    "    float(word['spam_count'] + SMOOTHER) / \\\n",
    "    float(spam_ewords + VOCAB)\n",
    "    \n",
    "    # calculate the probability of the word \n",
    "    # given not spam and add the smoother\n",
    "    word['wordGIVnspam'] = \\\n",
    "    float(word['not_spam_count'] + SMOOTHER) / \\\n",
    "    float(all_words-spam_ewords + VOCAB)\n",
    "\n",
    "# now let's print our model out to the file\n",
    "# this will be useful because it will make it\n",
    "# easier to load it in future functions\n",
    "\n",
    "# set the header\n",
    "print \"Word \\tCount \\tP(word|Spam) \\tP(word|Not Spam)\"\n",
    "\n",
    "# loop through each word\n",
    "for word in words_probs.keys():\n",
    "\n",
    "    # set the word name\n",
    "    word_name = word\n",
    "\n",
    "    # set the word that we'll be \n",
    "    # printing the probabilities for\n",
    "    word = words_probs[word]\n",
    "\n",
    "    # set each line as tab delimited\n",
    "    info = str(word_name) + \"\\t\" +\\\n",
    "    str(word['total']) + \"\\t\" +\\\n",
    "    str(word['wordGIVspam']) + \"\\t\" +\\\n",
    "    str(word['wordGIVnspam'])\n",
    "\n",
    "    # print each line\n",
    "    print info   \n",
    "\n",
    "# print a separating line to split each section\n",
    "print \"*~*~*~*~*\"\n",
    "\n",
    "##\n",
    "##\n",
    "## use the classifier to classify each email\n",
    "##\n",
    "##      \n",
    "\n",
    "# create a dictionary that will store the posterior\n",
    "# probabilities for each email\n",
    "email_probs = {}\n",
    "\n",
    "# now let's loop through each email in \n",
    "# the dictionary \n",
    "for email in emails.keys():\n",
    "    \n",
    "    # get the actual classification\n",
    "    truth = emails[email][\"spam\"]\n",
    "    \n",
    "    # if the email has no words, then set the \n",
    "    # prediction based on the posterior\n",
    "    # probabilities\n",
    "    if len(emails[email][\"words\"]) == 0:\n",
    "        spam_prob = log(prob_spam)\n",
    "        nspam_prob = log(prob_nspam)\n",
    "    \n",
    "    # else if the email has a word, then set \n",
    "    # the prediction based on the\n",
    "    # conditional probability\n",
    "    else:\n",
    "        \n",
    "        # initalize a set of spam and not spam \n",
    "        # probabilities that start with the \n",
    "        # priors\n",
    "        spam_prob = log(prob_spam)\n",
    "        nspam_prob = log(prob_nspam)\n",
    "        \n",
    "        # loop through each of the words in \n",
    "        # the list of words and \n",
    "        # update the spam and not spam probabilities\n",
    "        for word in emails[email]['words']:\n",
    "            \n",
    "            # let's grab the probabilities for spam\n",
    "            # and not spam\n",
    "            word_spam_prob = words_probs[word]['wordGIVspam']\n",
    "            word_nspam_prob = words_probs[word]['wordGIVnspam']\n",
    "            \n",
    "            # calculate the probability for each\n",
    "            # document by adding the log \n",
    "            # probabilities for each word\n",
    "            spam_prob = spam_prob + \\\n",
    "            log(word_spam_prob)\n",
    "            nspam_prob = nspam_prob + \\\n",
    "            log(word_nspam_prob)\n",
    "        \n",
    "        # add the email to the dictionary with its\n",
    "        # probabilities\n",
    "        email_probs[email] = [spam_prob,nspam_prob]\n",
    "        \n",
    "    # choose the prediction based on the \n",
    "    # probabilities\n",
    "    if(spam_prob > nspam_prob):\n",
    "        _prediction = 1\n",
    "    else:\n",
    "        _prediction = 0\n",
    "    \n",
    "    # print the output as the email id, the \n",
    "    # actual classification, the prediction\n",
    "    # as a tab-delimited line\n",
    "    info = email + \"\\t\" + str(truth) + \"\\t\" + \\\n",
    "    str(_prediction)\n",
    "    print info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the reducer on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \tCount \tP(word|Spam) \tP(word|Not Spam)\r\n",
      "yellow\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "four\t8\t0.000248138957816\t0.000202932372787\r\n",
      "prefix\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "railing\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "looking\t4\t0.000206782464847\t5.07330931967e-05\r\n",
      "granting\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "electricity\t1\t4.13564929694e-05\t0.000101466186393\r\n",
      "originality\t1\t8.27129859388e-05\t5.07330931967e-05\r\n",
      "homemakers\t1\t8.27129859388e-05\t5.07330931967e-05\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions and execute\n",
    "!chmod +x reducer.py\n",
    "!cat testingM.txt | sort -k1,1 | ~/w261/reducer.py > testingR.txt\n",
    "!head testingR.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Set the right permissions and execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 22:49:39 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/enronemail_1h.txt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/enronemail_1h.txt1464328179074\n",
      "rm: `/user/cloudera/w261-output-2-5': No such file or directory\n",
      "rm: cannot remove `/home/cloudera/w261/Outputs/Out_2_5': No such file or directory\n",
      "16/05/26 22:49:47 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/cloudera/w261/mapper.py, /home/cloudera/w261/reducer.py] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5648932769200571239.jar tmpDir=null\n",
      "16/05/26 22:49:49 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 22:49:50 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/05/26 22:49:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/26 22:49:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/26 22:49:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464226260520_0037\n",
      "16/05/26 22:49:51 INFO impl.YarnClientImpl: Submitted application application_1464226260520_0037\n",
      "16/05/26 22:49:52 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464226260520_0037/\n",
      "16/05/26 22:49:52 INFO mapreduce.Job: Running job: job_1464226260520_0037\n",
      "16/05/26 22:50:00 INFO mapreduce.Job: Job job_1464226260520_0037 running in uber mode : false\n",
      "16/05/26 22:50:00 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/26 22:50:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/26 22:50:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/26 22:50:36 INFO mapreduce.Job: Job job_1464226260520_0037 completed successfully\n",
      "16/05/26 22:50:36 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=228066\n",
      "\t\tFILE: Number of bytes written=817248\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=233302\n",
      "\t\tHDFS: Number of bytes written=252887\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5586688\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1083648\n",
      "\t\tTotal time spent by all map tasks (ms)=43646\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8466\n",
      "\t\tTotal vcore-seconds taken by all map tasks=43646\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8466\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5586688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1083648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32911\n",
      "\t\tMap output bytes=1032050\n",
      "\t\tMap output materialized bytes=227563\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=227563\n",
      "\t\tReduce input records=32911\n",
      "\t\tReduce output records=5592\n",
      "\t\tSpilled Records=65822\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=726\n",
      "\t\tCPU time spent (ms)=4630\n",
      "\t\tPhysical memory (bytes) snapshot=381284352\n",
      "\t\tVirtual memory (bytes) snapshot=2195144704\n",
      "\t\tTotal committed heap usage (bytes)=139984896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=233062\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=252887\n",
      "16/05/26 22:50:36 INFO streaming.StreamJob: Output directory: /user/cloudera/w261-output-2-5\n"
     ]
    }
   ],
   "source": [
    "# first let's clear our input directory to make\n",
    "# sure that we're starting off with a clean slate\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/*\n",
    "\n",
    "# modifies the permission to make the programs\n",
    "# executable\n",
    "!chmod +x ~/w261/mapper.py; chmod +x ~/w261/reducer.py\n",
    "\n",
    "# put the input data into the hadoop cluster\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/data/enronemail_1h.txt /user/cloudera/w261/\n",
    "\n",
    "# make sure that we don't already have this output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261-output-2-5\n",
    "!rm -r /home/cloudera/w261/Outputs/Out_2_5\n",
    "\n",
    "# run the hadoop command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-file /home/cloudera/w261/mapper.py    -mapper /home/cloudera/w261/mapper.py \\\n",
    "-file /home/cloudera/w261/reducer.py   -reducer /home/cloudera/w261/reducer.py \\\n",
    "-input /user/cloudera/w261/* -output /user/cloudera/w261-output-2-5\n",
    "\n",
    "# copy the output file to the local directory\n",
    "!hdfs dfs -copyToLocal /user/cloudera/w261-output-2-5/part-00000 /home/cloudera/w261/Outputs/\n",
    "\n",
    "# rename the file\n",
    "!mv /home/cloudera/w261/Outputs/part-00000 /home/cloudera/w261/Outputs/Out_2_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the output file\n",
    "We split our output file into:\n",
    "- Predictions_three: a file that shows our predictions for each model\n",
    "- Wordspreds_three: a file that shows our probabilities for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initalize the arrays to hold the information\n",
    "# for each document\n",
    "wordpreds = []\n",
    "predictions = []\n",
    "\n",
    "# open the output file\n",
    "with open (\"Outputs/Out_2_5\",\"r\") as myfile:\n",
    "    \n",
    "    # initalize a counter to keep track of the \n",
    "    # file divisions\n",
    "    count = 0\n",
    "    \n",
    "    # loop through every line in the file\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # strip the leading and trailing white space\n",
    "        line = line.strip()\n",
    "        \n",
    "        # we use the counter to keep track of\n",
    "        # file we write by incrementing it with\n",
    "        # every file divider\n",
    "        if line == \"*~*~*~*~*\":\n",
    "            count = count + 1\n",
    "        else:\n",
    "            \n",
    "            # add to wordpreds array\n",
    "            if count == 0:\n",
    "                wordpreds.append(line)\n",
    "            \n",
    "            # add to predicitons array\n",
    "            if count == 1:\n",
    "                predictions.append(line)\n",
    "\n",
    "# write to the files for each array\n",
    "with open(\"Outputs/wordspreds_three\",'w') as myfile:\n",
    "    for line in wordpreds:\n",
    "        myfile.write(line.strip()+\"\\n\")\n",
    "with open(\"Outputs/predictions_three\",'w') as myfile:\n",
    "    for line in predictions:\n",
    "        myfile.write(line.strip()+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare the training errors between the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error for our model that uses words with a minimum count of 3 is 0.44\n",
      "+----------------------------+----------------+\n",
      "|           Model            | Training Error |\n",
      "+----------------------------+----------------+\n",
      "|  Model without smoothing   |      0.0       |\n",
      "|    Model with smoothing    |      0.0       |\n",
      "| Model with 3 count minimum |      0.44      |\n",
      "+----------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# import Pretty table to help us compare errors\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# set the error for the model with smoothing\n",
    "three_error = trainingerror(\"Outputs/predictions_three\")\n",
    "print \"The training error for our model that uses \\\n",
    "words with a minimum count of 3 is\", three_error\n",
    "\n",
    "# create a new table to compare the \n",
    "# training errors\n",
    "pretty = PrettyTable([\"Model\",\"Training Error\"])\n",
    "pretty.add_row([\"Model without smoothing\",nosmooth_error])\n",
    "pretty.add_row([\"Model with smoothing\",smooth_error])\n",
    "pretty.add_row([\"Model with 3 count minimum\",three_error])\n",
    "\n",
    "# print the table\n",
    "print pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we only count words that appeared at least 3 times, we lose a lot of information. When we used every word, we were able to identify spam and not spam emails more easily because their vocabulary was non-overlapping. Now, we've reduced the vocabularly of the emails to only overlapping words. In essence, our model is only predicting not spam for all emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HW2.6 Benchmark \n",
    "*Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm. It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more. In this exercise, please complete the following:* <br>\n",
    "- *Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)*\n",
    "- *Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error*\n",
    "- *Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Data preparation for Sklearn\n",
    "Before putting the data through the sklearn algorithms, we have to prepare it. We do this by using the Sklearn's countvectorizers to transform the data into a bag of words. **Important note: we borrow much of this code from HW1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our bag of words has a shape of: (100, 5375)\n",
      "Our traiing labels have a shape of: (100,)\n"
     ]
    }
   ],
   "source": [
    "# import the count vectorizer function from the \n",
    "# appropriate Sklearn library and import numpy to reshape\n",
    "# the labels array into a 100 x 1 array\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# create arrays to hold the data\n",
    "email_ids = []\n",
    "train_labels = []\n",
    "email_text = []\n",
    "\n",
    "# begin by opening the file\n",
    "with open(\"data/enronemail_1h.txt\", \"r\") as myfile:\n",
    "    \n",
    "    # loop through each line\n",
    "    for record in myfile.readlines():\n",
    "        \n",
    "        # split each line based on the tabs\n",
    "        record = record.split(\"\\t\")\n",
    "        \n",
    "        # generate the text as a combination \n",
    "        # of the subject and body, if the email\n",
    "        # has both\n",
    "        if len(record) == 4:\n",
    "            text = record[2] + \" \" + record[3]\n",
    "        \n",
    "        # otherwise take the subject or body, \n",
    "        # whichever is present\n",
    "        else:\n",
    "            text = record[2]\n",
    "        \n",
    "        # add to the email_ids, to the labels, \n",
    "        # and to the email text\n",
    "        email_ids.append(record[0])\n",
    "        train_labels.append(record[1])\n",
    "        email_text.append(text)\n",
    "        \n",
    "# genearate a bag of words on the email texts \n",
    "# by using count vectorizer\n",
    "bag = CountVectorizer()\n",
    "train_data = bag.fit_transform(email_text)\n",
    "\n",
    "# convert email labels into a numpy array of size 100x1\n",
    "train_labels = np.array(train_labels).reshape(-1)\n",
    "\n",
    "# to verify that we've got the right thing, \n",
    "# let's print out the shape\n",
    "# of our training data and training labels. \n",
    "# it should be 100 rows \n",
    "# have a column for every word\n",
    "print \"Our bag of words has a shape of:\",train_data.shape\n",
    "print \"Our traiing labels have a shape of:\", train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outputing SKLearn Findings\n",
    "We create a function that will output the results from running SKLearn's algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputSK(filename,record_ids,truths,predictions):\n",
    "    \"\"\"Takes three arrays (record id, truth, \n",
    "    prediction) and a file name and arranges \n",
    "    them as a tab delimited file with columns: \n",
    "    record id, truth, and prediction and\n",
    "    outputs the file with specified file name\"\"\"\n",
    "    \n",
    "    # open the file as write-able\n",
    "    with open(filename, \"w\") as myfile:\n",
    "        \n",
    "        # loop through every record ids, truth, \n",
    "        # and prediction\n",
    "        for index,record in enumerate(record_ids):\n",
    "            \n",
    "            # create the line that has the information \n",
    "            # we'll write to the file\n",
    "            new_line = record_ids[index] + \"\\t\" + \\\n",
    "            truths[index] + \"\\t\" + \\\n",
    "            predictions[index] + \"\\n\"\n",
    "            \n",
    "            # write the line to the file\n",
    "            myfile.write(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SKLearn's Multinomial Naive Bayes Algorithm\n",
    "We use SKLearn's multinomial naive bayes algorithm on our email data to generate predictions of spam or not spam. We then use our output function to save the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out the output file 'sk_multinomial.txt' \n",
      "to see the results from the SK Learn\n",
      " Multinomial Naive Bayes algorithm.\n"
     ]
    }
   ],
   "source": [
    "# import the multinomial naive bayes algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create and train the model on our data\n",
    "sk_multinomial = MultinomialNB()\n",
    "sk_multinomial.fit(train_data,train_labels)\n",
    "\n",
    "# use our model to predict the training data\n",
    "sk_predictions = sk_multinomial.predict(train_data)\n",
    "\n",
    "# output the results to a file that we \n",
    "# can use for comparison\n",
    "outputSK(\"Outputs/sk_multinomial.txt\",email_ids,\\\n",
    "         train_labels,sk_predictions)\n",
    "\n",
    "print \"Check out the output file \\\n",
    "'sk_multinomial.txt' \\nto see the results from \\\n",
    "the SK Learn\\n Multinomial Naive Bayes algorithm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reporting the training error\n",
    "Now let's report the training error across the 4 models (our MapReduce without smoothing, our MapReduce with smoothing, our MapReduce only counting 3+ words, and SKLearn's basic Naive Bayes). We use the training error function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------+\n",
      "|             Model              | Training Error Rate |\n",
      "+--------------------------------+---------------------+\n",
      "| My MapReduce without smoothing |         0.0         |\n",
      "|  My MapReduce with smoothing   |         0.0         |\n",
      "|   Model with 3 count minimum   |         0.44        |\n",
      "|  SKLearn's Multinomial Model   |         0.0         |\n",
      "+--------------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# import the python library pretty table; \n",
    "# this wil help us print out the comparison \n",
    "# very clearly\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# get the training errors\n",
    "sk_multi_error = trainingerror(\"Outputs/sk_multinomial.txt\")\n",
    "\n",
    "\n",
    "# create the pretty table and add a row for each model\n",
    "pretty = PrettyTable([\"Model\",\"Training Error Rate\"])\n",
    "pretty.add_row([\"My MapReduce without smoothing\", nosmooth_error])\n",
    "pretty.add_row([\"My MapReduce with smoothing\", smooth_error])\n",
    "pretty.add_row([\"Model with 3 count minimum\",three_error])\n",
    "pretty.add_row([\"SKLearn's Multinomial Model\", sk_multi_error])\n",
    "\n",
    "# print out the table\n",
    "print pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the model with the 3 count minimum, our models and the SKLearn model achieved the same accuracy. This likely due to the fact that the vocabularlies for spam and not spam emails were fairly distinct. Our model with the 3 count minimum suffered from eliminating so much information and making the vocabularies more similar. We should also remember that we are reporting only training error here because we have only tested our models on our training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
