{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: comp.graphics\n",
      "Text: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "\n",
      "Label: talk.religion.misc\n",
      "Text: \n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      "\n",
      "Label: sci.space\n",
      "Text: \n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Label: alt.atheism\n",
      "Text: I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "\n",
      "\n",
      "Label: sci.space\n",
      "Text: AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#def P1(num_examples=5):\n",
    "### STUDENT START ###\n",
    "\n",
    "# set the number of example\n",
    "num_examples = 5\n",
    "\n",
    "# loop through each possible, printing the label, the text, and a new line\n",
    "for example in range(num_examples):\n",
    "    print \"Label:\", newsgroups_train.target_names[train_labels[example]]\n",
    "    print \"Text:\", train_data[example]\n",
    "    print \"\\n\"\n",
    "\n",
    "### STUDENT END ###\n",
    "#P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART A.\n",
      "The size of the vocabulary is 26879\n",
      "The average number of nonzero features for each example is 96.7059980334\n",
      "The portion is nonzero entries in the matrix is 0.00359782722696\n",
      "\n",
      "\n",
      "PART B.\n",
      "The 0th feature is a digit, 00\n",
      "The last feature is zyxel\n",
      "\n",
      "\n",
      "PART C.\n",
      "I confirm the the shape of the model is appropriate because the model still has 2034 rows, and has 4 columns, equal to the number of vocabulary words.\n",
      "Now, the average number of nonzero features for each example is 0.268436578171\n",
      "\n",
      "\n",
      "PART D.\n",
      "The bigram vectorizer has a vocabulary of 3168 , and the trigram vectorizer has a vocabulary of 29032 . I chose to bound my ngrams by white space to limit my ngrams to word boundaries.\n",
      "\n",
      "\n",
      "PART E.\n",
      "The size of the vocabulary when we prune words that appear in less than ten documents is 3064\n",
      "\n",
      "\n",
      "PART F.\n",
      "The fraction of words in dev data missing from the training data is 0.247876400345\n"
     ]
    }
   ],
   "source": [
    "#def P2():\n",
    "### STUDENT START ###\n",
    "\n",
    "# initalize a new count vectorizer with the goal of creating feature vectors \n",
    "bag = CountVectorizer()\n",
    "\n",
    "# transform the data into feature vectors\n",
    "bag_train = bag.fit_transform(train_data)\n",
    "\n",
    "## part a.\n",
    "print \"PART A.\"\n",
    "\n",
    "# find the shape of the feature vectors\n",
    "bag_shape = bag_train.shape\n",
    "\n",
    "print \"The size of the vocabulary is\", bag_shape[1]\n",
    "\n",
    "# create a variable to hold the total number of non-zero features for each example\n",
    "nonzeroes = 0\n",
    "\n",
    "# loop through each example in the training data\n",
    "for example in bag_train:\n",
    "    \n",
    "    # add the number of non-zeros to the nonzeros variable\n",
    "    nonzeroes = nonzeroes + example.nnz\n",
    "\n",
    "# calculate the average number of nonzero features for each example\n",
    "nonzeroes_avg = float(nonzeroes) / float(len(train_data))\n",
    "\n",
    "print \"The average number of nonzero features for each example is\", nonzeroes_avg\n",
    "\n",
    "# find the number of non-zero entries in the matrix\n",
    "bag_nonzero = bag_train.nnz\n",
    "\n",
    "# calculate the total number of entries\n",
    "bag_total = bag_shape[0] * bag_shape[1]\n",
    "\n",
    "# calcualte the portion of entries that are non-zero\n",
    "bag_nonzero_portion = float(bag_nonzero) / float(bag_total)\n",
    "\n",
    "print \"The portion is nonzero entries in the matrix is\", bag_nonzero_portion\n",
    "\n",
    "## part b. \n",
    "print \"\\n\"\n",
    "print \"PART B.\"\n",
    "\n",
    "# get the 0th feature name\n",
    "print \"The 0th feature is a digit,\", bag.get_feature_names()[0]\n",
    "\n",
    "# get the last feature name\n",
    "print \"The last feature is\", bag.get_feature_names()[-1]\n",
    "\n",
    "## part c. \n",
    "print \"\\n\"\n",
    "print \"PART C.\"\n",
    "\n",
    "# specify my own vocabulary\n",
    "vocab = {\"atheism\":0, \"graphics\":1, \"space\":2, \"religion\":3}\n",
    "\n",
    "# create a new model with the vocabulary\n",
    "bag2 = CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "# fit the model to the data\n",
    "bag_train2 = bag2.fit_transform(train_data)\n",
    "\n",
    "# find the shape of the model\n",
    "bag_shape2 = bag_train2.shape\n",
    "\n",
    "# the shape is correct if we have columns of the new vocabulary and the same number\n",
    "# of rows for examples\n",
    "print \"I confirm the the shape of the model is appropriate because the model still has\", bag_shape2[0], \"rows, \\\n",
    "and has\", bag_shape2[1], \"columns, equal to the number of vocabulary words.\"\n",
    "\n",
    "# create a variable to hold the total number of non-zero features for each example\n",
    "nonzeroes2 = 0\n",
    "\n",
    "# loop through each example in the training data\n",
    "for example in bag_train2:\n",
    "    \n",
    "    # add the number of non-zeros to the nonzeros variable\n",
    "    nonzeroes2 = nonzeroes2 + example.nnz\n",
    "\n",
    "# calculate the average number of nonzero features for each example\n",
    "nonzeroes_avg2 = float(nonzeroes2) / float(len(train_data))\n",
    "\n",
    "print \"Now, the average number of nonzero features for each example is\", nonzeroes_avg2 \n",
    "\n",
    "## part d. \n",
    "print \"\\n\"\n",
    "print \"PART D.\"\n",
    "\n",
    "# create a bigram count vectorizer, use 'char_wb' option to limit the bigrams to within\n",
    "# word boundaries\n",
    "bigram = CountVectorizer(analyzer='char_wb',ngram_range=(0,2))\n",
    "\n",
    "# fit and transform the data with the bigram count vectorizer\n",
    "bigram_vector = bigram.fit_transform(train_data)\n",
    "\n",
    "# create a trigram count vectorizer, use 'char_wb' option to limit the bigrams to within\n",
    "# word boundaries\n",
    "trigram = CountVectorizer(analyzer='char_wb',ngram_range=(0,3))\n",
    "\n",
    "# fit and transform the data with the trigram count vectorizer\n",
    "trigram_vector = trigram.fit_transform(train_data)\n",
    "\n",
    "# print the concluding statement\n",
    "print \"The bigram vectorizer has a vocabulary of\", bigram_vector.shape[1], \", and the trigram \\\n",
    "vectorizer has a vocabulary of\", trigram_vector.shape[1], \". I chose to bound my ngrams by white space to \\\n",
    "limit my ngrams to word boundaries.\"\n",
    "\n",
    "## part e. \n",
    "print \"\\n\"\n",
    "print \"PART E.\"\n",
    "\n",
    "# initalize a new count vectorizer that prunes words in less than 10 documents\n",
    "prune = CountVectorizer(min_df=10)\n",
    "\n",
    "# fit and transform the data with the pruning count vectorizer\n",
    "prune_vector = prune.fit_transform(train_data)\n",
    "\n",
    "# print the size of the vocabulary\n",
    "print \"The size of the vocabulary when we prune words that appear in less than ten documents is\", prune_vector.shape[1]\n",
    "\n",
    "## part f. \n",
    "print \"\\n\"\n",
    "print \"PART F.\"\n",
    "\n",
    "# create a new count vectorizer\n",
    "bagdev = CountVectorizer()\n",
    "\n",
    "# covert the vocabulary to a bag of words\n",
    "bagdev_train = bagdev.fit_transform(dev_data)\n",
    "\n",
    "# setify the vocabulary of both the training data and the dev data\n",
    "vocab_train = set(bag.vocabulary_)\n",
    "vocab_dev = set(bagdev.vocabulary_)\n",
    "\n",
    "# dev words missing from training data\n",
    "dev_missing = len(vocab_dev - vocab_train)\n",
    "\n",
    "# total dev words\n",
    "dev_total = len(vocab_dev)\n",
    "\n",
    "# calculate the fraction missing\n",
    "dev_missing_fraction = float(dev_missing) / float(dev_total)\n",
    "\n",
    "print \"The fraction of words in dev data missing from the training data is\", dev_missing_fraction\n",
    "\n",
    "### STUDENT END ###\n",
    "#P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K NEAREST NEIGHBORS\n",
      "When creating a k nearest neighbors model, the optimal value of k, that produces the highest F1 score, is 7 , which produces a F1 score of 0.450479100061 . The accuracy is 0.443786982249\n",
      "\n",
      "\n",
      "MULTINOMIAL NAIVE BAYES\n",
      "The best alpha for the multinomial naive bayes model is 0.01 with an accuracy of 0.779585798817\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION MODEL\n",
      "The best C value for the logistic regression model is 0.5 with an accuracy of 0.71449704142\n",
      "\n",
      "\n",
      "PART A.\n",
      "K nearest neighbors does not work well because there are too many features. In high dimensional like the example here, there is little difference between the closest and farthest points.\n",
      "\n",
      "\n",
      "PART B.\n",
      "Logistic regression doesn't work as well as Naive Bayes because the logistic regression model draws a linear decision boundary whereas Naive Bayes can draw a nonlinear decision boundary.\n",
      "\n",
      "\n",
      "PART C.\n",
      "C: 0.0001 |  [0.0077, 0.0119, 0.0094, 0.0091]\n",
      "C: 0.0010 |  [0.1651, 0.201, 0.1807, 0.1872]\n",
      "C: 0.0100 |  [2.5415, 2.9397, 2.8625, 2.25]\n",
      "C: 0.1000 |  [27.1279, 24.6594, 27.4587, 23.0245]\n",
      "C: 0.5000 |  [102.6214, 83.1035, 99.0207, 88.9928]\n",
      "C: 1.0000 |  [166.9776, 130.9002, 157.9589, 145.7439]\n",
      "C: 2.0000 |  [257.6129, 197.9211, 239.7811, 226.5991]\n",
      "C: 5.0000 |  [422.9158, 322.9439, 389.6248, 378.1508]\n",
      "C: 10.0000 |  [585.5139, 448.3766, 538.3965, 530.8331]\n",
      "C: 20.0000 |  [786.869, 601.4791, 723.8779, 721.6899]\n",
      "C: 100.0000 |  [1402.9252, 1098.0101, 1304.3267, 1323.5327]\n",
      "\n",
      "As we increase our C value, we also increase the sum of the weights. This relationship is not linear. Each subsequent increase in C does not increase the sum of the squared weights as much as the previous increase in C. This implies that the relationship is more logarithmic than linear.\n"
     ]
    }
   ],
   "source": [
    "#def P3():\n",
    "### STUDENT START ###\n",
    "\n",
    "## K nearest neighbors model\n",
    "print \"K NEAREST NEIGHBORS\"\n",
    "\n",
    "# vectorize the dev data with the same model\n",
    "bag_dev = bag.transform(dev_data)\n",
    "\n",
    "# initalize variables to hold the maximum k and maximum f1 score\n",
    "max_k = 0\n",
    "max_f1 = 0\n",
    "\n",
    "# lets loop through a reasonable range of k's, 1 through 20\n",
    "for k in range(1,21):\n",
    "    # initalize a k-nearest neighbors classifier\n",
    "    kneighbors = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # fit the model on the training data\n",
    "    kneighbors.fit(bag_train, train_labels)\n",
    "\n",
    "    # predict the development data\n",
    "    predictions = kneighbors.predict(bag_dev)\n",
    "\n",
    "    # calculate the f1 score\n",
    "    f1_score = metrics.f1_score(dev_labels, predictions, average='weighted')\n",
    "    \n",
    "    # check to see if the calculated f1 score is greater than the stored max f1 score\n",
    "    if f1_score > max_f1:\n",
    "        \n",
    "        # set the new k and max f1 score\n",
    "        max_k = k\n",
    "        max_f1 = f1_score\n",
    "        \n",
    "        # calculate an accuracy value\n",
    "        accuracy = np.mean(predictions==dev_labels)\n",
    "\n",
    "# print out the best k value and its accompanying f1 score\n",
    "print \"When creating a k nearest neighbors model, the optimal value of k, that produces the highest \\\n",
    "F1 score, is\", max_k, \", which produces a F1 score of\", max_f1, \". The accuracy is\", accuracy\n",
    "\n",
    "## Multinomial naive bayes model\n",
    "print \"\\n\"\n",
    "print \"MULTINOMIAL NAIVE BAYES\"\n",
    "\n",
    "# initalize the multinomail naive bayes model\n",
    "Multinomial = MultinomialNB()\n",
    "\n",
    "# initalize a set of reasonable alphas that we would like to search for the optimal alpha\n",
    "MNparameters = {'alpha':[0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "\n",
    "# set the alpha search with the given alpha options and the Multinomial model\n",
    "alpha_search = GridSearchCV(Multinomial,MNparameters)\n",
    "\n",
    "# fit the Gridsearch model on the training data\n",
    "alpha_search.fit(bag_train,train_labels)\n",
    "\n",
    "# find the best parameter\n",
    "best_alpha = alpha_search.best_params_\n",
    "\n",
    "# fit a model with the best alpha\n",
    "Multinomial_optimal = MultinomialNB(alpha = best_alpha['alpha'])\n",
    "Multinomial_optimal.fit(bag_train,train_labels)\n",
    "\n",
    "# calculate the accuracy for this multinomial model\n",
    "MN_predictions = Multinomial_optimal.predict(bag_dev)\n",
    "MN_accuracy = np.mean(MN_predictions == dev_labels)\n",
    "\n",
    "# write the concluding sentence\n",
    "print \"The best alpha for the multinomial naive bayes model is\", best_alpha['alpha'], \"with \\\n",
    "an accuracy of\", MN_accuracy\n",
    "\n",
    "## Logistic regression model\n",
    "print \"\\n\"\n",
    "print \"LOGISTIC REGRESSION MODEL\"\n",
    "\n",
    "# initalize the logistic regression model\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# initalize a reasonable set of C values\n",
    "Lparameters = {'C':[0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 100.0]}\n",
    "\n",
    "# set the C search with the given C options and the logistic model\n",
    "C_search = GridSearchCV(logistic,Lparameters)\n",
    "\n",
    "# fit the Gridsearch model to the data\n",
    "C_search.fit(bag_train,train_labels)\n",
    "\n",
    "# find the best C parameter\n",
    "best_C = C_search.best_params_\n",
    "\n",
    "# initalize a model with the best C\n",
    "logistic_optimal = LogisticRegression(C = best_C['C'])\n",
    "logistic_optimal.fit(bag_train,train_labels)\n",
    "\n",
    "# use the model to predict the dev data and comput the accuracy\n",
    "L_predictions = logistic_optimal.predict(bag_dev)\n",
    "L_accuracy = np.mean(L_predictions == dev_labels)\n",
    "\n",
    "# write the concluding sentence\n",
    "print \"The best C value for the logistic regression model is\", best_C['C'], \"with an accuracy of\", L_accuracy\n",
    "\n",
    "## part answers\n",
    "\n",
    "## part a.\n",
    "print \"\\n\"\n",
    "print \"PART A.\"\n",
    "print \"K nearest neighbors does not work well because there are too many features. In high dimensional \\\n",
    "like the example here, there is little difference between the closest and farthest points.\"\n",
    "\n",
    "## part b. \n",
    "print \"\\n\"\n",
    "print \"PART B.\"\n",
    "print \"Logistic regression doesn't work as well as Naive Bayes because the logistic regression model draws a \\\n",
    "linear decision boundary whereas Naive Bayes can draw a nonlinear decision boundary.\"\n",
    "\n",
    "## part c.\n",
    "print \"\\n\"\n",
    "print \"PART C.\"\n",
    "\n",
    "# loop through each of my potential C values\n",
    "for c in Lparameters['C']: \n",
    "    \n",
    "    # create the logistic model using the c value\n",
    "    logistics = LogisticRegression(C = c)\n",
    "    \n",
    "    # fit the model on the data\n",
    "    logistics.fit(bag_train, train_labels)\n",
    "    \n",
    "    # create an array to store the sums of the squared coefficients for each class\n",
    "    sum_squares = []\n",
    "    \n",
    "    # loop through each of the classes\n",
    "    for category in logistics.coef_:\n",
    "        \n",
    "        # append the sum of the squares of the coefficients to the storing array\n",
    "        # round to 4 decimal points to make it cleaner\n",
    "        sum_squares.append(round(sum(category**2),4))\n",
    "        \n",
    "    # print out each C value with the corresponding sum of the squared coeffecients\n",
    "    print \"C: %.4f | \" % (c), sum_squares \n",
    "    \n",
    "# briefly describe the relationship between the sum of these weights and the C values\n",
    "print \"\\nAs we increase our C value, we also increase the sum of the weights. This relationship is not linear. \\\n",
    "Each subsequent increase in C does not increase the sum of the squared weights as much as the previous increase \\\n",
    "in C. This implies that the relationship is more logarithmic than linear.\"\n",
    "\n",
    "### STUDENT END ###\n",
    "#P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: See above code for answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION, GREATEST WEIGHTS WITH WHOLE WORDS\n",
      "Classes:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "Feature: speaking  | Weights: [1.125, -0.3982, -0.4204, -0.3962]\n",
      "Feature: imaginative  | Weights: [1.0304, -0.0974, -0.3208, -0.8349]\n",
      "Feature: attributes  | Weights: [0.9899, -0.2207, -0.3406, -0.4633]\n",
      "Feature: pointy  | Weights: [0.9538, -0.6168, -0.7924, -0.0644]\n",
      "Feature: ucrl  | Weights: [0.9395, -0.4106, -0.4494, -0.4348]\n",
      "Feature: plastic  | Weights: [-0.7584, 1.9361, -1.3359, -0.7625]\n",
      "Feature: 89  | Weights: [-0.5825, 1.3463, -0.826, -0.4702]\n",
      "Feature: capitalization  | Weights: [-0.3348, 1.2668, -0.8065, -0.6263]\n",
      "Feature: complicates  | Weights: [-0.3589, 1.1251, -0.7021, -0.3787]\n",
      "Feature: washington  | Weights: [0.1439, 0.9775, -0.6819, -0.4865]\n",
      "Feature: 2004  | Weights: [-1.2601, -1.3162, 2.1622, -1.1708]\n",
      "Feature: exiting  | Weights: [-0.414, -0.6714, 1.2248, -0.6289]\n",
      "Feature: mania  | Weights: [-0.5724, -0.4798, 1.0114, -0.4676]\n",
      "Feature: future  | Weights: [-0.4699, -0.4654, 0.9364, -0.333]\n",
      "Feature: 2009  | Weights: [-0.3554, -0.3936, 0.9198, -0.3808]\n",
      "Feature: assisting  | Weights: [-0.7401, -0.4092, -0.5251, 1.1474]\n",
      "Feature: notwithstanding  | Weights: [-0.6077, -0.4186, -0.2704, 1.117]\n",
      "Feature: 074  | Weights: [-0.533, -0.1068, -0.3161, 1.0548]\n",
      "Feature: each  | Weights: [-0.3089, -0.2736, -0.448, 0.9129]\n",
      "Feature: hiking  | Weights: [-0.7936, -0.0795, -0.1491, 0.9047]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION, GREATEST WEIGHTS WITH BIGRAMS\n",
      "Classes:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "Feature: $8  | Weights: [0.1452, -0.0622, -0.0706, -0.0467]\n",
      "Feature: p)  | Weights: [0.1045, -0.2041, -0.0661, 0.0165]\n",
      "Feature: tu  | Weights: [0.0838, -0.0319, -0.0328, 0.0226]\n",
      "Feature: hi  | Weights: [0.076, -0.0471, -0.0026, -0.021]\n",
      "Feature: u]  | Weights: [0.0729, 0.0428, -0.0238, -0.0375]\n",
      "Feature: g+  | Weights: [-0.0208, 0.2729, -0.069, -0.09]\n",
      "Feature: 8*  | Weights: [-0.0414, 0.2443, -0.0153, -0.0133]\n",
      "Feature: vu  | Weights: [-0.0443, 0.2435, -0.1315, -0.0202]\n",
      "Feature: +  | Weights: [-0.0314, 0.2384, -0.0699, -0.0234]\n",
      "Feature: ?'  | Weights: [-0.0191, 0.2366, -0.0542, -0.0171]\n",
      "Feature: 6a  | Weights: [-0.0511, -0.2361, 0.1796, -0.0353]\n",
      "Feature: *u  | Weights: [-0.0935, -0.1135, 0.1666, -0.0643]\n",
      "Feature: 3g  | Weights: [-0.0123, -0.1061, 0.1538, -0.0282]\n",
      "Feature: /o  | Weights: [-0.0217, -0.2897, 0.1321, -0.0127]\n",
      "Feature: l1  | Weights: [-0.02, -0.1446, 0.1225, -0.0442]\n",
      "Feature:  4  | Weights: [-0.0017, -0.0489, -0.0222, 0.0949]\n",
      "Feature: f4  | Weights: [-0.0191, -0.0008, -0.0507, 0.088]\n",
      "Feature: pt  | Weights: [-0.0634, -0.0719, -0.0592, 0.0838]\n",
      "Feature: l<  | Weights: [-0.008, 0.0369, -0.0986, 0.0824]\n",
      "Feature: js  | Weights: [0.0009, -0.0178, -0.0793, 0.078]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The bigram logarithmic regression model had much lower weights than the word based model, implying that a given bigram feature provides less predictive power than a complete word. I was surprised by the fact that nonsensical features, like \"8*\" were the bigrams with the greatest weights. I would have expected more word prefixes and suffixes.\n"
     ]
    }
   ],
   "source": [
    "#def P4():\n",
    "### STUDENT START ###\n",
    "\n",
    "## logistic regression model with words\n",
    "\n",
    "# initalize a new logistic regression model\n",
    "logistic_words = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data\n",
    "logistic_words.fit(bag_train,train_labels)\n",
    "\n",
    "# write a function to find the largest coefficients for a logarithmic regression model\n",
    "def findMaxCoef(model, num_max = 1):\n",
    "    \"Finds the top num_max weights for each of the classes of a logistic regression model\"\n",
    "\n",
    "    # initalize an array to hold the indices of these weights, the ending length of the\n",
    "    indices = []\n",
    "    \n",
    "    # loop through each class' coefficients\n",
    "    for category in model.coef_:\n",
    "        \n",
    "        # create a new list to hold all the coefficients, this is one we intend to modify\n",
    "        # as we find the top num_max weights\n",
    "        coefficients = []\n",
    "        \n",
    "        # copy the coefficients into a new list\n",
    "        for coefficient in category:\n",
    "            coefficients.append(coefficient)\n",
    "\n",
    "        # add to the indices the number of weights we want\n",
    "        for weight in range(num_max):\n",
    "            \n",
    "            # get the index of the current max\n",
    "            index = np.argmax(coefficients)\n",
    "            \n",
    "            # add the current max of the coefficients\n",
    "            indices.append(index)\n",
    "            \n",
    "            # replace the max coefficient with zero\n",
    "            coefficients[index] = 0\n",
    "    \n",
    "    # return the array of indices, which will be the number of classes multiplied by the number of weights\n",
    "    return indices\n",
    "\n",
    "# write a function to create a table for each class of a logarthmic function with the highest weights\n",
    "def tableLogCoefs(model, bagged, num_max = 1):\n",
    "    \"function that takes a logarthmic model and outputs a class x num_max table, where num_max is the largest features\"\n",
    "    \n",
    "    # find the indices for the largest num_max weights\n",
    "    indices = findMaxCoef(model, num_max)\n",
    "    \n",
    "    # loop through each index printing out the weight for each class\n",
    "    for index in indices:\n",
    "        \n",
    "        # create an array to hold the weights for each class\n",
    "        weights = []\n",
    "        \n",
    "        # loop through each class\n",
    "        for category in model.coef_:\n",
    "            \n",
    "            # append to the weights array, the coefficient at the index\n",
    "            # round to 4 decimal places to make it easier to read\n",
    "            weights.append(round(category[index],4))\n",
    "        \n",
    "        # print out the feature and weights for each class\n",
    "        print \"Feature:\", bagged.vocabulary_.keys()[index], \" | Weights:\", weights\n",
    "\n",
    "# create the table for the logistic regression model with the top 5 weights for each class\n",
    "# print the labels for the classes at the top\n",
    "print \"LOGISTIC REGRESSION, GREATEST WEIGHTS WITH WHOLE WORDS\"\n",
    "print \"Classes: \", newsgroups_train.target_names\n",
    "tableLogCoefs(logistic_words, bag, 5)\n",
    "print \"\\n\"\n",
    "        \n",
    "## logistic regression model with bigrams\n",
    "\n",
    "# create a new logistic regression model that uses bigram words instead\n",
    "# fit it to the bigram data, already created in question 2 above\n",
    "logistic_bigrams = LogisticRegression()\n",
    "logistic_bigrams.fit(bigram_vector,train_labels)\n",
    "\n",
    "# use our already existing functions to create the table of weights for a bigram analysis\n",
    "# remember that we already created a bigram bag of words above, let's re-use it\n",
    "print \"\\n\"\n",
    "print \"LOGISTIC REGRESSION, GREATEST WEIGHTS WITH BIGRAMS\"\n",
    "print \"Classes: \", newsgroups_train.target_names\n",
    "tableLogCoefs(logistic_bigrams, bigram, 5)\n",
    "\n",
    "## concluding thoughts\n",
    "\n",
    "# print some concluding thoughts\n",
    "print \"\\n\"\n",
    "print \"\\n\"\n",
    "print \"The bigram logarithmic regression model had much lower weights than the word based model, implying \\\n",
    "that a given bigram feature provides less predictive power than a complete word. I was surprised by the fact that\\\n",
    " nonsensical features, like \\\"8*\\\" were the bigrams with the greatest weights. I would have expected more word \\\n",
    "prefixes and suffixes.\"\n",
    "\n",
    "### STUDENT END ###\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: See above for answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for the basic logistic regression model is 0.694417287185\n",
      "The F1 score for the logistic regression model with preprocessed data is 0.727065523843\n",
      "By pre-processing my data, I improved the F1 score by 0.0326482366573\n",
      "My new preprocessor reduced the size of my dictionary by 735 words.\n"
     ]
    }
   ],
   "source": [
    "#def empty_preprocessor(s):\n",
    "#    return s\n",
    "\n",
    "#def better_preprocessor(s):\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#def P5():\n",
    "### STUDENT START ###\n",
    "\n",
    "# let's create a function to preprocess our data, let's start by lowercasing everything\n",
    "def preLower(text_array):\n",
    "    \"lowercase all the words for each text in a given array of texts\"\n",
    "    \n",
    "    # create an array to hold the preprocessed text\n",
    "    processed = []\n",
    "    \n",
    "    # loop through each text in the array of texts\n",
    "    for text in text_array:\n",
    "        \n",
    "        # lowercase the text and append it to the processed array\n",
    "        processed.append(text.lower())\n",
    "    \n",
    "    # return the preprocessed text\n",
    "    return processed\n",
    "\n",
    "# let's create a function to replace numbers with a token, ^\n",
    "def preNoNumber(text_array):\n",
    "    \"replace all numbers with a token, ^\"\n",
    "    \n",
    "    # create an array to hold the preprocessed text\n",
    "    processed = []\n",
    "    \n",
    "    # loop through each text in the array of texts\n",
    "    for text in text_array:\n",
    "        \n",
    "        # replace each sequence of digits with the token \"^\" and append it to the processed array\n",
    "        processed.append(re.sub(\"\\d+\",\"^\",text))\n",
    "    \n",
    "    # return the preprocessed text\n",
    "    return processed\n",
    "\n",
    "# let's create a function to replace other non-letter characters with a space\n",
    "def preReadable(text_array):\n",
    "    \"replace all non-letter characters with a space\"\n",
    "    \n",
    "    # create an array to hold the preprocessed text\n",
    "    processed = []\n",
    "    \n",
    "    # loop through each text in the array of texts\n",
    "    for text in text_array:\n",
    "        \n",
    "        # remove each sequence of non-alphanumeric characters and append it to the processed array\n",
    "        # we exempt \"^\" because it our token for numbers\n",
    "        processed.append(re.sub(\"[^a-zA-Z0-9 ^]\",\" \",text))\n",
    "    \n",
    "    # return the preprocessed text\n",
    "    return processed\n",
    "\n",
    "# let's create a function to knock out all long words\n",
    "def preShorten(text_array):\n",
    "    \"deletes all words longer than 8\"\n",
    "    \n",
    "    # create an array to hold the preprocessed text\n",
    "    processed = []\n",
    "    \n",
    "    # loop through each text in the array of texts\n",
    "    for text in text_array:\n",
    "        \n",
    "        # remove each word longer than 8 \n",
    "        processed.append(re.sub(r\"\\W*\\b\\w{8,}\\b\",\"\",text))\n",
    "    \n",
    "    # return the preprocessed text\n",
    "    return processed\n",
    "\n",
    "# let's create a function that runs a text through all the preprocessers we just created\n",
    "def preProcessAll(text_array):\n",
    "    \"runs an array of texts through multiple pre-processors\"\n",
    "    \n",
    "    # return the array that is completely pre-processed\n",
    "    # return preLower(preNoNumber(preReadable(preShorten(text_array))))\n",
    "    # I have commented out the above code because the resulting pre-processing\n",
    "    # actually produces worse results. It can be uncommented to completely pre-\n",
    "    # process the text\n",
    "    \n",
    "    # return the preprocessed array\n",
    "    return preReadable(text_array)\n",
    "\n",
    "# preproccess the training and the dev data\n",
    "pre_train = preProcessAll(train_data)\n",
    "pre_dev = preProcessAll(dev_data)\n",
    "\n",
    "# create a new bag of words model, fitted to the training data\n",
    "# also go ahead and transform the dev data\n",
    "# as a preprocessing step, create the vectorizer to only use English words\n",
    "pre_bag = CountVectorizer(stop_words='english')\n",
    "pre_train_bag = pre_bag.fit_transform(pre_train)\n",
    "pre_dev_bag = pre_bag.transform(pre_dev)\n",
    "\n",
    "# create a base logistic regression model for comparison\n",
    "logistic_base = LogisticRegression()\n",
    "\n",
    "# fit the base logistic regression model to the data and use it to predict the dev data\n",
    "logistic_base.fit(bag_train,train_labels)\n",
    "predictions_base = logistic_base.predict(bag_dev)\n",
    "\n",
    "# calculate the f1 score\n",
    "f1_score_base = metrics.f1_score(dev_labels, predictions_base, average='weighted')\n",
    "print \"The F1 score for the basic logistic regression model is\", f1_score_base\n",
    "\n",
    "# create a logistic regression model for the preprocessed data\n",
    "logistic_pre = LogisticRegression()\n",
    "\n",
    "# fit the logistic regression to the data and use it to predict the dev data\n",
    "logistic_pre.fit(pre_train_bag,train_labels)\n",
    "predictions_pre = logistic_pre.predict(pre_dev_bag)\n",
    "\n",
    "# calculate the f1 score\n",
    "f1_score_pre = metrics.f1_score(dev_labels, predictions_pre, average='weighted')\n",
    "print \"The F1 score for the logistic regression model with preprocessed data is\", f1_score_pre\n",
    "print \"By pre-processing my data, I improved the F1 score by\", f1_score_pre - f1_score_base\n",
    "\n",
    "# calculate the reduction in the size of the dictionary\n",
    "print \"My new preprocessor reduced the size of my dictionary by\", bag_train.shape[1] - pre_train_bag.shape[1], \"words.\"\n",
    "\n",
    "### STUDENT END ###\n",
    "#P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic regression with L1 penalty produces 1609 nonzero weights, while the logistic regression with L2 penalty produces 107516 nonzero weights. The L1 penalty produces 105907 fewer weights.\n",
      "\n",
      "\n",
      "C: 0.1  | Vocabulary Size: 212  | Accuracy: 0.380177514793\n",
      "C: 0.5  | Vocabulary Size: 733  | Accuracy: 0.486686390533\n",
      "C: 1.0  | Vocabulary Size: 1104  | Accuracy: 0.504437869822\n",
      "C: 5.0  | Vocabulary Size: 2339  | Accuracy: 0.525147928994\n",
      "C: 10.0  | Vocabulary Size: 2571  | Accuracy: 0.538461538462\n",
      "C: 20.0  | Vocabulary Size: 3348  | Accuracy: 0.545857988166\n",
      "C: 50.0  | Vocabulary Size: 5881  | Accuracy: 0.609467455621\n",
      "C: 100.0  | Vocabulary Size: 7561  | Accuracy: 0.621301775148\n",
      "C: 250.0  | Vocabulary Size: 11831  | Accuracy: 0.671597633136\n",
      "C: 1000.0  | Vocabulary Size: 23344  | Accuracy: 0.687869822485\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aa6d490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def P6():\n",
    "\n",
    "# Keep this random seed here to make comparison easier.\n",
    "np.random.seed(0)\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "# create a new logistic regression model with L1 penalty and fit it to the data\n",
    "logistic_L1 = LogisticRegression(penalty='l1')\n",
    "logistic_L1.fit(bag_train,train_labels)\n",
    "\n",
    "# create a logistic regression model with the standard L2 penalty as a base comparison\n",
    "logistic_L2 = LogisticRegression(penalty='l2')\n",
    "logistic_L2.fit(bag_train,train_labels)\n",
    "\n",
    "# compare the nonzero weights of the logistic regressions created with L1 and L2 penalties\n",
    "print \"The logistic regression with L1 penalty produces\", np.count_nonzero(logistic_L1.coef_), \"nonzero weights,\\\n",
    " while the logistic regression with L2 penalty produces\", np.count_nonzero(logistic_L2.coef_), \"nonzero weights. \\\n",
    "The L1 penalty produces\", np.count_nonzero(logistic_L2.coef_) - np.count_nonzero(logistic_L1.coef_), \"fewer weights.\"\n",
    "print \"\\n\"\n",
    "\n",
    "## create a plot to compare the accuracy of the retrained model with the size of the vocabulary\n",
    "\n",
    "# set a reasonable range of C values to test\n",
    "Cs = [0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0, 250.0, 1000.0]\n",
    "\n",
    "# initalize an array to hold the vocabulary sizes and accuracies as we go about the pruning\n",
    "vocabulary_sizes = []\n",
    "accuracies = []\n",
    "\n",
    "# train a logistic model with each of these C values\n",
    "for c in Cs:\n",
    "    \n",
    "    # create a new bag of words model as we intend to destroy some of the vocabulary as we remove features\n",
    "    # initalize it with a fit_transform on the inital data\n",
    "    bag_log = CountVectorizer()\n",
    "    bag_log.fit_transform(train_data)\n",
    "    \n",
    "    # vectorize the training data\n",
    "    bag_log_train = bag_log.transform(train_data)\n",
    "    \n",
    "    # initalize a new logistic regression model, relax the convergence criteria and set the C\n",
    "    # fit the model on the training data\n",
    "    logistic_L1_c = LogisticRegression(tol=0.01, C=c, penalty='l1')\n",
    "    logistic_L1_c.fit(bag_log_train,train_labels)\n",
    "    \n",
    "    # create an array to store the sum of the weights across all four classes\n",
    "    weights = []\n",
    "    \n",
    "    # loop through each feature in the regression\n",
    "    for feature in range(len(logistic_L1_c.coef_[0])):\n",
    "        \n",
    "        # initalize the sum of weights at zero\n",
    "        sum_weights = 0.0\n",
    "        \n",
    "        # loop through each class\n",
    "        for category in logistic_L1_c.coef_:\n",
    "            \n",
    "            # add the weight to the sum_weights variable\n",
    "            sum_weights = sum_weights + abs(category[feature])\n",
    "            \n",
    "        # append the weight for each feature to the weights array\n",
    "        weights.append(sum_weights)\n",
    "    \n",
    "    # turn the vocabulary into a set that we might index it\n",
    "    vocab = []\n",
    "    vocab_set = set(bag_log.vocabulary_)\n",
    "    for word in vocab_set:\n",
    "        vocab.append(word)\n",
    "    \n",
    "    # create an array to store the new vocabulary that we care about\n",
    "    new_vocab = []\n",
    "    \n",
    "    # loop through the weights and eliminate from the vocabulary those features with a weight of zero\n",
    "    for index,weight in enumerate(weights):\n",
    "        \n",
    "        # if the weight for the feature is greater zero\n",
    "        if weight > 0:\n",
    "            \n",
    "            # identify the vocabulary word\n",
    "            vocab_interest = vocab[index]\n",
    "\n",
    "            # add the vocabulary to our new vocab array\n",
    "            new_vocab.append(vocab_interest)\n",
    "            \n",
    "    # store the size of the remaining vocabulary\n",
    "    vocabulary_sizes.append(len(new_vocab))\n",
    "    \n",
    "    # convert our new vocabulary to a set\n",
    "    new_vocab = set(new_vocab)\n",
    "    \n",
    "    # create a new CountVectorizer model with this new vocabulary\n",
    "    pruned_bag = CountVectorizer(vocabulary=new_vocab)\n",
    "    \n",
    "    # vectorize the data with this pruned vocabulary\n",
    "    bag_train_pruned = pruned_bag.transform(train_data)\n",
    "    \n",
    "    # run a new logistic regression model with a L2 penalty\n",
    "    logistic_L2_pruned = LogisticRegression(tol=0.01)\n",
    "    logistic_L2_pruned.fit(bag_train_pruned,train_labels)\n",
    "    \n",
    "    # vectorize the dev data\n",
    "    dev_pruned = pruned_bag.transform(dev_data)\n",
    "    \n",
    "    # calculate the accuracy of this logistic regression model and store it\n",
    "    predictions_L2 = logistic_L2_pruned.predict(dev_pruned)\n",
    "    accuracy_L2 = np.mean(predictions_L2 == dev_labels)\n",
    "    accuracies.append(accuracy_L2)\n",
    "    \n",
    "# print out the table that we will graph\n",
    "for index,c in enumerate(Cs):\n",
    "    print \"C:\", c, \" | Vocabulary Size:\", vocabulary_sizes[index], \" | Accuracy:\", accuracies[index] \n",
    "    \n",
    "# plot the accuracy against the vocabulary size for the various C values\n",
    "plt.plot(vocabulary_sizes, accuracies, 'ro')\n",
    "plt.show()\n",
    "\n",
    "### STUDENT END ###\n",
    "#P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF is short for frequency-inverse document frequency. It creates a matrix differently than CountVectorizer. CountVectorizer simply produces a bag of words matrix which produces for each document a vector with values of each word present in the model's vocabulary. TF-IDF adds a layer of complexity. It weights each word by its frequency in the corpus of documents. This decreases the value of those words that appear commonly throughout the corpus.\n",
      "\n",
      "************************************\n",
      "R-ratio: 287.179061792\n",
      "\n",
      "First off, let me congratulate you for not posting a flame about \"You sick\n",
      "perverts, you are immoral, you are all going to hell.\", which seems to be the\n",
      "usual \"religious\" post found on the alt.sex.* hierarchy.  Hopefully, you won't\n",
      "get flamed, either.\n",
      "\n",
      "You will, however, be argued with.  I personally think that your project is\n",
      "built on unsteady ground.\n",
      "\n",
      "First, I do not believe that there is any way to find an \"objective morality\". \n",
      "Morality and value are inherently subjective - they represent the beliefs of a\n",
      "person or a group of people.  They can be widely held, perhaps even\n",
      "overwhelmingly held, but they are never and _can_ never be objective.\n",
      "\n",
      "\n",
      "Second, I do not accept the assumptions that you make here.  If, as you say,\n",
      "you are trying to be objective, then why accept a morality to begin with by\n",
      "using the Christian Bible?  You're defeating your own purpose by doing so.\n",
      " \n",
      "\n",
      "Third, call me a pessimist, but you won't stop the flamage.  There will always\n",
      "be people who pop upin alt.sex.* to tell us how sick and twisted and evil we\n",
      "all are.  Just out of curiosity, do alt.sex readers show up unprovoked in the\n",
      "religion groups to tell you all that you are narrow-minded, censoring,\n",
      "overbearing totalitarianists?\n",
      " \n",
      "\n",
      "Hm.  Let me provide an example.  Four people get together over dinner, to\n",
      "discuss morality: you, me, a rather conservative Moslem, and a sociopath.  I\n",
      "start off by saying that I think it's immoral to force people to have sex with\n",
      "you.  You agree, but also say that it is immoral to have sex with someone of\n",
      "your own gender.  (Just a note: I really don't know your views on\n",
      "homosexuality, I am just using this as a common view of morality for the\n",
      "purposes of this example.)  The Moslem says that it is immoral for women to\n",
      "have their faces uncovered.\n",
      "\n",
      "The sociopath, who has become bored, kills all three of us and eats us, but\n",
      "feels no guilt because he has done nothing wrong morally in his own mind. \n",
      "                                                                         \n",
      "\n",
      "\t(Evidence deleted)\n",
      "\n",
      "I'm not going to accept your evidence for this.  You ask us to accept \"The Word\n",
      "of God\" that everything good comes from God.  This is only a valid argument for\n",
      "a person who shares your beliefs.\n",
      "\n",
      "Still, I must say that cataloging the major themes and motifs in erotica could\n",
      "be interesting for other reasons than yours, so good luck with this next part.\n",
      "\n",
      "\n",
      "Hmmm...do I detect just a wee bit of condescence here?\n",
      "                                                      \n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\t\t\t\tnot very objective. \n",
      "\n",
      "\n",
      "One could construe this to mean that beautiful people are better, or \"more\n",
      "good\" than non-beautiful people.  I would hope that people relize that this is\n",
      "not necessarily true.\n",
      "                     \n",
      "                    ^^^^\n",
      "      Might I suggest the word \"penis\"? It seems more in line with the tone of\n",
      "your post. \n",
      "          \n",
      "\n",
      "Bravo!  I respect you and your sense of humor, sir. \n",
      "\n",
      "                                                 \n",
      "Yes, and this theme is usually what the better stories are about.  However,\n",
      "they are not always selfish - I could point to examples in the work of Elf\n",
      "Sternberg, for example.\n",
      "\n",
      "\n",
      "It serves the same purpose as it does in pornographic movies: it affirms the\n",
      "virility of the male involved, as well as assuring the reader that he (the\n",
      "character) has orgasmed.                      \n",
      "\n",
      "\n",
      "Your Whole Picture [TM] unfortunately only applies to people who accept your\n",
      "church.\n",
      "         \n",
      "In addition, if sex is for procreation, then\n",
      "\n",
      "1)\tWhy did God make it pleasurable, so that people would want to do it,\n",
      "rather than building it in as instinct?\n",
      "2)\tWhy did God make it fallible?  Not every sexual encounter results in\n",
      "pregnancy, even among Catholics.  Does this mean that they have sinned?\n",
      " \n",
      " \n",
      "Granted.\n",
      "\n",
      "\n",
      "Pornography would not tend in those directions if there were not a demand for\n",
      "it.  Many people have violent fantasies that they would never act out in real\n",
      "life, but will think about and read about and mull over.\n",
      "\n",
      "Later,\n",
      "\t\t\t\t\t\tJeff                                   \n",
      "\n",
      "************************************\n",
      "\n",
      "\n",
      "************************************\n",
      "R-ratio: 325.003992285\n",
      "Looking for a graphics/CAD/or-whatever package on a X-Unix box that will\n",
      "take a file with records like:\n",
      "\n",
      "n  a  b  p\n",
      "\n",
      "where n = a count  - integer \n",
      "      a = entity a - string\n",
      "      b = entity b - string\n",
      "      p = type     - string\n",
      "\n",
      "and produce a networked graph with nodes represented with boxes or circles\n",
      "and the vertices represented by lines and the width of the line determined by\n",
      "n.  There would be a different line type for each type of vertice. The boxes\n",
      "need to be identified with the entity's name.  The number of entities < 1000\n",
      "and vertices < 100000.  It would be nice if the tool minimized line\n",
      "cross-overs and did a good job of layout.  ;-)\n",
      "\n",
      "  I have looked in the FAQ for comp.graphics and gnuplot without success. Any\n",
      "ideas would be appreciated?\n",
      "\n",
      "Thanks,\n",
      "************************************\n",
      "\n",
      "\n",
      "************************************\n",
      "R-ratio: 929.356520538\n",
      "G'day all,\n",
      "\n",
      "Can anybody point me at a utility which will read/convert/crop/whatnot/\n",
      "display HDF image files ? I've had a look at the HDF stuff under NCSA \n",
      "and it must take an award for odd directory structure, strange storage\n",
      "approaches and minimalist documentation :-)\n",
      "\n",
      "Part of the problem is that I want to look at large (5MB+) HDF files and\n",
      "crop out a section. Ideally I would like a hdftoppm type of utility, from\n",
      "which I can then use the PBMplus stuff quite merrily. I can convert the cropped\n",
      "part into another format for viewing/animation.\n",
      "\n",
      "Otherwise, can someone please explain how to set up the NCSA Visualisation S/W\n",
      "for HDF (3.2.r5 or 3.3beta) and do the above cropping/etc. This is for\n",
      "Suns with SunOS 4.1.2.\n",
      "\n",
      "Any help GREATLY appreciated. Ta muchly !\n",
      "\n",
      "Cheers,\n",
      "\tMarkus\n",
      "\n",
      "-- \n",
      "Markus Buchhorn, Parallel Computing Research Facility\n",
      "email = markus@octavia.anu.edu.au\n",
      "Australian National University, Canberra, 0200 , Australia.\n",
      "[International = +61 6, Australia = 06] [Phone = 2492930, Fax = 2490747]\n",
      "************************************\n",
      "\n",
      "\n",
      "We could improve the model by pre-processing some of the data. For exmaple, perhaps we could generalize numbers. In one of the examples above, the text used lots of measurements (e.g. '13 fps', '1.3 MB'). By using a token of some sort to represent numbers, we could focus on the units or other parts of the text that better describe this text than the specific numbers of this writer's particular circumstances. Another commonality from these examples above is their length. Perhaps the model is having trouble with longer texts. We could preprocess the data so that no text was longer than a specified length.\n"
     ]
    }
   ],
   "source": [
    "#def P7():\n",
    "### STUDENT START ###\n",
    "\n",
    "print \"TF-IDF is short for frequency-inverse document frequency. It creates a matrix differently than \\\n",
    "CountVectorizer. CountVectorizer simply produces a bag of words matrix which produces for each document a \\\n",
    "vector with values of each word present in the model's vocabulary. TF-IDF adds a layer of complexity. It weights\\\n",
    " each word by its frequency in the corpus of documents. This decreases the value of those words that appear \\\n",
    "commonly throughout the corpus.\\n\"\n",
    "\n",
    "# initalize a TFIDF vectorizer and create the model with the training data\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_train = tfidf.fit_transform(train_data)\n",
    "\n",
    "# train a logistic regression model with C=100\n",
    "logistic_tfidf = LogisticRegression(C=100)\n",
    "logistic_tfidf.fit(tfidf_train,train_labels)\n",
    "\n",
    "# transform the dev data into a vector and then predict the labels\n",
    "tfidf_dev = tfidf.transform(dev_data)\n",
    "predictions_tfidf = logistic_tfidf.predict(tfidf_dev)\n",
    "\n",
    "# initalize an array to hold the r-ratios\n",
    "r_ratios = []\n",
    "\n",
    "# calculate an R ratio for each dev data, such that R is maximum predicted probability\n",
    "# over the predicted probability of the correct label\n",
    "# loop through each text in the dev data\n",
    "for index,text in enumerate(tfidf_dev):\n",
    "    \n",
    "    # pull the maximum of the predicted probabilities\n",
    "    max_pred = max(logistic_tfidf.predict_proba(text)[0])\n",
    "\n",
    "    # pull the correct probability\n",
    "    actual_pred = logistic_tfidf.predict_proba(text)[0][dev_labels[index]]\n",
    "    \n",
    "    # calculate the R ratio\n",
    "    r_ratio = float(max_pred) / float(actual_pred)\n",
    "    r_ratios.append(r_ratio)\n",
    "\n",
    "# convert the r_ratios array to a numpy array\n",
    "r_ratios = np.array(r_ratios)    \n",
    "\n",
    "# get the indices of the 3 texts with the highest R-ratios\n",
    "max_indices = np.argpartition(r_ratios,-3)[-3:]\n",
    "\n",
    "# print out the texts at these indices\n",
    "for index in max_indices:\n",
    "    print \"************************************\"\n",
    "    print \"R-ratio:\", r_ratios[index]\n",
    "    print train_data[index]\n",
    "    print \"************************************\"\n",
    "    print \"\\n\"\n",
    "\n",
    "# print some suggestions for improving the model\n",
    "print \"We could improve the model by pre-processing some of the data. For exmaple, perhaps we could generalize \\\n",
    "numbers. In one of the examples above, the text used lots of measurements (e.g. '13 fps', '1.3 MB'). By using a \\\n",
    "token of some sort to represent numbers, we could focus on the units or other parts of the text that better \\\n",
    "describe this text than the specific numbers of this writer's particular circumstances. Another commonality \\\n",
    "from these examples above is their length. Perhaps the model is having trouble with longer texts. We could \\\n",
    "preprocess the data so that no text was longer than a specified length.\"\n",
    "    \n",
    "    \n",
    "## STUDENT END ###\n",
    "#P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: See above for answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) EXTRA CREDIT\n",
    "\n",
    "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
